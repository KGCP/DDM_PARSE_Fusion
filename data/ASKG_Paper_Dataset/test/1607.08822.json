{"sections": {"Abstract": "b'{\"search-keyword: clean-text| displaying exact matches\":\"arXiv:1607.08822v1 [cs.CV] 29 Jul 2016\\\\n\\\\n1\\\\n\\\\nSPICE: Semantic Propositional Image Caption\\\\nEvaluation\\\\nPeter Anderson1 , Basura Fernando1 , Mark Johnson2 , Stephen Gould1\\\\n1\\\\n\\\\nThe Australian National University, Canberra, Australia\\\\nfirstname.lastname@anu.edu.au\\\\n2\\\\nMacquarie University, Sydney, Australia\\\\nmark.johnson@mq.edu.au\\\\n\\\\nAbstract. There is considerable interest in the task of automatically\\\\ngenerating image captions. However, evaluation is challenging. Existing\\\\nautomatic evaluation metrics are primarily sensitive to n-gram overlap,\\\\nwhich is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an\\\\nimportant component of human caption evaluation, and propose a new\\\\nautomated caption evaluation metric defined over scene graphs coined\\\\nSPICE. Extensive evaluations across a range of models and datasets\\\\nindicate that SPICE captures human judgments over model-generated\\\\ncaptions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus\\\\n0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer\\\\nquestions such as which caption-generator best understands colors? and\\\\ncan caption-generators count?\\\\n\\\\n", "Introduction": "Introduction\\\\n\\\\nRecently there has been considerable interest in joint visual and linguistic problems, such as the task of automatically generating image captions [1,2]. Interest\\\\nhas been driven in part by the development of new and larger benchmark datasets\\\\nsuch as Flickr 8K [3], Flickr 30K [4] and MS COCO [5]. However, while new\\\\ndatasets often spur considerable innovationas has been the case with the MS\\\\nCOCO Captioning Challenge [6]benchmark datasets also require fast, accurate\\\\nand inexpensive evaluation metrics to encourage rapid progress. Unfortunately,\\\\nexisting metrics have proven to be inadequate substitutes for human judgment\\\\nin the task of evaluating image captions [7,3,8]. As such, there is an urgent need\\\\nto develop new automated evaluation metrics for this task [8,9]. In this paper,\\\\nwe present a novel automatic image caption evaluation metric that measures the\\\\nquality of generated captions by analyzing their semantic content. Our method\\\\nclosely resembles human judgment while offering the additional advantage that\\\\nthe performance of any model can be analyzed in greater detail than with other\\\\nautomated metrics.\\\\nOne of the problems with using metrics such as Bleu [10], ROUGE [11],\\\\nCIDEr [12] or METEOR [13] to evaluate captions, is that these metrics are pri-\\\\n\\\\n\\\\f2\\\\n\\\\nPeter Anderson, Basura Fernando, Mark Johnson, Stephen Gould\\\\n\\\\nFig. 1. Illustrates our methods main principle which uses semantic propositional content to assess the quality of image captions. Reference and candidate captions are\\\\nmapped through dependency parse trees (top) to semantic scene graphs (right)\\\\nencoding the objects (red), attributes (green), and relations (blue) present. Caption\\\\nquality is determined using an F-score calculated over tuples in the candidate and\\\\nreference scene graphs\\\\n\\\\nmarily sensitive to n-gram overlap. However, n-gram overlap is neither necessary\\\\nnor sufficient for two sentences to convey the same meaning [14].\\\\nTo illustrate the limitations of n-gram comparisons, consider the following\\\\ntwo captions (a,b) from the MS COCO dataset:\\\\n(a) A young girl standing on top of a tennis court.\\\\n(b) A giraffe standing on top of a green field.\\\\nThe captions describe two very different images. However, comparing these captions using any of the previously mentioned n-gram metrics produces a high\\\\nsimilarity score due to the presence of the long 5-gram phrase standing on top\\\\nof a in both captions. Now consider the captions (c,d) obtained from the same\\\\nimage:\\\\n(c) A shiny metal pot filled with some diced veggies.\\\\n(d) The pan on the stove has chopped vegetables in it.\\\\nThese captions convey almost the same meaning, but exhibit low n-gram similarity as they have no words in common.\\\\nTo overcome the limitations of existing n-gram based automatic evaluation\\\\nmetrics, in this work we hypothesize that semantic propositional content is an\\\\n\\\\n\\\\fSPICE: Semantic Propositional Image Caption Evaluation\\\\n\\\\n3\\\\n\\\\nimportant component of human caption evaluation. That is, given an image with\\\\nthe caption A young girl standing on top of a tennis court, we expect that a\\\\nhuman evaluator might consider the truth value of each of the semantic propositions contained thereinsuch as (1) there is a girl, (2) girl is young, (3) girl is\\\\nstanding, (4) there is a court, (5) court is tennis, and (6) girl is on top of court.\\\\nIf each of these propositions is clearly and obviously supported by the image, we\\\\nwould expect the caption to be considered acceptable, and scored accordingly.\\\\nTaking this main idea as motivation, we estimate caption quality by transforming both candidate and reference captions into a graph-based semantic representation called a scene graph. The scene graph explicitly encodes the objects,\\\\nattributes and relationships found in image captions, abstracting away most of\\\\nthe lexical and syntactic idiosyncrasies of natural language in the process. Recent\\\\nwork has demonstrated scene graphs to be a highly effective representation for\\\\nperforming complex image retrieval queries [15,16], and we demonstrate similar\\\\nadvantages when using this representation for caption evaluation.\\\\nTo parse an image caption into a scene graph, we use a two-stage approach\\\\nsimilar to previous works [16,17,18]. In the first stage, syntactic dependencies\\\\nbetween words in the caption are established using a dependency parser [19]\\\\npre-trained on a large dataset. An example of the resulting dependency syntax\\\\ntree, using Universal Dependency relations [20], is shown in Figure 1 top. In the\\\\nsecond stage, we map from dependency trees to scene graphs using a rule-based\\\\nsystem [16]. Given candidate and reference scene graphs, our metric computes\\\\nan F-score defined over the conjunction of logical tuples representing semantic\\\\npropositions in the scene graph (e.g., Figure 1 right). We dub this approach\\\\nSPICE for Semantic Propositional Image Caption Evaluation.\\\\nUsing a range of datasets and human evaluations, we show that SPICE outperforms existing n-gram metrics in terms of agreement with human evaluations\\\\nof model-generated captions, while offering scope for further improvements to\\\\nthe extent that semantic parsing techniques continue to improve. We make code\\\\navailable from the project page1 . Our main contributions are:\\\\n1. We propose SPICE, a principled metric for automatic image caption evaluation that compares semantic propositional content;\\\\n2. We show that SPICE outperforms metrics Bleu, METEOR, ROUGE-L and\\\\nCIDEr in terms of agreement with human evaluations; and\\\\n3. We demonstrate that SPICE performance can be decomposed to answer\\\\nquestions such as which caption-generator best understands colors? and\\\\ncan caption generators count?\\\\n\\\\n2\\\\n\\\\nBackground and ", "Related Work": "Related Work\\\\n\\\\n2.1\\\\n\\\\nCaption Evaluation Metrics\\\\n\\\\nThere is a considerable amount of work dedicated to the development of metrics\\\\nthat can be used for automatic evaluation of image captions. Typically, these\\\\n1\\\\n\\\\nhttp://panderson.me/spice\\\\n\\\\n\\\\f4\\\\n\\\\nPeter Anderson, Basura Fernando, Mark Johnson, Stephen Gould\\\\n\\\\nmetrics are posed as similarity measures that compare a candidate sentence to\\\\na set of reference or ground-truth sentences. Most of the metrics in common use\\\\nfor caption evaluation are based on n-gram matching. Bleu [10] is a modified precision metric with a sentence-brevity penalty, calculated as a weighted geometric\\\\nmean over different length n-grams. METEOR [13] uses exact, stem, synonym\\\\nand paraphrase matches between n-grams to align sentences, before computing a weighted F-score with an alignment fragmentation penalty. ROUGE [11]\\\\nis a package of a measures for automatic evaluation of text summaries using\\\\nF-measures. CIDEr [12] applies term frequency-inverse document frequency (tfidf) weights to n-grams in the candidate and reference sentences, which are then\\\\ncompared by summing their cosine similarity across n-grams. With the exception of CIDEr, these methods were originally developed for the evaluation of\\\\ntext summaries or machine translations (MT), and were subsequently adopted\\\\nfor image caption evaluation.\\\\nSeveral studies have analyzed the performance of n-gram metrics when used\\\\nfor image caption evaluation, by measuring correlation with human judgments\\\\nof caption quality. On the PASCAL 1K dataset, Bleu-1 was found to exhibit\\\\nweak or no correlation (Pearsons r of -0.17 and 0.05) [7]. Using the Flickr 8K\\\\n[3] dataset, METEOR exhibited moderate correlation (Spearmans  of 0.524)\\\\noutperforming ROUGE SU-4 (0.435), Bleu-smoothed (0.429) and Bleu-1 (0.345)\\\\n[8]. Using the PASCAL-50S and ABSTRACT-50S datasets, CIDEr and METEOR were found to have greater agreement with human consensus than Bleu\\\\nand ROUGE [12].\\\\nWithin the context of automatic MT evaluation, a number of papers have\\\\nproposed the use of shallow-semantic information such as semantic role labels\\\\n(SRLs) [14]. In the MEANT metric [21], SRLs are used to try to capture the\\\\nbasic event structure of sentences  who did what to whom, when, where and\\\\nwhy [22]. Using this approach, sentence similarity is calculated by first matching semantic frames across sentences by starting with the verbs at their head.\\\\nHowever, this approach does not easily transfer to image caption evaluation, as\\\\nverbs are frequently absent from image captions or not meaningful  e.g. a very\\\\ntall building with a train sitting next to it  and this can de-rail the matching\\\\nprocess. Our work differs from these approaches as we represent sentences using\\\\nscene graphs, which allow for noun / object matching between captions. Conceptually, the closest work to ours is probably the bag of aggregated semantic\\\\ntuples (BAST) metric [23] for image captions. However, this work required the\\\\ncollection of a purpose-built dataset in order to learn to identify Semantic Tuples, and the proposed metric was not evaluated against human judgments or\\\\nexisting metrics.\\\\n2.2\\\\n\\\\nSemantic Graphs\\\\n\\\\nScene graphs, or similar semantic structures, have been used in a number of\\\\nrecent works within the context of image and video retrieval systems to improve\\\\nperformance on complex queries [18,15,16]. Several of these papers have demonstrated that semantic graphs can be parsed from natural language descriptions\\\\n\\\\n\\\\fSPICE: Semantic Propositional Image Caption Evaluation\\\\n\\\\n5\\\\n\\\\n[18,16]. The task of transforming a sentence into its meaning representation has\\\\nalso received considerable attention within the computational linguistics community. Recent work has proposed a common framework for semantic graphs called\\\\nan abstract meaning representation (AMR) [24], for which a number of parsers\\\\n[25,26,17] and the Smatch evaluation metric [27] have been developed. However, in initial experiments, we found that AMR representations using Smatch\\\\nsimilarity performed poorly as image caption representations. Regardless of the\\\\nrepresentation used, the use of dependency trees as the starting point for parsing\\\\nsemantic graphs appears to be a common theme [16,17,18].\\\\n\\\\n3\\\\n\\\\nSPICE Metric\\\\n\\\\nGiven a candidate caption c and a set of reference captions S = {s1 , . . . , sm }\\\\nassociated with an image, our goal is to compute a score that captures the\\\\nsimilarity between c and S. For the purposes of caption evaluation the image\\\\nis disregarded, posing caption evaluation as a purely linguistic task similar to\\\\nmachine translation (MT) evaluation. However, because we exploit the semantic\\\\nstructure of scene descriptions and give primacy to nouns, our approach is better\\\\nsuited to evaluating computer generated image captions.\\\\nFirst, we transform both candidate caption and reference captions into an intermediate representation that encodes semantic propositional content. While we\\\\nare aware that there are other components of linguistic meaningsuch as figureground relationshipsthat are almost certainly relevant to caption quality, in\\\\nthis work we focus exclusively on semantic meaning. Our choice of semantic representation is the scene graph, a general structure consistent with several existing\\\\nvision datasets [16,15,28] and the recently released Visual Genome dataset [29].\\\\nThe scene graph of candidate caption c is denoted by G(c), and the scene graph\\\\nfor the reference captions S is denoted by G(S), formed as the union of scene\\\\ngraphs G(si ) for each si  S and combining synonymous object nodes. Next we\\\\npresent the semantic parsing step to generate scene graphs from captions.\\\\n3.1\\\\n\\\\nSemantic ParsingCaptions to Scene Graphs\\\\n\\\\nWe define the subtask of parsing captions to scene graphs as follows. Given a\\\\nset of object classes C, a set of relation types R, a set of attribute types A, and\\\\na caption c, we parse c to a scene graph:\\\\nG(c) = hO(c), E(c), K(c)i\\\\n\\\\n(1)\\\\n\\\\nwhere O(c)  C is the set of object mentions in c, E(c)  O(c)  R  O(c) is the\\\\nset of hyper-edges representing relations between objects, and K(c)  O(c)  A\\\\nis the set of attributes associated with objects. Note that in practice, C, R and\\\\nA are open-world sets that are expa", "Methodology": "nded as new object, relation and attribute\\\\ntypes are identified, placing no restriction on the types of objects, relation and\\\\nattributes that can be represented, including stuff nouns such as grass, sky, etc.\\\\nAn example of a parsed scene graph is illustrated in Figure 2.\\\\n\\\\n\\\\f6\\\\n\\\\nPeter Anderson, Basura Fernando, Mark Johnson, Stephen Gould\\\\n\\\\nFig. 2. A typical example of a scene graph (right) parsed from a set of reference image\\\\ncaptions (left)\\\\n\\\\nOur scene graph implementation departs slightly from previous work in image retrieval [15,16], in that we do not represent multiple instances of a single\\\\nclass of object separately in the graph. In previous work, duplication of object\\\\ninstances was necessary to enable scene graphs to be grounded to image regions.\\\\nIn our work, we simply represent object counts as attributes of objects. While\\\\nthis approach does not distinguish collective and distributive readings [16], it\\\\nsimplifies scene graph alignment and ensures that each incorrect numeric modifier is only counted as a single error.\\\\nTo complete this subtask, we adopt a variant of the rule-based version of\\\\nthe Stanford Scene Graph Parser [16]. A Probabilistic Context-Free Grammar\\\\n(PCFG) dependency parser [19] is followed by three post-processing steps that\\\\nsimplify quantificational modifiers, resolve pronouns and handle plural nouns.\\\\nThe resulting tree structure is then parsed according to nine simple linguistic\\\\nrules to extract lemmatized objects, relations and attributes, which together\\\\ncomprise the scene graph. As an example, one of the linguistic rules captures\\\\namod\\\\nadjectival modifiers, such as the young  girl example from Figure 1, which\\\\nresults in the object mention girl with attribute young. Full details of the\\\\npipeline can be found in the original paper.\\\\nSPICE slightly modifies the original parser [16] to better evaluate image captions. First, we drop the plural nouns transformation that duplicates individual\\\\nnodes of the graph according to the value of their numeric modifier. Instead,\\\\n\\\\n\\\\fSPICE: Semantic Propositional Image Caption Evaluation\\\\n\\\\n7\\\\n\\\\nnumeric modifiers are encoded as object attributes. Second, we add an additional linguistic rule that ensures that nouns will always appear as objects in\\\\nthe scene grapheven if no associated relations can identifiedas disconnected\\\\ngraph nodes are easily handled by our semantic proposition F-score calculation.\\\\nNotwithstanding the use of the Stanford Scene Graph Parser, our proposed\\\\nSPICE metric is not tied to this particular parsing pipeline. In fact, it is our hope\\\\nthat ongoing advances in syntactic and semantic parsing will allow SPICE to be\\\\nfurther improved in future releases. We also note that since SPICE operates on\\\\nscene graphs, in principle it could be used to evaluate captions on scene graph\\\\ndatasets [16,15,28] that have no reference captions at all. Evaluation of SPICE\\\\nunder these circumstances is left to future work.\\\\n3.2\\\\n\\\\nF-score Calculation\\\\n\\\\nTo evaluate the similarity of candidate and reference scene graphs, we view the\\\\nsemantic relations in the scene graph as a conjunction of logical propositions, or\\\\ntuples. We define the function T that returns logical tuples from a scene graph\\\\nas:\\\\nT (G(c)) , O(c)  E(c)  K(c)\\\\n\\\\n(2)\\\\n\\\\nEach tuple contains either one, two or three elements, representing objects, attributes and relations, respectively. For example, the scene graph in Figure 1\\\\nwould be represented with the following tuples:\\\\n{ (girl), (court), (girl, young), (girl, standing)\\\\n(court, tennis), (girl, on-top-of, court) }\\\\nViewing the semantic propositions in the scene graph as a set of tuples, we\\\\ndefine the binary matching operator  as the function that returns matching\\\\ntuples in two scene graphs. We then define precision P , recall R, and SP ICE\\\\nas:\\\\n|T (G(c))  T (G(S))|\\\\n|T (G(c))|\\\\n|T (G(c))  T (G(S))|\\\\nR(c, S) =\\\\n|T (G(S))|\\\\n2  P (c, S)  R(c, S)\\\\nSP ICE(c, S) = F1 (c, S) =\\\\nP (c, S) + R(c, S)\\\\nP (c, S) =\\\\n\\\\n(3)\\\\n(4)\\\\n(5)\\\\n\\\\nwhere for matching tuples, we reuse the wordnet synonym matching approach\\\\nof METEOR [13], such that tuples are considered to be matched if their lemmatized word forms are equalallowing terms with different inflectional forms to\\\\nmatchor if they are found in the same wordnet sysnet.\\\\nUnlike Smatch [27], a recently proposed metric for evaluating AMR parsers\\\\nthat considers multiple alignments of AMR graphs, we make no allowance for\\\\n\\\\n\\\\f8\\\\n\\\\nPeter Anderson, Basura Fernando, Mark Johnson, Stephen Gould\\\\n\\\\npartial credit when only one element of a tuple is incorrect. In the domain of\\\\nimage captions, many relations (such as in and on) are so common they arguably\\\\ndeserve no credit when applied to the wrong objects.\\\\nBeing an F-score, SPICE is simple to understand, and easily interpretable\\\\nas it is naturally bounded between 0 and 1. Unlike CIDEr, SPICE does not\\\\nuse cross-dataset statisticssuch as corpus word frequenciesand is therefore\\\\nequally applicable to both small and large datasets.\\\\n3.3\\\\n\\\\nGameability\\\\n\\\\nWhenever the focus of research is reduced to a single benchmark number, there\\\\nare risks of unintended side-effects [30]. For example, algorithms optimized for\\\\nperformance against a certain metric may produce high scores, while losing sight\\\\nof the human judgement that the metric was supposed to represent.\\\\nSPICE measures how well caption generators recover objects, attributes and\\\\nthe relations between them. A potential concern then, is that the metric could\\\\nbe gamed by generating captions that represent only objects, attributes and\\\\nrelations, while ignoring other important aspects of grammar and syntax. Because SPICE neglects fluency, as with n-gram metrics, it implicitly assuming\\\\nthat captions are well-formed. If this assumption is untrue in a particular application, a fluency metric, such as surprisal [31,32], could be included in the\\\\nevaluation. However, by default we have not included any fluency adjustments\\\\nas conceptually we favor simpler, more easily interpretable metrics. To model\\\\nhuman judgement in a particular task as closely as possible, a carefully tuned\\\\nensemble of metrics including SPICE capturing various dimensions of correctness\\\\nwould most likely be the best.\\\\n\\\\n4\\\\n\\\\n", "Experiment": "Experiments\\\\n\\\\nIn this section, we compare SPICE to existing caption evaluation metrics. We\\\\nstudy both system-level and caption-level correlation with human judgments.\\\\nData for the evaluation is drawn from four datasets collected in previous studies,\\\\nrepresenting a variety of captioning models. Depending on the dataset, human\\\\njudgments may consist of either pairwise rankings or graded scores, as described\\\\nfurther below.\\\\nOur choice of correlation coefficients is consistent with an emerging consensus from the WMT Metrics Shared Task [33,34] for scoring machine translation\\\\nmetrics. To evaluate system-level correlation, we use the Pearson correlation coefficient. Although Pearsons  measures linear association, it is smoother than\\\\nrank-based correlation coefficients when the number of data points is small and\\\\nsystems have scores that are very close together. For caption-level correlation,\\\\nwe evaluate using Kendalls  rank correlation coefficient, which evaluates the\\\\nsimilarity of pairwise rankings. Where human judgments consist of graded scores\\\\nrather than pairwise rankings, we generate pairwise rankings by comparing scores\\\\n\\\\n\\\\fSPICE: Semantic Propositional Image Caption Evaluation\\\\n\\\\n9\\\\n\\\\nTable 1. System-level Pearsons  correlation between evaluation metrics and human\\\\njudgments for the 15 competition entries plus human captions in the 2015 COCO\\\\nCaptioning Challenge [6]. SPICE more accurately reflects human judgment overall\\\\n(M1M2), and across each dimension of quality (M3M5, representing correctness,\\\\ndetailedness and saliency)\\\\n\\\\n\\\\n\\\\nM1\\\\np-value\\\\n(0.369)\\\\n(0.862)\\\\n(0.590)\\\\n(0.036)\\\\n(0.097)\\\\n(0.000)\\\\n(0.000)\\\\n\\\\n\\\\n0.29\\\\n0.10\\\\n0.20\\\\n0.57\\\\n0.47\\\\n0.86\\\\n0.89\\\\n\\\\nM2\\\\np-value\\\\n(0.271)\\\\n(0.703)\\\\n(0.469)\\\\n(0.022)\\\\n(0.070)\\\\n(0.000)\\\\n(0.000)\\\\n\\\\n\\\\n0.72\\\\n0.58\\\\n0.65\\\\n0.86\\\\n0.81\\\\n0.90\\\\n0.89\\\\n\\\\nM3\\\\np-value\\\\n(0.002)\\\\n(0.018)\\\\n(0.006)\\\\n(0.000)\\\\n(0.000)\\\\n(0.000)\\\\n(0.000)\\\\n\\\\n\\\\n\\\\nM4\\\\np-value\\\\n\\\\n-0.54\\\\n-0.63\\\\n-0.55\\\\n-0.10\\\\n-0.21\\\\n0.39\\\\n0.46\\\\n\\\\n(0.030)\\\\n(0.010)\\\\n(0.030)\\\\n(0.710)\\\\n(0.430)\\\\n(0.000)\\\\n(0.070)\\\\n\\\\n\\\\n0.44\\\\n0.30\\\\n0.38\\\\n0.74\\\\n0.65\\\\n0.95\\\\n0.97\\\\n\\\\nM5\\\\np-value\\\\n\\\\nBleu-1\\\\nBleu-4\\\\nROUGE-L\\\\nMETEOR\\\\nCIDEr\\\\nSPICE-exact\\\\nSPICE\\\\n\\\\n0.24\\\\n0.05\\\\n0.15\\\\n0.53\\\\n0.43\\\\n0.84\\\\n0.88\\\\n\\\\n(0.091)\\\\n(0.265)\\\\n(0.142)\\\\n(0.001)\\\\n(0.007)\\\\n(0.000)\\\\n(0.000)\\\\n\\\\nM1\\\\nM2\\\\nM3\\\\nM4\\\\nM5\\\\n\\\\nPercentage of captions evaluated as better or equal to human caption.\\\\nPercentage of captions that pass the Turing Test.\\\\nAverage correctness of the captions on a scale 15 (incorrect - correct).\\\\nAverage detail of the captions from 15 (lacking details - very detailed).\\\\nPercentage of captions that are similar to human description.\\\\n\\\\nover all pairs in the dataset. In datasets containing multiple independent judgments over the same caption pairs, we also report inter-human correlation. We\\\\ninclude further analysis, including additional results, examples and failure cases\\\\non our project page2 .\\\\n4.1\\\\n\\\\nDatasets\\\\n\\\\nMicrosoft COCO 2014. The COCO dataset [6] consists of 123,293 images,\\\\nsplit into an 82,783 image training set and a 40,504 image validation set. An\\\\nadditional 40,775 images are held out for testing. Images are annotated with\\\\nfive human-generated captions (C5 data), although 5,000 randomly selected test\\\\nimages have 40 captions each (C40 data).\\\\nCOCO human judgements were collected using Amazon Mechanical Turk\\\\n(AMT) for the purpose of evaluating submissions to the 2015 COCO Captioning\\\\nChallenge [6]. A total of 255,000 human judgments were collected, representing\\\\nthree independent answers to five different questions that were posed in relation\\\\nto the 15 competition entries, plus human and random entries (17 total). The\\\\nquestions capture the dimensions of overall caption quality (M1 - M2), correctness (M3), detailedness (M4), and saliency (M5), as detailed in Table 1. For\\\\npairwise rankings (M1, M2 and M5), each entry was evaluated using the same\\\\nsubset of 1000 images from the C40 test set. All AMT evaluators consisted of\\\\nUS located native speakers, white-listed from previous work. Metric scores for\\\\ncompetition entries were obtained from the COCO organizers, using our code\\\\n2\\\\n\\\\nhttp://panderson.me/spice\\\\n\\\\n\\\\f10\\\\n\\\\nPeter Anderson, Basura Fernando, Mark Johnson, Stephen Gould\\\\n\\\\nto calculate SPICE. The SPICE methodology was fixed before evaluating on\\\\nCOCO. At no stage were we given access to the COCO test captions.\\\\nFlickr 8K. The Flickr 8K dataset [3] contains 8,092 images annotated with five\\\\nhuman-generated reference captions each. The images were manually selected\\\\nto focus mainly on people and animals performing actions. The dataset also\\\\ncontains graded human quality scores for 5,822 captions, with scores ranging\\\\nfrom 1 (the selected caption is unrelated to the image) to 4 (the selected caption\\\\ndescribes the image without any errors). Each caption was scored by three expert\\\\nhuman evaluators sourced from a pool of native speakers. All evaluated captions\\\\nwere sourced from the dataset, but association to images was performed using an\\\\nimage retrieval system. In our evaluation we exclude 158 correct image-caption\\\\npairs where the candidate caption appears in the reference set. This reduces all\\\\ncorrelation scores but does not disproportionately impact any metric.\\\\nComposite Dataset. We refer to an additional dataset of 11,985 human judgments over Flickr 8K, Flickr 30K [4] and COCO captions as the composite\\\\ndataset [35]. In this dataset, captions were scored using AMT on a graded correctness scale from 1 (The description has no relevance to the image) to 5 (The\\\\ndescription relates perfectly to the image). Candidate captions were sourced\\\\nfrom the human reference captions and two recent captioning models [36,35].\\\\nPASCAL-50S To create the PASCAL-50S dataset [12], 1,000 images from the\\\\nUIUC PASCAL Sentence Dataset [37]originally containing five captions per\\\\nimagewere annotated with 50 captions each using AMT. The selected images\\\\nrepresent 20 classes including people, animals, vehicles and household objects.\\\\nThe dataset also includes human judgments over 4,000 candidate sentence\\\\npairs. However, unlike in previous studies, AMT workers were not asked to evaluate captions against images. Instead, they were asked to evaluate caption triples\\\\nby identifying Which of the sentences, B or C, is more similar to sentence A?,\\\\nwhere sentence A is a reference caption, and B and C are candidates. If reference\\\\ncaptions vary in quality, this approach may inject more noise into the evaluation process, however the differences between this approach and the previous\\\\napproaches to human evaluations have not been studied. For each candidate\\\\nsentence pair (B,C) evaluations were collected against 48 of the 50 possible reference captions. Candidate sentence pairs were generated from both human and\\\\nmodel captions, paired in four ways: human-correct (HC), human-incorrect (HI),\\\\nhuman-model (HM), and model-model (MM).\\\\n4.2\\\\n\\\\nSystem-Level Correlation\\\\n\\\\nIn Table 1 we report system-level correlations between metrics and human judgments over entries in the 2015 COCO Captioning Challenge [6]. Each entry is\\\\nevaluated using the same 1000 image subset of the COCO C40 test set. SPICE\\\\n\\\\n\\\\fSPICE: Semantic Propositional Image Caption Evaluation\\\\n\\\\n11\\\\n\\\\nFig. 3. Evaluation metrics vs. human judgments for the 15 entries in the 2015 COCO\\\\nCaptioning Challenge. Each data point represents a single model with human-generated\\\\ncaptions marked in red. Only SPICE scores human-generated captions significantly\\\\nhigher than challenge entries, which is consistent with human judgment\\\\n\\\\nsignificantly outperforms existing metrics, reaching a correlation coefficient of\\\\n0.88 with human quality judgments (M1), compared to 0.43 for CIDEr and 0.53\\\\nfor METEOR. As illustrated in Table 1, SPICE more accurately reflects human\\\\njudgment overall (M1 - M2), and across each dimension of quality (M3 - M5,\\\\nrepresenting correctness, detailedness and saliency). Interestingly, only SPICE\\\\nrewards caption detail (M4). Bleu and ROUGE-L appear to penalize detailedness, while the results for CIDEr and METEOR are not statistically significant.\\\\nAs illustrated in Figure 3, SPICE is the only metric to correctly rank humangenerated captions firstCIDEr and METEOR rank human captions 7th and\\\\n4th, respectively. SPICE is also the only metric to correctly select the top-5\\\\nnon-human entries. To help understand the importance of synonym-matching,\\\\nwe also evaluated SPICE using exact-matching only (SPICE-exact in Table 1).\\\\nPerformance degraded only marginally, although we expect synonym-matching\\\\nto become more important when fewer reference captions are available.\\\\n4.3\\\\n\\\\nColor Perception, Counting and Other Questions\\\\n\\\\nExisting n-gram evaluation metrics have little to offer in terms of understanding\\\\nthe relative strengths and weaknesses, or error modes, of various models. How-\\\\n\\\\n\\\\f12\\\\n\\\\nPeter Anderson, Basura Fernando, Mark Johnson, Stephen Gould\\\\n\\\\nTable 2. F-scores by semantic proposition subcategory. SPICE is comprised of object, relation and attribute tuples. Color, count and size are attribute subcategories.\\\\nAlthough the best models outperform the human baseline in their use of object color\\\\nattributes, none of the models exhibits a convincing ability to count\\\\n\\\\nHuman [6]\\\\nMSR [38]\\\\nGoogle [39]\\\\nMSR Captivator [40]\\\\nBerkeley LRCN [1]\\\\nMontreal/Toronto [2]\\\\nm-RNN [41]\\\\nNearest Neighbor [42]\\\\nm-RNN [43]\\\\nPicSOM\\\\nMIL\\\\nBrno University [44]\\\\nMLBL [45]\\\\nNeuralTalk [36]\\\\nACVT\\\\nTsinghua Bigeye\\\\nRandom\\\\n\\\\nSPICE Object Relation Attribute Color Count Size\\\\n0.074 0.190\\\\n0.023\\\\n0.054\\\\n0.055 0.095 0.026\\\\n0.064\\\\n0.176\\\\n0.018\\\\n0.039\\\\n0.063 0.033 0.019\\\\n0.063\\\\n0.173\\\\n0.018\\\\n0.039\\\\n0.060 0.005 0.009\\\\n0.062\\\\n0.174\\\\n0.019\\\\n0.032\\\\n0.054 0.008 0.009\\\\n0.061\\\\n0.170\\\\n0.023\\\\n0.026\\\\n0.030 0.015 0.010\\\\n0.061\\\\n0.171\\\\n0.023\\\\n0.026\\\\n0.023 0.002 0.010\\\\n0.060\\\\n0.170\\\\n0.021\\\\n0.026\\\\n0.038 0.007 0.004\\\\n0.060\\\\n0.168\\\\n0.022\\\\n0.026\\\\n0.027 0.014 0.013\\\\n0.059\\\\n0.170\\\\n0.022\\\\n0.022\\\\n0.031 0.002 0.005\\\\n0.057\\\\n0.162\\\\n0.018\\\\n0.027\\\\n0.025 0.000 0.012\\\\n0.054\\\\n0.157\\\\n0.017\\\\n0.023\\\\n0.036 0.007 0.009\\\\n0.053\\\\n0.144\\\\n0.012\\\\n0.036\\\\n0.055 0.029 0.025\\\\n0.052\\\\n0.152\\\\n0.017\\\\n0.021\\\\n0.015 0.000 0.004\\\\n0.051\\\\n0.153\\\\n0.018\\\\n0.016\\\\n0.013 0.000 0.007\\\\n0.051\\\\n0.152\\\\n0.015\\\\n0.021\\\\n0.019 0.001 0.008\\\\n0.046\\\\n0.138\\\\n0.013\\\\n0.017\\\\n0.017 0.000 0.009\\\\n0.008\\\\n0.029\\\\n0.000\\\\n0.000\\\\n0.000 0.004 0.000\\\\n\\\\never, SPICE has the useful property that it is defined over tuples that are easy to\\\\nsubdivide into meaningful categories. For example, precision, recall and F-scores\\\\ncan be quantified separately for objects, attributes and relations, or analyzed to\\\\nany arbitrary level of detail by subdividing tuples even further.\\\\nTo demonstrate this capability, in Table 2 we review the performance of 2015\\\\nCOCO Captioning Challenge submissions in terms of color perception, counting\\\\nability, and understanding of size attributes by using word lists to isolate attribute tuples that contain colors, the numbers from one to ten, and size-related\\\\nadjectives, respectively. This affords us some insight, for example, into whether\\\\ncaption generators actually understand color, and how good they are at counting.\\\\nAs shown in Table 2, the MSR entry [38] incorporating specifically trained\\\\nvisual detectors for nouns, verbs and adjectivesexceeds the human F-score\\\\nbaseline for tuples containing color attributes. However, there is less evidence\\\\nthat any of these models have learned to count objects.\\\\n4.4\\\\n\\\\nCaption-Level Correlation\\\\n\\\\nIn Table 3 we report caption-level correlations between automated metrics and\\\\nhuman judgments on Flickr 8K [3] and the composite dataset [35]. At the caption\\\\nlevel, SPICE achieves a rank correlation coefficient of 0.45 with Flickr 8K human\\\\n\\\\n\\\\fSPICE: Semantic Propositional Image Caption Evaluation\\\\n\\\\n13\\\\n\\\\nTable 3. Caption-level Kendalls  correlation between evaluation metrics and graded\\\\nhuman quality scores. At the caption-level SPICE modestly outperforms existing metrics. All p-values (not shown) are less than 0.001\\\\n\\\\nFlickr 8K [3] Composite [35]\\\\nBleu-1\\\\nBleu-4\\\\nROUGE-L\\\\nMETEOR\\\\nCIDEr\\\\nSPICE\\\\n\\\\n0.32\\\\n0.14\\\\n0.32\\\\n0.42\\\\n0.44\\\\n0.45\\\\n\\\\n0.26\\\\n0.18\\\\n0.28\\\\n0.35\\\\n0.36\\\\n0.39\\\\n\\\\nInter-human\\\\n\\\\n0.73\\\\n\\\\n-\\\\n\\\\nscores, compared to 0.44 for CIDEr and 0.42 for METEOR. Relative to the correlation between human scores of 0.73, this represents only a modest improvement\\\\nover existing metrics. However, as reported in Section 4.2, SPICE more closely\\\\napproximates human judgment when aggregated over more captions. Results\\\\nare similar on the composite dataset, with SPICE achieving a rank correlation\\\\ncoefficient of 0.39, compared to 0.36 for CIDEr and 0.35 for METEOR. As this\\\\ndataset only includes one score per image-caption pair, inter-human agreement\\\\ncannot be established.\\\\nFor consistency with previous evaluations on the PASCAL-50S dataset [12],\\\\ninstead of reporting rank correlations we evaluate on this dataset using accuracy. A metric is considered accurate if it gives an equal or higher score to the\\\\ncaption in each candidate pair most commonly preferred by human evaluators.\\\\nTo help quantify the impact of reference captions on performance, the number of\\\\nreference captions available to the metrics is varied from 1 to 48. This approach\\\\nfollows the original work on this dataset [12], although our results differ slightly\\\\nwhich may be due to randomness in the choice of reference caption subsets, or\\\\ndifferences in metric implementations (we use the MS COCO evaluation code).\\\\nOn PASCAL-50S, there is little difference in overall performance between\\\\nSPICE, METEOR and CIDEr, as shown in Figure 4 left. However, of the four\\\\nkinds of captions pairs, SPICE performs best in terms of distinguishing between\\\\ntwo model-generated captions (MM pairs) as illustrated in Table 4 and Figure 4\\\\nright. This is important as distinguishing better performing algorithms is the\\\\nprimary motivation for this work.\\\\n\\\\n5\\\\n\\\\n", "Discussion": "Conclusion and Future Work\\\\n\\\\nWe introduce SPICE, a novel semantic evaluation metric that measures how\\\\neffectively image captions recover objects, attributes and the relations between\\\\nthem. Our experiments demonstrate that, on natural image captioning datasets,\\\\nSPICE captures human judgment over model-generated captions better than\\\\n\\\\n\\\\f14\\\\n\\\\nPeter Anderson, Basura Fernando, Mark Johnson, Stephen Gould\\\\n\\\\nFig. 4. Pairwise classification accuracy of automated metrics at matching human judgment with 1-50 reference captions\\\\nTable 4. Caption-level classification accuracy of evaluation metrics at matching human judgment on PASCAL-50S with 5 reference captions. SPICE is best at matching\\\\nhuman judgments on pairs of model-generated captions (MM). METEOR is best at\\\\ndifferentiating human and model captions (HM) and human captions where one is\\\\nincorrect (HI). Bleu-1 performs best given two correct human captions (HC)\\\\n\\\\nBleu-1\\\\nBleu-2\\\\nROUGE-L\\\\nMETEOR\\\\nCIDEr\\\\nSPICE\\\\n\\\\nHC\\\\n\\\\nHI\\\\n\\\\nHM\\\\n\\\\nMM\\\\n\\\\nAll\\\\n\\\\n64.9\\\\n56.6\\\\n61.7\\\\n64.0\\\\n61.9\\\\n63.3\\\\n\\\\n95.2\\\\n93.0\\\\n95.3\\\\n98.1\\\\n98.0\\\\n96.3\\\\n\\\\n90.7\\\\n87.2\\\\n91.7\\\\n94.2\\\\n91.0\\\\n87.5\\\\n\\\\n60.1\\\\n58.0\\\\n60.3\\\\n66.8\\\\n64.6\\\\n68.2\\\\n\\\\n77.7\\\\n73.7\\\\n77.3\\\\n80.8\\\\n78.9\\\\n78.8\\\\n\\\\nexisting n-gram metrics such as Bleu, METEOR, ROUGE-L and CIDEr. Nevertheless, we are aware that significant challenges still remain in semantic parsing,\\\\nand hope that the development of more powerful parsers will underpin further\\\\nimprovements to the metric. In future work we hope to use human annotators\\\\nto establish an upper bound for how closely SPICE approximates human judgments given perfect semantic parsing. We release our code and hope that our\\\\nwork will help in the development of better captioning models.\\\\n\\\\nAcknowledgements We are grateful to the COCO Consortium (in particular, Matteo R. Ronchi, Tsung-Yi Lin, Yin Cui and Piotr Doll \\\\nar) for agreeing to run our SPICE\\\\ncode against entries in the 2015 COCO Captioning Challenge. We would also like to\\\\nthank Sebastian Schuster for sharing the Stanford Scene Graph Parser code in advance\\\\nof public release, Ramakrishna Vedantam and Somak Aditya for sharing their human\\\\ncaption judgments, and Kelvin Xu, Jacob Devlin and Qi Wu for providing model-\\\\n\\\\n\\\\fSPICE: Semantic Propositional Image Caption Evaluation\\\\n\\\\n15\\\\n\\\\ngenerated captions for evaluation. This work was funded in part by the Australian\\\\nCentre for Robotic Vision.\\\\n\\\\nReferences\\\\n1. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S.,\\\\nSaenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual\\\\nrecognition and description. In: CVPR. (2015)\\\\n2. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel, R.S.,\\\\nBengio, Y.: Show, attend and tell: Neural image caption generation with visual\\\\nattention. arXiv preprint arXiv:1502.03044 (2015)\\\\n3. Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a ranking\\\\ntask: Data, models and evaluation metrics. JAIR 47 (2013) 853899\\\\n4. Young, P., Lai, A., Hodosh, M., Hockenmaier, J.: From image descriptions to visual\\\\ndenotations: New similarity metrics for semantic inference over event descriptions.\\\\nTACL 2 (2014) 6778\\\\n5. Lin, T., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P.,\\\\nZitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV. (2014)\\\\n6. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick, C.L.:\\\\nMicrosoft COCO captions: Data collection and evaluation server. arXiv preprint\\\\narXiv:1504.00325 (2015)\\\\n7. Kulkarni, G., Premraj, V., Ordonez, V., Dhar, S., Li, S., Choi, Y., Berg, A.C.,\\\\nBerg, T.L.: BabyTalk: Understanding and generating simple image descriptions.\\\\nPAMI 35(12) (2013) 28912903\\\\n8. Elliott, D., Keller, F.: Comparing automatic evaluation measures for image description. In: ACL. (2014) 452457\\\\n9. Bernardi, R., Cakici, R., Elliott, D., Erdem, A., Erdem, E., Ikizler-Cinbis, N.,\\\\nKeller, F., Muscat, A., Plank, B.: Automatic description generation from images:\\\\nA survey of models, datasets, and evaluation measures. JAIR 55 (2016) 409442\\\\n10. Papineni, K., Roukos, S., Ward, T., Zhu, W.: Bleu: a method for automatic evaluation of machine translation. In: ACL. (2002)\\\\n11. Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In: ACL\\\\nWorkshop. (2004) 2526\\\\n12. Vedantam, R., Zitnick, C.L., Parikh, D.: CIDEr: Consensus-based image description evaluation. In: CVPR. (2015)\\\\n13. Denkowski, M., Lavie, A.: Meteor universal: Language specific translation evaluation for any target language. In: EACL 2014 Workshop on Statistical Machine\\\\nTranslation. (2014)\\\\n14. Gim enez, J., M`\\\\narquez, L.: Linguistic features for automatic evaluation of heterogenous MT systems. In: ACL Second Workshop on Statistical Machine Translation\\\\n15. Johnson, J., Krishna, R., Stark, M., Li, L.J., Shamma, D.A., Bernstein, M.S., FeiFei, L.: Image retrieval using scene graphs. In: CVPR. (2015)\\\\n16. Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., Manning, C.D.: Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In: EMNLP 4th Workshop on Vision and Language. (2015)\\\\n17. Wang, C., Xue, N., Pradhan, S.: A transition-based algorithm for AMR parsing.\\\\nIn: HLT-NAACL. (2015)\\\\n18. Lin, D., Fidler, S., Kong, C., Urtasun, R.: Visual semantic search: Retrieving\\\\nvideos via complex textual queries. In: CVPR. (2014)\\\\n\\\\n\\\\f16\\\\n\\\\nPeter Anderson, Basura Fernando, Mark Johnson, Stephen Gould\\\\n\\\\n19. Klein, D., Manning, C.D.: Accurate unlexicalized parsing. In: ACL. (2003)\\\\n20. De Marneffe, M.C., Dozat, T., Silveira, N., Haverinen, K., Ginter, F., Nivre, J.,\\\\nManning, C.D.: Universal stanford dependencies: A cross-linguistic typology. In:\\\\nLREC. Volume 14. (2014) 45854592\\\\n21. Lo, C.k., Tumuluru, A.K., Wu, D.: Fully automatic semantic MT evaluation. In:\\\\nACL Seventh Workshop on Statistical Machine Translation. (2012)\\\\n22. Pradhan, S.S., Ward, W., Hacioglu, K., Martin, J.H., Jurafsky, D.: Shallow semantic parsing using support vector machines. In: HLT-NAACL. (2004) 233240\\\\n23. Ellebracht, L., Ramisa, A., Swaroop, P., Cordero, J., Moreno-Noguer, F., Quattoni., A.: Semantic tuples for evaluation of image sentence generation. EMNLP\\\\n4th Workshop on Vision and Language (2015)\\\\n24. Banarescu, L., Bonial, C., Cai, S., Georgescu, M., Griffitt, K., Hermjakob, U.,\\\\nKnight, K., Koehn, P., Palmer, M., Schneider, N.: Abstract meaning representation\\\\n(AMR) 1.0 specification. EMNLP (2012) 15331544\\\\n25. Flanigan, J., Thomson, S., Carbonell, J., Dyer, C., Smith, N.A.: A discriminative\\\\ngraph-based parser for the abstract meaning representation. In: ACL. (2014)\\\\n26. Werling, K., Angeli, G., Manning, C.: Robust subgraph generation improves abstract meaning representation parsing. In: ACL. (2015)\\\\n27. Cai, S., Knight, K.: Smatch: an evaluation metric for semantic feature structures.\\\\nIn: ACL (2). (2013) 748752\\\\n28. Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer\\\\nimage-to-sentence models. In: CVPR. (2015) 26412649\\\\n29. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. arXiv\\\\npreprint arXiv:1602.07332 (2016)\\\\n30. Torralba, A., Efros, A.A.: Unbiased look at dataset bias. In: CVPR. (June 2011)\\\\n31. Hale, J.: A probabilistic Earley parser as a psycholinguistic model. In: NAACL.\\\\n(2001) 18\\\\n32. Levy, R.: Expectation-based syntactic comprehension. Cognition 106(3) (2008)\\\\n11261177\\\\n33. Stanojevi c, M., Kamran, A., Koehn, P., Bojar, O.: Results of the WMT15 metrics\\\\nshared task. In: ACL Tenth Workshop on Statistical Machine Translation. (2015)\\\\n256273\\\\n34. Machacek, M., Bojar, O.: Results of the WMT14 metrics shared task. In: ACL\\\\nNinth Workshop on Statistical Machine Translation. (2014) 293301\\\\n35. Aditya, S., Yang, Y., Baral, C., Fermuller, C., Aloimonos, Y.: From images to sentences through scene description graphs using commonsense reasoning and knowledge. arXiv preprint arXiv:1511.03292 (2015)\\\\n36. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image\\\\ndescriptions. In: CVPR. (2015)\\\\n37. Rashtchian, C., Young, P., Hodosh, M., Hockenmaier, J.: Collecting image annotations using Amazons Mechanical Turk. In: HLT-NAACL. (2010) 139147\\\\n38. Fang, H., Gupta, S., Iandola, F.N., Srivastava, R., Deng, L., Dollar, P., Gao, J.,\\\\nHe, X., Mitchell, M., Platt, J.C., Zitnick, C.L., Zweig, G.: From captions to visual\\\\nconcepts and back. In: CVPR. (2015)\\\\n39. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image\\\\ncaption generator. In: CVPR. (2015)\\\\n\\\\n\\\\fSPICE: Semantic Propositional Image Caption Evaluation\\\\n\\\\n17\\\\n\\\\n40. Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., Mitchell,\\\\nM.: Language models for image captioning: The quirks and what works. arXiv\\\\npreprint arXiv:1505.01809 (2015)\\\\n41. Mao, J., Wei, X., Yang, Y., Wang, J., Huang, Z., Yuille, A.L.: Learning like a\\\\nchild: Fast novel visual concept learning from sentence descriptions of images. In:\\\\nCVPR. (2015) 25332541\\\\n42. Devlin, J., Gupta, S., Girshick, R.B., Mitchell, M., Zitnick, C.L.: Exploring nearest\\\\nneighbor approaches for image captioning. arXiv preprint arXiv:1505.04467 (2015)\\\\n43. Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., Yuille, A.: Deep captioning with\\\\nmultimodal recurrent neural networks (m-rnn). arXiv preprint arXiv:1412.6632\\\\n(2014)\\\\n44. Kol \\\\nar, M., Hradis, M., Zemc k, P.: Technical report: Image captioning with semantically similar images. arXiv preprint arXiv:1506.03995 (2015)\\\\n45. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Multimodal neural language models.\\\\nICML 14 (2014) 595603\\\\n\\\\n\\\\f\"}\\n'"}, "key_words": {"Abstract": [["caption evaluation propose", 0.6459], ["propositional image caption", 0.6259], ["image captions evaluation", 0.6249], ["caption evaluation", 0.6185], ["captions evaluation", 0.6183]], "Introduction": [["caption evaluation metric", 0.7135], ["image caption evaluation", 0.7032], ["evaluating image captions", 0.7008], ["evaluate captions metrics", 0.6984], ["representation caption evaluation", 0.6878]], "Related Work": [["caption evaluation measuring", 0.6339], ["purposes caption evaluation", 0.6232], ["caption evaluation nverbs", 0.6019], ["evaluation image captions", 0.592], ["caption evaluation n5", 0.5907]], "Methodology": [["scene graph parser", 0.6732], ["scene graphs define", 0.6672], ["scene graph conjunction", 0.6504], ["parsed scene graph", 0.6384], ["example scene graph", 0.6251]], "Experiment": [["caption level correlations", 0.7237], ["caption level correlation", 0.7124], ["caption evaluation metrics", 0.6779], ["captions scored using", 0.6432], ["captions significantly nhigher", 0.6411]], "Discussion": [["captions evaluation work", 0.6172], ["caption evaluation n15", 0.6114], ["captions evaluation", 0.5973], ["caption evaluation n17", 0.5972], ["human model captions", 0.5902]]}, "summarization": {"Abstract": "SPICE: Semantic Propositional Image Caption is a new automated caption evaluation metric defined over scene graphs. We show that SPICE captures human judgments over model-generated captions better than other automatic metrics. The authors argue that semantic propositional content is an important component of human caption evaluation.", "Introduction": "we present a novel automatic image caption evaluation metric for automatically generating image captions. We use semantic propositional content to assess the quality of image caption quality. The method is based on a syntactic dependency tree and a semantic scene graph. We argue that the method resembles human judgment for the task of evaluating image caption quality.", "Related Work": "metrics can be used for automatic evaluation of image captions. Most metrics for caption evaluation are based on n-gram matching. Our work represents sentences using scene graphs, which allow for noun / object matching. We show how these metrics compare with other metrics for image caption evaluation.", "Methodology": "we use a variant of the Stanford Scene Graph Parser to parse image captions. we represent objects as attributes of objects, relations and attributes in the scene graph. SPICE can be used to evaluate captions on any type of image without a reference image. we use a set of linguistic rules to extract objects and relations.", "Experiment": "spICE more accurately reflects human judgment overall and across each dimension of quality. We compare SPICE to existing caption evaluation metrics. we compare system-level and caption-level correlation with human judgments. SPICE is used to assess the accuracy of captioning models and human judgment.", "Discussion": "SPICE is a new semantic evaluation metric for image captioning. It captures human judgment over model-generated captions better than existing metrics. The metric is best at matching human judgments on pairs of model- generated captions. The authors hope to improve the accuracy of SPICE's semantic parsing."}}