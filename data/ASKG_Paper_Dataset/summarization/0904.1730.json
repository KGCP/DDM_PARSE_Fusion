{"sections": {"Abstract": "b'{\"search-keyword: clean-text| displaying exact matches\":\"1\\\\n\\\\nFeedback-based online network coding\\\\n\\\\narXiv:0904.1730v1 [cs.NI] 10 Apr 2009\\\\n\\\\nJay Kumar Sundararajan, Student Member, IEEE, Devavrat Shah, Member, IEEE,\\\\nMuriel M edard, Fellow, IEEE\\\\n\\\\nAbstractCurrent approaches to the practical implementation of network coding are batch-based, and often\\\\ndo not use feedback, except possibly to signal completion\\\\nof a file download. In this paper, the various benefits of\\\\nusing feedback in a network coded system are studied.\\\\nIt is shown that network coding can be performed in a\\\\ncompletely online manner, without the need for batches or\\\\ngenerations, and that such online operation does not affect\\\\nthe throughput. Although these ideas are presented in a\\\\nsingle-hop packet erasure broadcast setting, they naturally\\\\nextend to more general lossy networks which employ\\\\nnetwork coding in the presence of feedback. The impact\\\\", "Introduction": "nof feedback on queue size at the sender and decoding\\\\ndelay at the receivers is studied. Strategies for adaptive\\\\ncoding based on feedback are presented, with the goal\\\\nof minimizing the queue size and delay. The asymptotic\\\\nbehavior of these metrics is characterized, in the limit of\\\\nthe traffic load approaching capacity. Different notions of\\\\ndecoding delay are considered, including an order-sensitive\\\\nnotion which assumes that packets are useful only when\\\\ndelivered in order. Our work may be viewed as a natural\\\\nextension of Automatic Repeat reQuest (ARQ) schemes to\\\\ncoded networks.\\\\nIndex TermsNetwork Coding, Decoding Delay, ARQ\\\\n\\\\nI. I NTRODUCTION\\\\nThis paper is a step towards low-delay, highthroughput solutions based on network coding, for realtime data streaming applications over a packet erasure\\\\nnetwork. In particular, it considers the role of feedback\\\\nfor queue management and delay control in such systems.\\\\nA. ", "Related Work": "Background\\\\nReliable communication over a network of packet\\\\nerasure channels is a well studied problem. Several\\\\nsolutions have been proposed, especially in the case\\\\nwhen there is no feedback. We compare below, three\\\\nThe authors are with the Department of Electrical Engineering and Computer Science, at the Massachusetts Institute of\\\\nTechnology, Cambridge, MA 02139. Email: {jaykumar, devavrat,\\\\nmedard}@mit.edu\\\\nManuscript received ; revised . Parts of this work were presented\\\\nat IEEE ITW 2007, IEEE ISIT 2008 and ISITA 2008.\\\\n\\\\nsuch approaches  digital fountain codes, random linear\\\\nnetwork coding and priority encoding transmission.\\\\n1. Digital fountain codes: The digital fountain\\\\ncodes ([1], [2]) constitute a well-known approach to this\\\\nproblem. From a block of k transmit packets, the sender\\\\ngenerates random linear combinations in such a way\\\\nthat the receiver can, with high probability, decode the\\\\nblock once it receives any set of slightly more than k\\\\nlinear combinations. This approach has low complexity\\\\nand requires no feedback, except to signal successful\\\\ndecoding of the block. However, fountain codes are\\\\ndesigned for a point-to-point erasure channel and in their\\\\noriginal form, do not extend readily to a network setting.\\\\nConsider a two-link tandem network. An end-to-end\\\\nfountain code with simple forwarding at the middle node\\\\nwill result in throughput loss. If the middle node chooses\\\\nto decode and re-encode an entire block, the scheme will\\\\nbe sub-optimal in terms of delay, as pointed out by [3]. In\\\\nthis sense, the fountain code approach is not composable\\\\nacross links. For the special case of tree networks, there\\\\nhas been some recent work on composing fountain codes\\\\nacross links by enabling the middle node to re-encode\\\\neven before decoding the entire block [4].\\\\n2. Random linear network coding: Network coding\\\\nwas originally introduced for the case of error-free\\\\nnetworks with specified link capacities ([5], [6]), and\\\\nwas extended to the case of erasure networks [7]. In\\\\ncontrast to fountain codes, the random linear network\\\\ncoding solution of [8] does not require decoding at\\\\nintermediate nodes and can be applied in any network.\\\\nEach node transmits a random linear combination of all\\\\ncoded packets it has received so far. This solution ensures\\\\nthat with high probability, the transmitted packet will\\\\nhave what we call the innovation guarantee property,\\\\ni.e., it will be innovative1 to every receiver that receives\\\\nit successfully, except if the receiver already knows as\\\\nmuch as the sender. Thus, every successful reception will\\\\nbring a unit of new information. In [8], this scheme is\\\\nshown to achieve capacity for the case of a multicast\\\\nsession.\\\\n1\\\\n\\\\nAn innovative packet is a linear combination of packets which is\\\\nlinearly independent of previously received linear combinations, and\\\\nthus conveys new information.\\\\n\\\\n\\\\f2\\\\n\\\\nAn important problem with both fountain codes and\\\\nrandom linear network coding is that although they are\\\\nrateless, the encoding operation is performed on a block\\\\n(or generation) of packets. This means that in general,\\\\nthere is no guarantee that the receiver will be able to\\\\nextract and pass on to higher layers, any of the original\\\\npackets from the coded packets till the entire block has\\\\nbeen received. This leads to a decoding delay.\\\\nSuch a decoding delay is not a problem if the higher\\\\nlayers will anyway use a block only as a whole (e.g., file\\\\ndownload). This corresponds to traditional approaches\\\\nin information theory where the message is assumed\\\\nto be useful only as a whole. No incentive is placed\\\\non decoding a part of the message using a part of\\\\nthe codeword. However, many applications today involve\\\\nbroadcasting a continuous stream of packets in real-time\\\\n(e.g., video streaming). Sources generate a stream of\\\\nmessages which have an intrinsic temporal ordering. In\\\\nsuch cases, playback is possible only till the point up to\\\\nwhich all packets have been recovered, which we call the\\\\nfront of contiguous knowledge. Thus, there is incentive to\\\\ndecode the older messages earlier, as this will reduce the\\\\nplayback latency. The above schemes would segment the\\\\nstream into blocks and process one block at a time. Block\\\\nsizes will have to be large to ensure high throughput.\\\\nHowever, if playback can begin only after receiving a\\\\nfull block, then large blocks will imply a large delay.\\\\nThis raises an interesting question: can we code in\\\\nsuch a way that playback can begin even before the full\\\\nblock is received? In other words, we are more interested\\\\nin packet delay than block delay. These issues have been\\\\nstudied using various approaches by [9], [10] and [11] in\\\\na point-to-point setting. However, in a network setting,\\\\nthe problem is not well understood. Moreover, these\\\\nworks do not consider the queue management aspects\\\\nof the problem. In related work, [12] and [13] address\\\\nthe question of how many original packets are revealed\\\\nbefore the whole block is decoded in a fountain code\\\\nsetting. However, performance may depend on not only\\\\nhow much data reaches the receiver in a given time,\\\\nbut also which part of the data. For instance, playback\\\\ndelay depends on not just the number of original packets\\\\nthat are recovered, but also the order in which they are\\\\nrecovered.\\\\n3. Priority encoding transmission: The scheme\\\\nproposed in [14], known as priority encoding transmission (PET), addresses this problem by proposing a\\\\ncode for the erasure channel that ensures that a receiver\\\\nwill receive the first (or highest priority) i messages\\\\nusing the first ki coded packets, where ki increases with\\\\ndecreasing priority. In [15], [16], this is extended to\\\\nsystems that perform network coding. A concatenated\\\\n\\\\nnetwork coding scheme is proposed in [16], with a delaymitigating pre-coding stage. This scheme guarantees that\\\\nthe kth innovative reception will enable the receiver to\\\\ndecode the kth message. In such schemes however, the\\\\nability to decode messages in order requires a reduction\\\\nin throughput because of the pre-coding stage.\\\\nB. Motivation\\\\nThe main motivation for our current work is that the\\\\navailability of feedback brings the hope of simultaneously achieving the best possible throughput along with\\\\nminimal packet delay and queue size.\\\\nReliable communication over a point-to-point packet\\\\nerasure channel with full feedback can be achieved\\\\nusing the Automatic Repeat reQuest (ARQ) scheme \\\\nwhenever a packet gets erased, the sender retransmits it.\\\\nEvery successful reception conveys a new packet, implying throughput optimality. Moreover, this new packet\\\\nis always the next unknown packet, which implies the\\\\nlowest possible packet delay. Since there is feedback,\\\\nthe sender never stores anything the receiver already\\\\nknows, implying optimal queue size. Thus, this simple\\\\nscheme simultaneously achieves the optimal throughput\\\\nalong with minimal delay and queue size. Moreover, the\\\\nscheme is completely online and not block-based.\\\\nHowever, if we go beyond a single point-to-point link,\\\\nARQ is not sufficient in general. Coding across packets\\\\nis necessary to achieve optimal throughput, even if we\\\\nallow acknowledgments. For instance, in the network\\\\ncoding context, link-by-link ARQ cannot achieve the\\\\nmulticast capacity of the butterfly network from network\\\\ncoding literature [5]. Similarly, ARQ is sub-optimal for\\\\nbroadcast-mode links because retransmitting a packet\\\\nthat some receivers did not get is wasteful for the\\\\nothers that already have it. In contrast, network coding\\\\nachieves the multicast capacity of any network and also\\\\nreadily extends to networks with broadcast-mode links.\\\\nThus, in such situations, coding is indispensable from a\\\\nthroughput perspective.\\\\nThis leads to the question  how to combine the\\\\nbenefits of ARQ and network coding? The goal is to\\\\nextend ARQs desirable properties in the point-to-point\\\\ncontext, to systems that require coding across packets.\\\\nThe problem with applying ARQ to a coded system\\\\nis that a new reception may not always reveal the next\\\\nunknown packet to the receiver. Instead, it may bring in\\\\na linear equation involving the packets. In conventional\\\\nARQ, upon receiving an ACK, the sender drops the\\\\nACKed packet and transmits the next one. But in a coded\\\\nsystem, upon receiving an ACK for a linear equation, it\\\\nis not clear which linear combination the sender should\\\\n\\\\n\\\\f3\\\\n\\\\npick for its next transmission to obtain the best system\\\\nperformance. This is important because, if the receiver\\\\nhas to collect many equations before it can decode the\\\\nunknowns involved, this could lead to a large decoding\\\\ndelay.\\\\nA related question is: upon receiving the ACK for\\\\na linear equation, which packet can be excluded from\\\\nfuture coding, i.e., which packet can be dropped from the\\\\nsenders queue? If packets arrive at the sender according\\\\nto some stochastic process, (as in [17], [18]) and links\\\\nare lossy (as in [7], [8]), then the queue management\\\\naspect of the problem also becomes important.\\\\nOne option is to drop packets that all receivers have\\\\ndecoded, as this would not affect the reliability. However, storing all undecoded packets may be suboptimal.\\\\nConsider a situation where the sender has n packets\\\\np1 , p2 . . . , pn , and all receivers have received (n1) linear combinations: (p1 +p2 ), (p2 +p3 ), . . . , (pn1 +pn ).\\\\nA drop-when-decoded scheme will not allow the sender\\\\nto drop any packet, since no packet can be decoded by\\\\nany receiver yet. However, the backlog in the amount\\\\nof information, also called the virtual queue ([17], [18]),\\\\nhas a size of just 1. We ideally want the physical queue to\\\\ntrack the virtual queue in size. (Indeed, in this example,\\\\nit would be sufficient if the sender stores any one pi in\\\\norder to ensure reliable delivery.)\\\\nThese issues motivate the following questions  if we\\\\nhave feedback in a system with network coding, what is\\\\nthe best possible tradeoff between throughput, delay and\\\\nqueue size? In particular, how close can we get to the\\\\nperformance of ARQ for the point-to-point case? These\\\\nare the questions we address in this paper.\\\\nII. O UR CONTRIBUTION\\\\nIn this paper, we show that by proper use of feedback,\\\\nit is possible to perform network coding in a completely\\\\nonline manner similar to ARQ schemes, without the need\\\\nfor a block-based approach. We study the benefits of\\\\nfeedback in a coded network in terms of the following\\\\ntwo aspects  queue management and decoding delay.\\\\nA. Queue management\\\\nNote: In this work, we treat packets as vectors over\\\\na finite field. We restrict our attention to linear network\\\\ncoding. Therefore, the state of knowledge of a node can\\\\nbe viewed as a vector space over the field (see Section\\\\nIII for further details).\\\\nWe propose a new acknowledgment mechanism that\\\\nuses feedback to acknowledge degrees of freedom2 instead of original decoded packets. Based on this new\\\\n2\\\\nHere, degree of freedom refers to a new dimension in the appropriate vector space representing the senders knowledge.\\\\n\\\\nform of ACKs, we propose an online coding module that\\\\nnaturally generalizes ARQ to coded systems. The code\\\\nimplies a queue update algorithm that ensures that the\\\\nphysical queue size at the sender will track the backlog\\\\nin degrees of freedom.\\\\nIt is clear that packets that have been decoded by all\\\\nreceivers need not be retained at the sender. But, our\\\\nproposal is more general than that. The key intuition\\\\nis that we can ensure reliable transmission even if\\\\nwe restrict the senders transmit packet to be chosen\\\\nfrom a subspace that is independent3 of the subspace\\\\nrepresenting the common knowledge available at all the\\\\nreceivers.\\\\nIn other words, the sender need not use for coding\\\\n(and hence need not store) any information that has\\\\nalready been received by all the receivers. Therefore,\\\\nat any point in time, the queue simply needs to store\\\\na basis for a coset space with respect to the subspace\\\\nof knowledge common to all the receivers. We define\\\\na specific way of computing this basis using the new\\\\nnotion of a node seeing a message packet, which is\\\\ndefined below.\\\\nDefinition 1 (Index of a packet): For any positive integer k, the kth packet that arrives at the sender is said\\\\nto have an index k.\\\\nDefinition 2 (Seeing a packet): A node is said to have\\\\nseen a message packet p if it has received enough\\\\ninformation to compute a linear combination of the\\\\nform (p + q), where q is itself a linear combination\\\\ninvolving only packets with an index greater than that of\\\\np. (Decoding implies seeing, as we can pick q = 0.)\\\\nIn our scheme, the feedback is utilized as follows.\\\\nIn conventional ARQ, a receiver ACKs a packet upon\\\\ndecoding it successfully. However, in our scheme a\\\\nreceiver ACKs a packet when it sees the packet.\\\\nOur new scheme is called the drop-when-seen algorithm\\\\nbecause the sender drops a packet if all receivers have\\\\nseen (ACKed) it.\\\\nSince decoding implies seeing, the senders queue is\\\\nexpected to be shorter under our scheme compared to\\\\nthe drop-when-decoded scheme. However, we will need\\\\nto show that in spite of dropping seen packets even\\\\nbefore they are decoded, we can still ensure reliable\\\\ndelivery. To prove this, we present a deterministic coding\\\\nscheme that uses only unseen packets and still guarantees\\\\nthat the coded packet will simultaneously cause each\\\\nreceiver that receives it successfully, to see its next\\\\nunseen packet. We will prove later that seeing a new\\\\npacket translates to receiving a new degree of freedom.\\\\n3\\\\nA subspace S1 is said to be independent of another subspace S2\\\\nif S1  S2 = {0}. See [19] for more details.\\\\n\\\\n\\\\f4\\\\n\\\\nThis means, the innovation guarantee property is satisfied\\\\nand therefore, reliability and 100% throughput can be\\\\nachieved (see Algorithm 2 (b) and corresponding Theorems 6 and 8 in Section IV-C).\\\\nThe intuition is that if all receivers have seen p, then\\\\ntheir uncertainty can be resolved using only packets with\\\\nindex more than that of p because after decoding these\\\\npackets, the receivers can compute q and hence obtain p\\\\nas well. Therefore, even if the receivers have not decoded\\\\np, no information is lost by dropping it, provided it has\\\\nbeen seen by all receivers.\\\\nNext, we present an example that explains our algorithm for a simple two-receiver case. Section IV-C3\\\\nextends this scheme to more receivers.\\\\nExample: Table I shows a sample of how the proposed\\\\nidea works in a packet erasure broadcast channel with\\\\ntwo receivers A and B. The senders queue is shown after\\\\nthe arrival point and before the transmission point of a\\\\nslot (see Section III for details on the setup). In each slot,\\\\nbased on the ACKs, the sender identifies the next unseen\\\\npacket for A and B. If they are the same packet, then\\\\nthat packet is sent. If not, their XOR is sent. It can be\\\\nverified that with this rule, every reception causes each\\\\nreceiver to see its next unseen packet.\\\\nIn slot 1, p1 reaches A but not B. In slot 2, (p1  p2 )\\\\nreaches A and B. Since A knows p1 , it can also decode\\\\np2 . As for B, it has now seen (but not decoded) p1 .\\\\nAt this point, since A and B have seen p1 , the sender\\\\ndrops it. This is fine even though B has not yet decoded\\\\np1 , because B will eventually decode p2 (in slot 4),\\\\nat which time it can obtain p1 . Similarly, p2 , p3 and\\\\np4 will be dropped in slots 3, 5 and 6 respectively.\\\\nHowever, the drop-when-decoded policy will drop p1\\\\nand p2 in slot 4, and p3 and p4 in slot 6. Thus, our new\\\\nstrategy clearly keeps the queue shorter. This is formally\\\\nproved in Theorem 1 and Theorem 6. The example\\\\nalso shows that it is fine to drop packets before they\\\\nare decoded. Eventually, the future packets will arrive,\\\\nthereby allowing the decoding of all the packets.\\\\nRelated earlier work: In [20], Shrader and\\\\nEphremides study the queue stability and delay of\\\\nblock-based random linear coding versus uncoded\\\\nARQ for stochastic arrivals in a broadcast setting.\\\\nHowever, this work does not consider the combination\\\\nof coding and feedback in one scheme. In related work,\\\\n[21] studies the case of load-dependent variable sized\\\\ncoding blocks with ACKs at the end of a block, using\\\\na bulk-service queue model. The main difference in\\\\nour work is that receivers ACK packets even before\\\\ndecoding them, and this enables the sender to perform\\\\n\\\\nonline coding.\\\\nSagduyu and Ephremides [22] consider online\\\\nfeedback-based adaptation of the code, and propose a\\\\ncoding scheme for the case of two receivers. This work\\\\nfocuses on the maximum possible stable throughput, and\\\\ndoes not consider the use feedback to minimize queue\\\\nsize or decoding delay. In [23], the authors study the\\\\nthroughput of a block-based coding scheme, where receivers acknowledge the successful decoding of an entire\\\\nblock, allowing the sender to move to the next block.\\\\nNext, they consider the option of adapting the code based\\\\non feedback for the multiple receiver case. They build\\\\non the two-receiver case of [22] and propose a greedy\\\\ndeterministic coding scheme that may not be throughput\\\\noptimal, but picks a linear combination such that the\\\\nnumber of receivers that immediately decode a packet\\\\nis maximized. In contrast, in our work we consider\\\\nthroughput-optimal policies that aim to minimize queue\\\\nsize and delay.\\\\nIn [24], Lacan and Lochin proposes an erasure coding\\\\nalgorithm called Tetrys to ensure reliability in spite of\\\\nlosses on the acknowledgment path. While this scheme\\\\nalso employs coding in the presence of feedback, their\\\\napproach is to make minimal use of the feedback, in\\\\norder to be robust to feedback losses. As opposed to\\\\nsuch an approach, we investigate how best to use the\\\\navailable feedback to improve the coding scheme and\\\\nother performance metrics. For instance, in the scheme in\\\\n[24], packets are acknowledged (if at all) only when they\\\\nare decoded, and these are then dropped from the coding\\\\nwindow. However, we show in this work that by dropping\\\\npackets when they are seen, we can maintain a smaller\\\\ncoding window without compromising on reliability and\\\\nthroughput. A smaller coding window translates to lower\\\\nencoding complexity and smaller queue size at the sender\\\\nin the case of stochastic arrivals.\\\\nThe use of ACKs and coded retransmissions in a\\\\npacket erasure broadcast channel has been considered\\\\nfor multiple unicasts [25] and multicast ([26], [27], [28],\\\\n[29]). The main goal of these works however, is to\\\\noptimize the throughput. Other metrics such as queue\\\\nmanagement and decoding delay are not considered.\\\\nIn our work, we focus on using feedback to optimize\\\\nthese metrics as well, in addition to achieving 100%\\\\nthroughput in a multicast setting. Our coding module\\\\n(in Section IV-C5) is closely related to the one proposed\\\\nby Larsson in an independent work [28]. However, our\\\\nalgorithm is specified using the more general framework\\\\nof seen packets, which allows us to derive the dropwhen-seen queue management algorithm and bring out\\\\nthe connection between the physical queue and virtual\\\\nqueue sizes. Reference [28] does not consider the queue\\\\n\\\\n\\\\f5\\\\n\\\\nTime\\\\n\\\\nSenders queue\\\\n\\\\nTransmitted\\\\npacket\\\\n\\\\nChannel\\\\nstate\\\\n\\\\nA\\\\nDecoded\\\\n\\\\n1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n\\\\np1\\\\np1 , p2\\\\np2 , p3\\\\np3\\\\np3 , p4\\\\np4\\\\n\\\\np1\\\\np1  p2\\\\np2  p3\\\\np3\\\\np3  p4\\\\np4\\\\n\\\\n\\\\n\\\\n9\\\\n9\\\\n\\\\n\\\\n\\\\nA,\\\\nA,\\\\nA,\\\\nA,\\\\nA,\\\\nA,\\\\n\\\\n9\\\\n\\\\n\\\\n\\\\n9\\\\n\\\\n\\\\nB\\\\nB\\\\nB\\\\nB\\\\nB\\\\nB\\\\n\\\\np1\\\\np1 , p2\\\\np1 , p2\\\\np1 , p2\\\\np1 , p2\\\\np1 , p2 , p3 , p4\\\\n\\\\nB\\\\nSeen\\\\nbut\\\\nnot\\\\ndecoded\\\\np3\\\\n-\\\\n\\\\nDecoded\\\\n\\\\np1 , p2 , p3\\\\np1 , p2 , p3\\\\np1 , p2 , p3 , p4\\\\n\\\\nSeen\\\\nbut\\\\nnot\\\\ndecoded\\\\np1\\\\np1 , p2\\\\n-\\\\n\\\\nTABLE I\\\\nA N EXAMPLE OF THE DROP - WHEN - SEEN ALGORITHM\\\\n\\\\nmanagement problem. Moreover, using the notion of\\\\nseen packets allows our algorithm to be compatible even\\\\nwith random coding. This in turn enables a simple ACK\\\\nformat and makes it suitable for practical implementation. (See Remark 2 for further discussion.)\\\\nImplications of our new scheme: The newly proposed\\\\nscheme has many useful implications:\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQueue size: The physical queue size is upperbounded by the sum of the backlogs in degrees of\\\\nfreedom between the sender and all the receivers.\\\\nThis fact implies that as the traffic load approaches\\\\ncapacity (as load factor   1), the expected\\\\n\\\\u0011size of\\\\n\\\\u0010\\\\n1\\\\n. This\\\\nthe physical queue at the sender is O 1\\\\nis the same order as for single-receiver ARQ, and\\\\nhence, is order-optimal.\\\\nQueuing analysis: Our scheme forms a natural\\\\nbridge between the virtual and physical queue sizes.\\\\nIt can be used to extend results on the stability\\\\nof virtual queues such as [17], [18] and [30] to\\\\nphysical queues. Moreover, various results obtained\\\\nfor virtual queues from traditional queuing theory,\\\\nsuch as the transform based analysis for the queue\\\\nsize of M/G/1 queues, or even a Jackson network\\\\ntype of result [8], can be extended to the physical\\\\nqueue size of nodes in a network coded system.\\\\nSimple queue management: Our approach based\\\\non seen packets ensures that the sender does not\\\\nhave to store linear combinations of the packets in\\\\nthe queue to represent the basis of the coset space.\\\\nInstead, it can store the basis using the original\\\\nuncoded packets themselves. Therefore, the queue\\\\nfollows a simple first-in-first-out service discipline.\\\\nOnline encoding: All receivers see packets in the\\\\nsame order in which they arrived at the sender.\\\\nThis gives a guarantee that the information deficit\\\\nat the receiver is restricted to a set of packets that\\\\nadvances in a streaming manner and has a stable\\\\nsize (namely, the set of unseen packets). In this\\\\nsense, the proposed encoding scheme is truly online.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nEasy decoding: Every transmitted linear combination is sparse  at most n packets are coded together\\\\nfor the n receiver case. This reduces the decoding\\\\ncomplexity as well as the overhead for embedding\\\\nthe coding coefficients in the packet header.\\\\nExtensions: We present our scheme for a single\\\\npacket erasure broadcast channel. However, our\\\\nalgorithm is composable across links and can be\\\\napplied to a tandem network of broadcast links.\\\\nWith suitable modifications, it can potentially be\\\\napplied to a more general setup like the one in\\\\n[7] provided we have feedback. Such extensions are\\\\ndiscussed further in Section VII.\\\\n\\\\nB. Decoding delay\\\\nThe drop-when-seen algorithm and the associated coding module do not guarantee that the seen packets will be\\\\ndecoded immediately. In general, there will be a delay in\\\\ndecoding, as the receiver will have to collect enough linear combinations involving the unknown packets before\\\\nbeing able to decode the packets.\\\\nOnline feedback-based adaptation of the code with the\\\\ngoal of minimizing decoding delay has been studied in\\\\nthe context of a packet erasure broadcast channel in [31].\\\\nHowever, their notion of delay ignores the order in which\\\\npackets are decoded. For the special case of only two\\\\nreceivers, [32] proposes a feedback-based coding algorithm that not only achieves 100% throughput, but also\\\\nguarantees that every successful innovative reception will\\\\ncause the receiver to decode a new packet. We call\\\\nthis property instantaneous decodability. However, this\\\\napproach does not extend to the case of more than two receivers. With prior knowledge of the erasure pattern, [31]\\\\ngives an offline algorithm that achieves optimal delay\\\\nand throughput for the case of three receivers. However,\\\\nin the online case, even with only three receivers, [32]\\\\nshows through an example (Example V.1) that it is\\\\nnot possible to simultaneously guarantee instantaneous\\\\ndecodability as well as throughput optimality.\\\\n\\\\n\\\\f6\\\\n\\\\nIn the light of this example, our current work aims\\\\nfor a relaxed version of instantaneous decodability while\\\\nstill retaining the requirement of optimal throughput.\\\\nWe consider a situation with stochastic arrivals and\\\\nstudy the problem using a queuing theory approach.\\\\nLet  and  be the arrival rate and the channel quality\\\\nparameter respectively. Let  , / be the load factor.\\\\nWe consider asymptotics when the load factor on the\\\\nsystem tends to 1, while keeping either  or  fixed at a\\\\nnumber less than 1. The optimal throughput requirement\\\\nmeans that the queue of undelivered packets is stable\\\\nfor all values of  less than 1. Our new requirement\\\\non decoding delay is that the growth of the average\\\\n1\\\\ndecoding delay as a function of 1\\\\nas   1, should\\\\nbe of the same order as for the single receiver case.\\\\nThe expected per-packet delay of a receiver in a system\\\\nwith more than one receiver is clearly lower bounded by\\\\nthe corresponding quantity for a single-receiver system.\\\\nThus, instead of instantaneous decoding, we aim to\\\\nguarantee asymptotically optimal decoding delay as the\\\\nsystem load approaches capacity. The motivation is that\\\\nin most practical systems, delay becomes a critical issue\\\\nonly when the system starts approaching its full capacity.\\\\nWhen the load on the system is well within its capacity,\\\\nthe delay is usually small and hence not an issue. For the\\\\ncase of two receivers, it can be shown that this relaxed\\\\nrequirement is satisfied by the scheme in [32] due to\\\\nthe instantaneous decodability property, i.e., the scheme\\\\nachieves the asymptotically optimal average decoding\\\\ndelay per packet for the two-receiver case.\\\\nIn our current work, we provide a new coding module\\\\nfor the case of three receivers that achieves optimal\\\\nthroughput. We conjecture that at the same time\\\\nit also achieves an asymptotically optimal decoding\\\\ndelay as the system approaches capacity, in the\\\\nfollowing sense. With a single receiver, the optimal\\\\nscheme is ARQ with no coding and we show that this\\\\nachieves\\\\n\\\\u0010\\\\n\\\\u0011 an expected per-packet delay at the sender of\\\\n1\\\\n 1 . For the three-receiver system, we conjecture\\\\n\\\\u0010\\\\n\\\\n\\\\u0011\\\\n\\\\n1\\\\n, and\\\\nthat our scheme also achieves a delay of O 1\\\\nthus meets the lower bound in an asymptotic sense. We\\\\nalso study a stronger notion of delay, namely the delivery\\\\ndelay, which measures delay till the point when the\\\\npacket can be delivered to the application above, with\\\\nthe constraint that packets cannot be delivered out of\\\\norder. We conjecture that our scheme is asymptotically\\\\noptimal even in terms of delivery delay.\\\\n\\\\nWe have verified these conjectures through simulations for values of  that are very close to 1. It is\\\\nuseful to note that asymptotically optimal decoding delay translates to asymptotically optimal expected queue\\\\n\\\\noccupancy at the sender using the simple queuing rule of\\\\ndropping packets that have been decoded by all receivers.\\\\nAdaptive coding allows the senders code to incorporate receivers states of knowledge and thereby enables\\\\nthe sender to control the evolution of the front of\\\\ncontiguous knowledge. Our schemes may thus be viewed\\\\nas a step towards feedback-based control of the tradeoff\\\\nbetween throughput and decoding delay, along the lines\\\\nsuggested in [33].\\\\nC. Organization\\\\nThe rest of the paper is organized as follows. Section\\\\nIII describes the packet erasure broadcast setting. Section\\\\nIV is concerned with adaptive codes that minimize the\\\\nsenders queue size. In Section IV-A, we define and\\\\nanalyze a baseline algorithm that drops packets only\\\\nwhen they have been decoded by all receivers. Section\\\\nIV-B presents a generic form of our newly proposed\\\\nalgorithm, and introduces the idea of excluding from the\\\\nsenders queue, any knowledge that is common to all\\\\nreceivers. We show that the algorithm guarantees that the\\\\nphysical queue size tracks the virtual queue size. Section\\\\nIV-C presents an easily implementable variant of the\\\\ngeneric algorithm of Section IV-B, called the drop-whenseen algorithm. The drop-when-seen algorithm consists\\\\nof a queuing module that provides guarantees on the\\\\nqueue size, and a coding module that provides guarantees\\\\non reliability and throughput, while complying with the\\\\nqueuing module. In Section VI, we investigate adaptive\\\\ncodes aimed at minimizing the receivers decoding delay.\\\\nFor the case of three receivers, we propose a new coding\\\\nmodule that is proved to be throughput optimal and\\\\nconjectured to be asymptotically optimal in terms of\\\\ndelay. Section VII presents some ideas on extending\\\\nthe algorithms to more general topologies and scenarios.\\\\nFinally, Section VIII gives the conclusions.\\\\nIII. T HE\\\\n\\\\nSETUP\\\\n\\\\nIn this paper, we consider a communication problem\\\\nwhere a sender wants to broadcast a stream of data to n\\\\nreceivers. The data are organized into packets, which are\\\\nessentially vectors of fixed size over a finite field Fq . A\\\\npacket erasure broadcast channel connects the sender to\\\\nthe receivers. Time is slotted. The details of the queuing\\\\nmodel and its dynamics are described next.\\\\nThe queuing model\\\\nThe sender is assumed to have an infinite buffer, i.e.,\\\\na queue with no preset size constraints. We assume that\\\\nthe sender is restricted to use linear codes. Thus, every\\\\ntransmission is a linear combination of packets from the\\\\n\\\\n\\\\f7\\\\n\\\\nincoming stream that are currently in the buffer. The\\\\nvector of coefficients used in the linear combination summarizes the relation between the coded packet and the\\\\noriginal stream. We assume that this coefficient vector is\\\\nembedded in the packet header. A node can compute\\\\nany linear combination whose coefficient vector is in\\\\nthe linear span of the coefficient vectors of previously\\\\nreceived coded packets. In this context, the state of\\\\nknowledge of a node can be defined as follows.\\\\nDefinition 3 (Knowledge of a node): The knowledge\\\\nof a node at some point in time is the set of all linear\\\\ncombinations of the original packets that the node can\\\\ncompute, based on the information it has received up\\\\nto that point. The coefficient vectors of these linear\\\\ncombinations form a vector space called the knowledge\\\\nspace of the node.\\\\nWe use the notion of a virtual queue to represent the\\\\nbacklog between the sender and receiver in terms of\\\\nlinear degrees of freedom. This notion was also used\\\\nin [17], [18] and [30]. There is one virtual queue for\\\\neach receiver.\\\\nDefinition 4 (Virtual queue): For j = 1, 2, . . . , n, the\\\\nsize of the j th virtual queue is defined to be the difference between the dimension of the knowledge space of\\\\nthe sender and that of the j th receiver.\\\\nWe will use the term physical queue to refer to the\\\\nsenders actual buffer, in order to distinguish it from\\\\nthe virtual queues. Note that the virtual queues do not\\\\ncorrespond to real storage.\\\\nDefinition 5 (Degree of freedom): The term degree of\\\\nfreedom refers to one dimension in the knowledge space\\\\nof a node. It corresponds to one packet worth of data.\\\\nDefinition 6 (Innovative packet): A coded packet\\\\nwith coefficient vector c is said to be innovative to\\\\na receiver with knowledge space V if c \\\\n/ V . Such\\\\na packet, if successfully received, will increase the\\\\ndimension of the receivers knowledge space by one\\\\nunit.\\\\nDefinition 7 (Innovation guarantee property): Let V\\\\ndenote the senders knowledge space, and Vj denote the\\\\nknowledge space of receiver j for j = 1, 2, . . . , n. A\\\\ncoding scheme is said to have the innovation guarantee\\\\nproperty if in every slot, the coefficient vector of the\\\\ntransmitted linear combination is in V \\\\\\\\Vj for every j\\\\nsuch that Vj 6= V . In other words, the transmission is\\\\ninnovative to every receiver except when the receiver\\\\nalready knows everything that the sender knows.\\\\n\\\\nArrivals\\\\nPackets arrive into the senders physical queue according to a Bernoulli process4 of rate . An arrival at the\\\\nphysical queue translates to an arrival at each virtual\\\\nqueue since the new packet is a new degree of freedom\\\\nthat the sender knows, but none of the receivers knows.\\\\nService\\\\nThe channel accepts one packet per slot. Each receiver either receives this packet with no errors (with\\\\nprobability ) or an erasure occurs (with prob", "Methodology": "ability\\\\n(1  )). Erasures occur independently across receivers\\\\nand across slots. The receivers are assumed to be capable\\\\nof detecting an erasure.\\\\nWe only consider coding schemes that satisfy the\\\\ninnovation guarantee property. This property implies that\\\\nif the virtual queue of a receiver is not empty, then\\\\na successful reception reveals a previously unknown\\\\ndegree of freedom to the receiver and the virtual queue\\\\nsize decreases by one unit. We can thus map a successful\\\\nreception by some receiver to one unit of service of the\\\\ncorresponding virtual queue. This means, in every slot,\\\\neach virtual queue is served independently of the others\\\\nwith probability .\\\\nThe relation between the service of the virtual queues\\\\nand the service of the physical queue depends on the\\\\nqueue update scheme used, and will be discussed separately under each update policy.\\\\nFeedback\\\\nWe assume perfect delay-free feedback. In Algorithm\\\\n1 below, feedback is used to indicate successful decoding. For all the other algorithms, the feedback is needed\\\\nin every slot to indicate the occurrence of an erasure.\\\\nTiming\\\\nFigure 1 shows the relative timing of various events\\\\nwithin a slot. All arrivals are assumed to occur just after\\\\nthe beginning of the slot. The point of transmission is\\\\nafter the arrival point. For simplicity, we assume very\\\\nsmall propagation time. Specifically, we assume that the\\\\ntransmission, unless erased by the channel, reaches the\\\\nreceivers before they send feedback for that slot and\\\\nfeedback from all receivers reaches the sender before\\\\nthe end of the same slot. Thus, the feedback incorporates\\\\nthe current slots reception also. Based on this feedback,\\\\npackets are dropped from the physical queue just before\\\\n4\\\\n\\\\nWe have assumed Bernoulli arrivals for ease of exposition. However, we expect the results to hold for more general arrival processes\\\\nas well.\\\\n\\\\n\\\\f8\\\\nPoint where\\\\nstate variables\\\\nare measured\\\\n\\\\nSlot number t\\\\n\\\\n  + \\\\n\\\\n\\\\n + \\\\n\\\\n0\\\\n\\\\nPoint of\\\\narrival\\\\n\\\\nPoint of\\\\ntransmission\\\\n\\\\nPoint of\\\\nfeedback\\\\n\\\\nPoint of\\\\ndeparture for\\\\nphysical queue\\\\n\\\\nFig. 1. Relative timing of arrival, service and departure points within\\\\na slot\\\\n\\\\nthe end of the slot, according to the queue update rule.\\\\nQueue sizes are measured at the end of the slot.\\\\nThe load factor is denoted by  := /. In what\\\\nfollows, we will study the asymptotic behavior of the\\\\nexpected queue size and decoding delay under various\\\\npolicies, as   1 from below. For the asymptotics, we\\\\nassume that either  or  is fixed, while the other varies\\\\ncausing  to increase to 1.\\\\nIV. Q UEUE\\\\n\\\\nSIZE\\\\n\\\\nIn this section, we first present a baseline algorithm \\\\nretain packets in the queue until the feedback confirms\\\\nthat they have been decoded by all the receivers. Then,\\\\nwe present a new queue update rule that is motivated\\\\nby a novel coding algorithm. The new rule allows the\\\\nphysical queue size to track the virtual queue sizes.\\\\nA. Algorithm 1: Drop when decoded (baseline)\\\\nWe first present the baseline scheme which we will\\\\ncall Algorithm 1. It combines a random coding strategy\\\\nwith a drop-when-decoded rule for queue update. The\\\\ncoding scheme is an online version of [8] with no preset\\\\ngeneration size  a coded packet is formed by computing\\\\na random linear combination of all packets currently in\\\\nthe queue. With such a scheme, the innovation guarantee\\\\nproperty will hold with high probability, provided the\\\\nfield size is large enough (We assume the field size is\\\\nlarge enough to ignore the probability that the coded\\\\npacket is not innovative. It can be incorporated into\\\\nthe model by assuming a slightly larger probability of\\\\nerasure because a non-innovative packet is equivalent to\\\\nan erasure.).\\\\nFor any receiver, the packets at the sender are unknowns, and each received linear combination is an\\\\nequation in these unknowns. Decoding becomes possible\\\\nwhenever the number of linearly independent equations\\\\ncatches up with the number of unknowns involved. The\\\\ndifference between the number of unknowns and number\\\\nof equations is essentially the backlog in degrees of\\\\nfreedom, i.e., the virtual queue size. Thus, a virtual\\\\n\\\\n1\\\\n\\\\n\\\\nTime\\\\n\\\\n  + \\\\n\\\\n\\\\n  + \\\\n\\\\n\\\\n2\\\\n\\\\n\\\\n3\\\\n\\\\n\\\\nFig. 2. Markov chain representing the size of a virtual queue. Here\\\\n  := (1  ) and \\\\n\\\\n  := (1  ).\\\\n\\\\nqueue becoming empty translates to successful decoding\\\\nat the corresponding receiver. Whenever a receiver is\\\\nable to decode in this manner, it informs the sender.\\\\nBased on this, the sender tracks which receivers have\\\\ndecoded each packet, and drops a packet if it has been\\\\ndecoded by all receivers. From a reliability perspective,\\\\nthis is fine because there is no need to involve decoded\\\\npackets in the linear combination.\\\\nRemark 1: In general, it may be possible to solve for\\\\nsome of the unknowns even before the virtual queue\\\\nbecomes empty. For example, this could happen if a\\\\nnewly received linear combination cancels everything\\\\nexcept one unknown in a previously known linear combination. It could also happen if some packets were\\\\ninvolved in a subset of equations that can be solved\\\\namong themselves locally. Then, even if the overall\\\\nsystem has more unknowns than equations, the packets\\\\ninvolved in the local system can be decoded. However,\\\\nthese are secondary effects and we ignore them in this\\\\nanalysis. Equivalently, we assume that if a packet is\\\\ndecoded before the virtual queue becomes empty, the\\\\nsender ignores the occurrence of this event and waits for\\\\nthe next emptying of the virtual queue before dropping\\\\nthe packet. We believe this assumption will not change\\\\nthe asymptotic behavior of the queue size, since decoding\\\\nbefore the virtual queue becoming empty is a rare event\\\\nwith random linear coding over a large field.\\\\n1) The virtual queue size in steady state: We will now\\\\nstudy the behavior of the virtual queues in steady state.\\\\nBut first, we introduce some notation:\\\\nQ(t) := Size of the senders physical queue at the end\\\\nof slot t\\\\nQj (t) := Size of the j th virtual queue at the end of slot\\\\nt\\\\nFigure 2 shows the Markov chain for Qj (t). If  < ,\\\\nthen the chain {Qj (t)} is positive recurrent and has a\\\\nsteady state distribution given by [34]:\\\\nk := lim P[Qj (t) = k] = (1  )k ,\\\\nt\\\\n\\\\nk  0 (1)\\\\n\\\\n(1)\\\\nwhere  = (1)\\\\n.\\\\nThus, the expected size of any virtual queue in steady\\\\n\\\\n\\\\f9\\\\n\\\\nUsing this fact, we can compute the expectation of Dj\\\\nas follows:\\\\n\\\\nstate is given by:\\\\nlim E[Qj (t)] =\\\\n\\\\nt\\\\n\\\\n\\\\nX\\\\n\\\\njj = (1  ) \\\\n\\\\nj=0\\\\n\\\\n\\\\n(1  )\\\\n\\\\n(2)\\\\n\\\\nNext, we analyze the physical queue size under this\\\\nscheme.\\\\n2) The physical queue size in steady state: The following theorem characterizes the asymptotic behavior of\\\\nthe queue size under Algorithm 1, as the load on the\\\\nsystem approaches capacity (  1).\\\\nTheorem 1: The expected size of \\\\u0010the physical\\\\nqueue\\\\n\\\\u0011\\\\n1\\\\nin steady state for Algorithm 1 is  (1)2 .\\\\nComparing with Equation (2), this result makes it clear\\\\nthat the physical queue size does not track the virtual\\\\nqueue size. (We assume that  and  are themselves\\\\naway from 1, but only their ratio approaches 1 from\\\\nbelow.)\\\\nIn the rest of this subsection, we present the arguments\\\\nthat lead to the above result. Let T be the time an\\\\narbitrary arrival in steady state spends in the physical\\\\nqueue before departure, excluding the slot in which the\\\\narrival occurs (Thus, if a packet departs immediately\\\\nafter it arrives, then T is 0.). A packet in the physical\\\\nqueue will depart when each virtual queue has become\\\\nempty at least once since its arrival. Let Dj be the\\\\ntime starting from the new arrival, till the next emptying\\\\nof the j th virtual queue. Then, T = maxj Dj and so,\\\\nE[T ]  E[Dj ]. Hence, we focus on E[Dj ].\\\\nWe condition on the event that the state seen by the\\\\nnew arrival just before it joins the queue, is some state\\\\nk. There are two possibilities for the queue state at the\\\\nend of the slot in which the packet arrives. If the channel\\\\nis ON in that slot, then there is a departure and the state\\\\nat the end of the slot is k. If the channel is OFF, then\\\\nthere is no departure and the state is (k + 1). Now, Dj\\\\nis simply the first passage time from the state at the end\\\\nof that slot to state 0, i.e., the number of slots it takes\\\\nfor the system to reach state 0 for the first time, starting\\\\nfrom the state at the end of the arrival slot. Let u,v\\\\ndenote the expected first passage time from state u to\\\\nstate v . The expected first passage time from state u to\\\\nstate 0, for u > 0 is derived in Appendix A, and is given\\\\nby the following expression:\\\\nu,0 =\\\\n\\\\nu\\\\n\\\\n\\\\nNow, because of the property that Bernoulli arrivals\\\\nsee time averages (BASTA) [35], an arbitrary arrival sees\\\\nthe same distribution for the size of the virtual queues,\\\\nas the steady state distribution given in Equation (1).\\\\n\\\\nE[Dj ] =\\\\n=\\\\n\\\\n\\\\nX\\\\n\\\\nk=0\\\\n\\\\nX\\\\n\\\\nk=0\\\\n\\\\nX\\\\n\\\\nP(New arrival sees state k)E[Dj |State k]\\\\nk [k,0 + (1  )k+1,0 ]\\\\n\\\\nk + (1  )(k + 1)\\\\n\\\\nk=0\\\\n\\\\n1\\\\n\\\\n=\\\\n\\\\n(1  )2\\\\n=\\\\n\\\\nk \\\\n\\\\n(3)\\\\n\\\\nNow, the expected time that an arbitrary arrival in\\\\nsteady state spends in the system is given by:\\\\n1\\\\nE[T ] = E[max Dj ]  E[Dj ] = \\\\nj\\\\n(1  )2\\\\n\\\\u0012\\\\n\\\\n\\\\u0013\\\\n\\\\nSince each virtual queue is positive recurrent (assuming\\\\n < ), the physical queue will also become empty\\\\ninfinitely often. Then we can use Littles law to find\\\\nthe expected physical queue size.\\\\nThe expected queue size of the physical queue in\\\\nsteady state if we use algorithm 1 is given by:\\\\n1\\\\nlim E[Q(t)] = E[T ] = \\\\nt\\\\n(1  )2\\\\n\\\\u0012\\\\n\\\\n\\\\u0013\\\\n\\\\nThis discussion thus completes the proof of Theorem 1\\\\nstated above.\\\\nB. Algorithm 2 (a): Drop common knowledge\\\\nIn this section, we first present a generic algorithm\\\\nthat operates at the level of knowledge spaces and their\\\\nbases, in order to ensure that the physical queue size\\\\ntracks the virtual queue size. Later, we shall describe a\\\\nsimple-to-implement variant of this generic algorithm.\\\\n1) An intuitive description: The aim of this algorithm\\\\nis to drop as much data as possible from the senders\\\\nbuffer while still satisfying the reliability requirement\\\\nand the innovation guarantee property. In other words,\\\\nthe sender should store just enough data so that it can\\\\nalways compute a linear combination which is simultaneously innovative to all receivers who have an information deficit. As we shall see, the innovation guarantee\\\\nproperty is sufficient for good performance.\\\\nAfter each slot, every receiver informs the sender\\\\nwhether an erasure occurred, using perfect feedback.\\\\nThus, there is a slot-by-slot feedback requirement which\\\\nmeans that the frequency of feedback messages is higher\\\\nthan in Algorithm 1. The main idea is to exclude from the\\\\nqueue, any knowledge that is known to all the receivers.\\\\nMore specifically, the queues contents must correspond\\\\nto some basis of a vector space that is independent of the\\\\n\\\\n\\\\f10\\\\n\\\\nintersection of the knowledge spaces of all the receivers.\\\\nWe show in Lemma 2 that with this queuing rule, it\\\\nis always possible to compute a linear combination of\\\\nthe current contents of the queue that will guarantee\\\\ninnovation, as long as the field size is more than n, the\\\\nnumber of receivers.\\\\nThe fact that the common knowledge is dropped\\\\nsuggests a modular or incremental approach to the\\\\nsenders operations. Although the knowledge spaces of\\\\nthe receivers keep growing with time, the sender only\\\\nneeds to operate with the projection of these spaces\\\\non dimensions currently in the queue, since the coding\\\\nmodule does not care about the remaining part of the\\\\nknowledge spaces that is common to all receivers. Thus,\\\\nthe algorithm can be implemented in an incremental\\\\nmanner. It will be shown that this incremental approach\\\\nis equivalent to the cumulative approach.\\\\nTable II shows the main correspondence between the\\\\nnotions used in the uncoded case and the coded case. We\\\\nnow present the queue update algorithm formally. Then\\\\nwe present theorems that prove that under this algorithm,\\\\nthe physical queue size at the sender tracks the virtual\\\\nqueue size.\\\\nAll operations in the algorithm occur over a finite field\\\\nof size q > n. The basis of a nodes knowledge space is\\\\nstored as the rows of a basis matrix. The representation\\\\nand all operations are in terms of local coefficient vectors\\\\n(i.e., with respect to the current contents of the queue)\\\\nand not global ones (i.e., with respect to the original\\\\npackets).\\\\n2) Formal description of the algorithm:\\\\nAlgorithm 2 (a)\\\\n1. Initialize basis matrices B , B1 , . . . , Bn to the\\\\nempty matrix. These contain the bases of the\\\\nincremental knowledge spaces of the sender and\\\\nreceivers in that order.\\\\n2. Initialize the vector g to the zero vector. This will\\\\nhold the coefficients of the transmitted packet in\\\\neach slot.\\\\nIn every time slot, do:\\\\n3. Incorporate new arrivals:\\\\nLet a be the number of new packets that arrived\\\\nat the beginning of the slot. Place these packets\\\\nat the end of the queue. Let B have b rows. Set\\\\nB to Ia+b . (Im denotes the identity matrix of size\\\\nm.) Note that B will always be an identity matrix.\\\\nTo make the number of columns of all matrices\\\\nconsistent (i.e., equal to a + b), append a all-zero\\\\ncolumns to each Bj .\\\\n4. Transmission:\\\\nIf B is not empty, update g to be any vector that is\\\\nin span(B), but not in {j:Bj (B} span(Bj ). (Note:\\\\n\\\\nspan(B) denotes the row space of B .)\\\\nLemma 2 shows that such a g exists. Let\\\\ny1 , y2 , . . . yQ represent the current contents of the\\\\nqueue, where the queue size Q = (a+b). Compute\\\\nP\\\\nthe linear combination Q\\\\ni=1 gi yi and transmit it\\\\non the packet erasure broadcast channel. If B is\\\\nempty, set g to 0 and transmit nothing.\\\\n5. Incorporate feedback:\\\\nOnce the feedback arrives, for every receiver j = 1\\\\nto n, do:\\\\nIf g 6= 0 and the transmission was successfully received by receiver j in this slot,\\\\nappend g as a new row to Bj .\\\\n6. Separate out the knowledge that is common to all\\\\nreceivers:\\\\nCompute the following (the set notation used here\\\\nconsiders the matrices as a set of row vectors):\\\\nB := Any basis of nj=1 span(Bj ).\\\\nB\\\\n:= Completion of B into a basis of\\\\nspan(B).\\\\nB  := B  \\\\\\\\B .\\\\nBj\\\\n:= Completion of B into a basis of\\\\nspan(Bj ) in such a way that, if\\\\nwe define Bj := Bj \\\\\\\\B , then the\\\\nfollowing holds: Bj  span(B  ).\\\\nLemma 1 proves that this is possible.\\\\n7. Update the queue contents:\\\\nReplace the contents of the queue with packets\\\\n of the form PQ h y for each h \\\\ny1 , y2 , . . . yQ\\\\n\\\\ni=1 i i\\\\n\\\\nB . The new queue size Q is thus equal to the\\\\nnumber of rows in B  .\\\\n8. Recompute local coefficient vectors with respect to\\\\nthe new queue contents:\\\\nFind a matrix Cj such that Bj = Xj B  (this is\\\\npossible because Bj  span(B  )). Call Xj the\\\\nnew Bj . Update the value of B to IQ .\\\\n9. Go back to step 3 for the next slot.\\\\nThe above algorithm essentially removes, at the end\\\\nof each slot, the common knowledge (represented by\\\\nthe basis B ) and retains only the remainder B  . The\\\\nknowledge spaces of the receivers are also represented\\\\nin an incremental manner in the form of Bj , excluding\\\\nthe common knowledge. Since Bj  span(B  ), the Bj\\\\nvectors can be completely described in terms of the vectors in B  . It is as if B has been completely removed\\\\nfrom the entire setting, and the only goal remaining is to\\\\nconvey span(B  ) to the receivers. Hence, it is sufficient\\\\nto store linear combinations corresponding to B  in the\\\\nqueue. B  and Bj get mapped to the new B and Bj ,\\\\nand the process repeats in the next slot.\\\\nLemma 1: In step 5 of the algorithm above, it is possible to complete B into a basis Bj of each span(Bj )\\\\n\\\\n\\\\f11\\\\n\\\\nKnowledge represented by\\\\nAmount of knowledge\\\\n\\\\nUncoded Networks\\\\nSet of received packets\\\\nNumber of packets received\\\\n\\\\nQueue stores\\\\n\\\\nAll undelivered packets\\\\n\\\\nUpdate rule after\\\\neach transmission\\\\n\\\\nIf a packet has been received by all\\\\nreceivers drop it.\\\\n\\\\nCoded Networks\\\\nVector space spanned by the coefficient vectors\\\\nof the received linear combinations\\\\nNumber of linearly independent (innovative) linear combinations of packets received (i.e., dimension of the knowledge space)\\\\nLinear combination of packets which form a basis\\\\nfor the coset space of the common knowledge at\\\\nall receivers\\\\nRecompute the common knowledge space V ;\\\\nStore a new set of linear combinations so that\\\\ntheir span is independent of V\\\\n\\\\nTABLE II\\\\nT HE\\\\n\\\\nUNCODED VS . CODED CASE\\\\n\\\\nsuch that Bj  span(B  ).\\\\nProof: We show that any completion of B into a\\\\nbasis of span(Bj ) can be changed to a basis with the\\\\nrequired property.\\\\nLet B = {b1 , b2 , . . . , bm }. Suppose we complete\\\\nthis into a basis Cj of span(Bj ) such that:\\\\nCj = B  {c1 , c2 , . . . , c|Bj |m }\\\\n\\\\nNow, we claim that at the beginning of step 6,\\\\nspan(Bj )  span(B) for all j . This can be proved\\\\nby induction on the slot number, using the way the\\\\nalgorithm updates B and the Bj s. Intuitively, it says that\\\\nany receiver knows a subset of what the sender knows.\\\\nTherefore, for each vector c  Cj \\\\\\\\B , c must also be\\\\nin span(B). Now, since B  B  is a basis of span(B),\\\\nP\\\\n\\\\n\\\\n\\\\nwe can write c as m\\\\ni=1 i bi +c with c  span(B ). In\\\\n\\\\nthis manner, each ci gives a distinct ci . It is easily seen\\\\nthat Cj := B  {c1 , c2 , . . . , c|Bj |m } is also a basis\\\\nof the same space that is spanned by Cj . Moreover, it\\\\nsatisfies the property that Cj \\\\\\\\B  span(B  ).\\\\nLemma 2: ([30]) Let V be a vector space with\\\\ndimension k over a field of size q , and let V1 , V2 , . . . Vn ,\\\\nbe subspaces of V , of dimensions k1 , k2 , . . . , kn respectively. Suppose that k > ki for all i = 1, 2, . . . , n. Then,\\\\nthere exists a vector that is in V but is not in any of the\\\\nVi s, if q > n.\\\\nProof: See [30] for the proof.\\\\nThis lemma is also closely related to the result in [28],\\\\nwhich derives the smallest field size needed to ensure\\\\ninnovation guarantee.\\\\n3) Connecting the physical and virtual queue sizes:\\\\nIn this subsection, we will prove the following result that\\\\nrelates the size of the physical queue at the sender and\\\\nthe virtual queues, which themselves correspond to the\\\\nbacklog in degrees of freedom.\\\\nTheorem 2: For Algorithm 2 (a), the physical queue\\\\nsize at the sender is upper bounded by the sum of the\\\\n\\\\nbacklog differences between the sender and each receiver\\\\nin terms of the number of degrees of freedom.\\\\nLet a(t) denote the number of arrivals in slot t, and let\\\\nA(t) be the total number of arrivals up to and including\\\\nP\\\\nslot t, i.e., A(t) = tt =0 a(t ). Let B(t) (resp. Bj (t))\\\\nbe the matrix B (resp. Bj ) after incorporating the slot t\\\\narrivals, i.e., at the end of step 3 in slot t. Let H(t) be\\\\na matrix whose rows are the global coefficient vectors\\\\nof the queue contents at the end of step 3 in time slot t,\\\\ni.e., the coefficient vectors in terms of the original packet\\\\nA(t)\\\\nstream. Note that each row of H(t) is in Fq .\\\\nLet g(t) denote the vector g at the calculated in step\\\\n4 in time slot t, i.e., the local coefficient vector of the\\\\npacket transmitted in slot t. Also, let B (t) (resp. B  (t),\\\\nBj (t) and Bj (t)) denote the matrix B (resp. B  , Bj\\\\nand Bj ) at the end of step 6 in time slot t.\\\\nLemma 3: The rows of H(t) are linearly independent\\\\nfor all t.\\\\nProof: The proof is by induction on t.\\\\nBasis step: In the beginning of time slot 1, a(1)\\\\npackets arrive. So, H(1) = Ia(1) and hence the rows\\\\nare linearly independent.\\\\nInduction hypothesis: Assume H(t  1) has linearly\\\\nindependent rows.\\\\nInduction step: The queue is updated such that the\\\\nlinear combinations corresponding to local coefficient\\\\nvectors in B  are stored, and subsequently, the a(t)\\\\nnew arrivals are appended. Thus, the relation between\\\\nH(t  1) and H(t) is:\\\\nH(t) =\\\\n\\\\n\\\\\"\\\\n\\\\nB  (t  1)H(t  1)\\\\n0\\\\n0\\\\nIa(t)\\\\n\\\\n#\\\\n\\\\nNow, B  (t  1) has linearly independent rows, since\\\\nthe rows form a basis. The rows of H(t  1) are also\\\\nlinearly independent by hypothesis. Hence, the rows of\\\\nB  (t  1)H(t  1) will also be linearly independent.\\\\nAppending a(t) zeros and then adding an identity matrix\\\\n\\\\n\\\\f12\\\\nSTEP 6\\\\nSeparate out\\\\ncommon\\\\nknowledge\\\\n\\\\nSTEP 5\\\\nIncorporate channel\\\\nstate feedback\\\\nSTEP 3\\\\nIncorporate arrivals\\\\nof slot t U (t ), V (t )\\\\n\\\\n\\\\nU (t) = U\\\\n(t)  U  (t)\\\\n\\\\nUj (t)\\\\nUj (t)\\\\n\\\\nU \\' j (t ), V \\' j (t )\\\\nU \\'  (t ), V \\'  (t )\\\\nU \\' \\' (t ), U \\' \\' j (t )\\\\n\\\\nU j (t ), V j (t )\\\\nSlot (t-1)\\\\n\\\\ndomain:\\\\n=\\\\n\\\\n\\\\n(t)\\\\nU\\\\n\\\\n\\\\n\\\\n\\\\nUj (t)\\\\n\\\\n U (t),\\\\n\\\\n(5)\\\\nand,\\\\n\\\\nj = 1, 2, . . . , n\\\\n\\\\nSlot t\\\\n\\\\nFrom the above properties, we can infer that U1 (t) +\\\\n+ . . . Un (t)  U  (t). After incorporating the\\\\narrivals in slot t + 1, this gives U1 (t + 1) + U2 (t + 1) +\\\\n. . . Un (t + 1)  U (t + 1). Since this is true for all t, we\\\\nwrite it as:\\\\nU1 (t) + U2 (t) + . . . Un (t)  U (t)\\\\n\\\\nblock in the right bottom corner does not affect the linear\\\\nindependence. Hence, H(t) also has linearly independent\\\\nrows.\\\\nDefine the following:\\\\n:=\\\\n:=\\\\n:=\\\\n:=\\\\n:=\\\\n:=\\\\n\\\\nRow span of\\\\nRow span of\\\\nRow span of\\\\nnj=1 Uj (t)\\\\nRow span of\\\\nRow span of\\\\n\\\\nH(t)\\\\nBj (t)H(t)\\\\nBj (t)H(t)\\\\nB  (t)H(t)\\\\nBj (t)H(t)\\\\n\\\\nAll the vector spaces defined above are subspaces of\\\\nA(t)\\\\nFq . Figure 3 shows the points at which these subspaces\\\\nare defined in the slot.\\\\nThe fact that H(t) has full row rank (proved above in\\\\nLemma 3) implies that the operations performed by the\\\\nalgorithm in the domain of the local coefficient vectors\\\\ncan be mapped to the corresponding operations in the\\\\ndomain of the global coefficient vectors:\\\\n (t) is indeed the row\\\\n1) The intersection subspace U\\\\nspan of B (t)H(t).\\\\n2) Let Rj (t) be an indicator (0-1) random variable\\\\nwhich takes the value 1 iff the transmission in\\\\nslot t is successfully received without erasure by\\\\nreceiver j and in addition, receiver j does not\\\\nhave all the information that the sender has. Let\\\\ng j (t) := Rj (t)g(t)H(t). Then,\\\\n\\\\nUj (t) = Uj (t)  span(g j (t))\\\\n\\\\n(7)\\\\n\\\\nU2 (t)\\\\n\\\\nFig. 3. The main steps of the algorithm, along with the times at\\\\nwhich the various U (t)s are defined\\\\n\\\\nU (t)\\\\nUj (t)\\\\nUj (t)\\\\n (t)\\\\nU\\\\nU  (t)\\\\nUj (t)\\\\n\\\\n(6)\\\\n\\\\n(4)\\\\n\\\\nwhere  denotes direct sum of vector spaces. The\\\\nway the algorithm chooses g(t) guarantees that\\\\nif Rj (t) is non-zero, then g j (t) will be outside\\\\nthe corresponding Uj (t), i.e., it will be innovative.\\\\nThis fact is emphasized by the direct sum in this\\\\nequation.\\\\n3) Because of the way the algorithm performs the\\\\ncompletion of the bases in the local domain in\\\\nstep 6, the following properties hold in the global\\\\n\\\\n(8)\\\\n\\\\nNow, in order to relate the queue size to the backlog in\\\\nnumber of degrees of freedom, we define the following\\\\nvector spaces which represent the cumulative knowledge\\\\nof the sender and receivers (See Figure 3 for the timing):\\\\nV (t)\\\\n\\\\n:=\\\\n\\\\nVj (t)\\\\n\\\\n:=\\\\n\\\\nVj (t)\\\\n\\\\n:=\\\\n\\\\nV (t)\\\\nV (t)\\\\n\\\\n:=\\\\n:=\\\\n\\\\nSenders knowledge space after incorporating the arrivals (at the end of step 3)\\\\nA(t)\\\\nin slot t. This is simply equal to Fq\\\\nReceiver j s knowledge space at the end\\\\nof step 3 in slot t\\\\nReceiver j s knowledge space in slot\\\\nt, after incorporating the channel state\\\\nfeedback into Vj (t), i.e., Vj (t) = Vj (t)\\\\nspan(g j (t)).\\\\nnj=1 Vj (t)\\\\nnj=1 Vj (t)\\\\n\\\\nFor completeness, we now prove the following facts\\\\nabout direct sums of vector spaces that we will use.\\\\nLemma 4: Let V be a vector space and let\\\\nV , U1 , U2 , . . . Un be subspaces of V such that, V is\\\\nindependent of the span of all the Uj s, i.e., dim[V \\\\n(U1 + U2 + . . . + Un )] = 0. Then,\\\\nV  [ni=1 Ui ] = ni=1 [V  Ui ]\\\\n\\\\nSee Appendix B for the proof.\\\\nLemma 5: Let A, B, and C be three vector spaces\\\\nsuch that B is independent of C and A is independent\\\\nof B  C . Then the following hold:\\\\n1) A is independent of B .\\\\n2) A  B is independent of C .\\\\n3) A  (B  C) = (A  B)  C .\\\\nSee Appendix C for the proof.\\\\nTheorem 3: For all t  0,\\\\nV (t) = V (t)  U (t)\\\\nVj (t) = V (t)  Uj (t)\\\\nV (t)\\\\n\\\\n= V (t) \\\\n\\\\n\\\\nU\\\\n(t)\\\\n\\\\nj = 1, 2, . . . n\\\\n\\\\n\\\\f13\\\\n\\\\nProof: The proof is by induction on t.\\\\nBasis step:\\\\nAt t = 0, V (0), U (0) as well as all the Vj (0)s and\\\\nUj (0)s are initialized to {0}. Consequently, V (0) is\\\\nalso {0}. It is easily seen that these initial values satisfy\\\\nthe equations in the theorem statement.\\\\n\\\\nV (t) = V (t)  U (t)\\\\n\\\\n(9)\\\\n\\\\nVj (t) = V (t)  Uj (t), j = 1, 2, . . . n\\\\n\\\\nV (t) = V (t)  U\\\\n(t)\\\\n\\\\n(10)\\\\n(11)\\\\n\\\\nInduction Step: We now prove that they hold in slot (t +\\\\n1). We have:\\\\nV (t)\\\\n=\\\\n=\\\\n=\\\\n\\\\n(from (9))\\\\n\\\\n\\\\nV (t)  [U\\\\n(t)  U  (t)]\\\\n\\\\n[V (t)  U\\\\n(t)]  U  (t)\\\\nV (t)  U  (t)\\\\n\\\\n(from (5))\\\\n(Lemma 5)\\\\n(from (11))\\\\n\\\\nThus, we have proved:\\\\nV (t) = V (t)  U  (t)\\\\n\\\\n(12)\\\\n\\\\nNow, we incorporate the arrivals in slot (t + 1). This\\\\nconverts V (t) to V (t+1), U  (t) to U (t+1), and V (t)\\\\nto V (t + 1), due to the following operations:\\\\nBasis of V (t + 1) =\\\\nBasis of U (t + 1)\\\\nBasis of V (t + 1)\\\\n\\\\nh\\\\n\\\\n=\\\\n\\\\n\\\\\"\\\\n\\\\n=\\\\n\\\\n\\\\\"\\\\n\\\\nBasis of V (t) 0\\\\n\\\\nVj (t + 1) = V (t + 1)  Uj (t + 1),\\\\n\\\\nj = 1, 2, . . . n\\\\n\\\\nAnd finally,\\\\nV (t + 1)\\\\n\\\\nInduction Hypothesis:\\\\nWe assume the equations hold at t, i.e.,\\\\n\\\\n= V (t)  U (t)\\\\n\\\\nVj (t), V (t), and Uj (t), thereby converting them into\\\\nbases of Vj (t + 1), V (t + 1), and Uj (t + 1) respectively.\\\\nThese changes do not affect the above relation, and we\\\\nget:\\\\n\\\\ni\\\\n\\\\nBasis of U  (t)\\\\n0\\\\n0\\\\nIa(t+1)\\\\nBasis of V (t)\\\\n0\\\\n0\\\\nIa(t+1)\\\\n\\\\n#\\\\n#\\\\n\\\\nIncorporating these modifications into (12), we get:\\\\nV (t + 1) = V (t + 1)  U (t + 1)\\\\n\\\\nNow, consider each receiver j = 1, 2, . . . n.\\\\nVj (t)\\\\n\\\\n=\\\\n\\\\nnj=1 Vj (t + 1)\\\\n\\\\n=\\\\n\\\\nnj=1 [Vj (t + 1)  span(g j (t + 1))]\\\\n\\\\n=\\\\n\\\\nnj=1 [V (t + 1)  Uj (t + 1)  span(g j (t + 1))]\\\\n\\\\n(a)\\\\n\\\\n=\\\\n\\\\nV (t + 1)  nj=1 [Uj (t + 1)  span(g j (t + 1))]\\\\n\\\\n=\\\\n\\\\n\\\\nV (t + 1)  U\\\\n(t + 1)\\\\n\\\\nStep (a) is justified as follows. Using equation (8) and\\\\nthe fact that g j (t+1) was chosen to be inside U (t+1), we\\\\ncan show that the span of all the [Uj (t+1)span(g j (t+\\\\n1))]s is inside U (t + 1). Now, from the induction step\\\\nabove, V (t + 1) is independent of U (t + 1). Therefore,\\\\nV (t + 1) is independent of the span of all the [Uj (t +\\\\n1)  span(g j (t + 1))]s. We can therefore apply Lemma\\\\n4.\\\\nTheorem 4: Let Q(t) denote the size of the queue after\\\\nthe arrivals in slot t have been appended to the queue.\\\\nQ(t) = dim V (t)  dim V (t)\\\\n\\\\nProof:\\\\nQ(t) = dim U (t) = dim U  (t  1) + a(t)\\\\n\\\\n= dim U (t  1)  dim U\\\\n(t  1) + a(t)\\\\n\\\\n(using (5)\\\\n\\\\n= dim V (t  1)  dim V (t  1)  dim U\\\\n(t) + a(t)\\\\n\\\\n(from Theorem 3)\\\\n= dim V (t  1)  dim V (t) + a(t)\\\\n\\\\n(from Theorem 3)\\\\n= dim V (t)  dim V (t)\\\\n\\\\n= Vj (t)  span(g j (t))\\\\n= [V (t)  Uj (t)]  span(g j (t))\\\\n\\\\n(from (10))\\\\n\\\\n= V (t)  [Uj (t)  span(g j (t))]\\\\n\\\\n(Lemma 5)\\\\n\\\\n=\\\\n=\\\\n=\\\\n=\\\\n\\\\nV (t)  Uj (t)\\\\n\\\\nV (t)  [U\\\\n(t)  Uj (t)]\\\\n\\\\n[V (t)  U\\\\n(t)]  Uj (t)\\\\nV (t)  Uj (t)\\\\n\\\\nLemma 6: Let V1 , V2 , . . . , Vk be subspaces of a vector\\\\nspace V . Then, for k  1,\\\\n\\\\n(from (4))\\\\n(from (6))\\\\n(Lemma 5)\\\\n(from (11))\\\\n\\\\nIncorporating the new arrivals into the subspaces involves adding a(t + 1) all-zero columns to the bases of\\\\n\\\\ndim(V1  V2  . . .  Vk ) \\\\n\\\\nk\\\\nX\\\\n\\\\ndim(Vi )  (k  1)dim(V )\\\\n\\\\ni=1\\\\n\\\\nProof: For any two subspaces X and Y of V ,\\\\ndim(X  Y ) + dim(X + Y ) = dim(X) + dim(Y )\\\\n\\\\nwhere X + Y denotes the span of subspaces X and Y .\\\\n\\\\n\\\\f14\\\\n\\\\nlinear combinations of packets in the queue like in\\\\nAlgorithm 2 (a). Instead only original packets need to be\\\\ndim(X  Y ) = dim(X) + dim(Y )  dim(X + Y )\\\\nstored, and the queue can be operated in a simple first dim(X) + dim(Y )  dim(V )\\\\n(13)in-first-out manner. We now present some mathematical\\\\n(since X + Y is also a subspace of V ) preliminaries before describing the algorithm.\\\\n1) Some preliminaries: The newly proposed algoNow, we prove the lemma by induction on k.\\\\nrithm uses the notion of reduced row echelon form\\\\nBasis step:\\\\n(RREF) of a matrix to represent the knowledge of a\\\\nk = 1 : LHS = dim(V1 ), RHS = dim(V1 )\\\\nreceiver. Hence, we first recapitulate the definition and\\\\nk = 2 : LHS = dim(V1  V2 ), RHS = dim(V1 ) +\\\\nsome properties of the RREF from [19], and present the\\\\ndim(V2 )  dim(V )\\\\nconnection between the RREF and the notion of seeing\\\\nThe claim follows from inequality (13).\\\\npackets.\\\\nInduction Hypothesis:\\\\nDefinition 8 (Reduced row echelon form (RREF)): A\\\\nFor some arbitrary k,\\\\nmatrix is said to be in reduced row echelon form if it\\\\nk1\\\\nsatisfies the following conditions:\\\\nX\\\\nk1\\\\ndim(i=1\\\\nVi ) \\\\ndim(Vi )  (k  2)dim(V )\\\\n1) The first nonzero entry of every row is 1.\\\\ni=1\\\\n2) The first nonzero entry of any row is to the right\\\\nInduction Step:\\\\nof the first nonzero entry of the previous row.\\\\nk1\\\\nVi )\\\\ndim(ki=1 Vi ) = dim(Vk  i=1\\\\n3) The entries above the first nonzero row of any row\\\\nk1\\\\nare all zero.\\\\n dim(Vk ) + dim(i=1\\\\nVi )  dim(V ) (using (13))\\\\n#\\\\n\\\\\"k1\\\\nThe\\\\nRREF leads to a standard way to represent a vecX\\\\ntor\\\\nspace.\\\\nGiven a vector space, consider the following\\\\ndim(Vi )  (k  2)dim(V )\\\\n dim(Vk ) +\\\\noperation  arrange the basis vectors in any basis of\\\\ni=1\\\\ndim(V )\\\\nthe space as the rows of a matrix, and perform Gaussian\\\\nk\\\\nelimination. This process essentially involves a sequence\\\\nX\\\\n=\\\\ndim(Vi )  (k  1)dim(V )\\\\nof elementary row transformations and it produces a\\\\ni=1\\\\nunique matrix in RREF such that its row space is the\\\\nThe above result can be rewritten as:\\\\ngiven vector space. We call this the RREF basis matrix\\\\nk\\\\nof the space. We will use this representation for the\\\\nX\\\\ndim(V )dim(V1 V2 . . . Vk ) \\\\n[dim(V )dim(Vi )] knowledge space of the receivers.\\\\ni=1\\\\nLet V be the knowledge space of some receiver. Sup(14)\\\\npose m packets have arrived at the sender so far. Then the\\\\nreceivers knowledge consists of linear combinations of\\\\nUsing this result, we can now prove Theorem 2.\\\\nsome collection of these m packets, i.e., V is a subspace\\\\nProof of Theorem 2: If we apply Lemma 6 to the\\\\nof Fm\\\\nq . Using the procedure outlined above, we can\\\\nvector spaces Vj (t), j = 1, 2, . . . , n and V (t), then the\\\\ncompute the dim(V )  m RREF basis matrix of V over\\\\nleft hand side of inequality (14) becomes the sender\\\\nFq .\\\\nqueue size (using Theorem 4), while the right hand side\\\\nIn the RREF basis, the first nonzero entry of any row\\\\nbecomes the sum of the differences in backlog between\\\\nis called a pivot. Any column with a pivot is called\\\\nthe sender and the receivers, in terms of the number of\\\\na pivot column. By definition, each pivot occurs in a\\\\ndegrees of freedom. Thus, we have proved Theorem 2.\\\\ndifferent column. Hence, the number of pivot columns\\\\nequals the number of nonzero rows, which is dim[V ].\\\\nLet pk denote the packet with index k. The columns\\\\nC. Algorithm 2 (b): Drop when seen\\\\nare ordered so that column k maps to packet pk . The\\\\nThe drop-when-seen algorithm can be viewed as a following theorem connects the notion of seeing packets\\\\nspecialized variant of the generic Algorithm 2 (a) given to the RREF basis.\\\\nabove. It uses the notion of seen packets (defined in SecTheorem 5: A node has seen a packet with index k if\\\\ntion II) to represent the bases of the knowledge spaces. and only if the kth column of the RREF basis B of the\\\\nThis leads to a simple and easy-to-implement version knowledge space V of the node is a pivot column.\\\\nProof: The if part is clear. If column k of B\\\\nof the algorithm which, besides ensuring that physical\\\\nqueue size tracks virtual queue size, also provides some is a pivot column, then the corresponding pivot row\\\\npractical benefits. For instance, the sender need not store corresponds to a linear combination known to the node,\\\\nHence,\\\\n\\\\n\\\\f15\\\\n\\\\nof the form pk + q, where q involves only packets with\\\\nindex more than k. Thus, the node has seen pk .\\\\nFor the only if part, suppose column k of B does not\\\\ncontain a pivot. Th", "Experiment": "en, in any linear combination of the\\\\nrows, rows with pivot after column k cannot contribute\\\\nanything to column k. Rows with pivot before column k\\\\nwill result in a non-zero term in some column to the left\\\\nof k. Since every vector in V is a linear combination of\\\\nthe rows of B , the first non-zero term of any vector in\\\\nV cannot be in column k. Thus, pk could not have been\\\\nseen.\\\\nSince the number of pivot columns is equal to the\\\\ndimension of the vector space, we obtain the following\\\\ncorollary.\\\\nCorollary 1: The number of packets seen by a receiver is equal to the dimension of its knowledge space.\\\\nThe next corollary introduces a useful concept.\\\\nCorollary 2: If receiver j has seen packet pk , then it\\\\nknows exactly one linear combination of the form pk +q\\\\nsuch that q involves only unseen packets with index more\\\\nthan k.\\\\nProof: We use the same notation as above. The\\\\nreceiver has seen pk . Hence, column k in B is a pivot\\\\ncolumn. By definition of RREF, in the row containing the\\\\npivot in column k, the pivot value is 1 and subsequent\\\\nnonzero terms occur only in non-pivot columns. Thus,\\\\nthe corresponding linear combination has the given form\\\\npk +q, where q involves only unseen packets with index\\\\nmore than k.\\\\nWe now prove uniqueness by contradiction. Suppose\\\\nthe receiver knows another such linear combination pk +\\\\nq where q also involves only unseen packets. Then, the\\\\nreceiver must also know (q  q ). But this means the\\\\nreceiver has seen some packet involved in either q or q\\\\n a contradiction.\\\\nDefinition 9 (Witness): We denote the unique linear\\\\ncombination guaranteed by Corollary 2 as Wj (pk ), the\\\\nwitness for receiver j seeing pk .\\\\n2) Description of Algorithm 2 (b): The central idea\\\\nof the algorithm is to keep track of seen packets instead\\\\nof decoded packets. The two main parts of the algorithm\\\\nare the coding and queue update modules.\\\\nIn Section IV-C5, we present the formal description\\\\nof our coding module. The coding module computes a\\\\nlinear combination g that will cause any receiver that\\\\nreceives it, to see its next unseen packet. First, for each\\\\nreceiver, the sender computes its knowledge space using\\\\nthe feedback and picks out its next unseen packet. Only\\\\nthese packets will be involved in g, and hence we call\\\\nthem the transmit set. Now, we need to select coefficients\\\\nfor each packet in this set. Clearly, the receiver(s) waiting\\\\nto see the oldest packet in the transmit set (say p1 ) will\\\\n\\\\nbe able to see it as long as its coefficient is not zero.\\\\nConsider a receiver that is waiting to see the second\\\\noldest packet in the transmit set (say p2 ). Since the\\\\nreceiver has already seen p1 , it can subtract the witness\\\\nfor p1 , thereby canceling it from g. The coefficient of\\\\np2 must be picked such that after subtracting the witness\\\\nfor p1 , the remaining coefficient of p2 in g is nonzero. The same idea extends to the other coefficients.\\\\nThe receiver can cancel packets involved in g that it\\\\nhas already seen by subtracting suitable multiples of the\\\\ncorresponding witnesses. Therefore, the coefficients for\\\\ng should be picked such that for each receiver, after\\\\ncanceling the seen packets, the remaining coefficient of\\\\nthe next unseen packet is non-zero. Then, the receiver\\\\nwill be able to see its next unseen packet. Theorem 8\\\\nproves that this is possible if the field size is at least\\\\nn, the number of receivers. With two receivers, the\\\\ncoding module is a simple XOR based scheme (see Table\\\\nI). Our coding scheme meets the innovation guarantee\\\\nrequirement because Theorem 5 implies that a linear\\\\ncombination that would cause a new packet to be seen\\\\nbrings in a previously unknown degree of freedom.\\\\nThe fact that the coding module uses only the next unseen packet of all receivers readily implies the following\\\\nqueue update rule. Drop a packet if all receivers have\\\\nseen it. This simple rule ensures that the physical queue\\\\nsize tracks the virtual queue size.\\\\nRemark 2: In independent work, [28] proposes a coding algorithm which uses the idea of selecting those\\\\npackets for coding, whose indices are one more than\\\\neach receivers rank. This corresponds to choosing the\\\\nnext unseen packets in the special case where packets\\\\nare seen in order. Moreover, this algorithm picks coding\\\\ncoefficients in a deterministic manner, just like our\\\\ncoding module. Therefore, our module is closely related\\\\nto the algorithm of [28].\\\\nHowever, our algorithm is based on the framework of\\\\nseen packets. This allows several benefits. First, it immediately leads to the drop-when-seen queue management\\\\nalgorithm, as described above. In contrast, [28] does not\\\\nconsider queuing aspects of the problem. Second, in this\\\\nform, our algorithm readily generalizes to the case where\\\\nthe coding coefficients are picked randomly. The issue\\\\nwith random coding is that packets may be seen out\\\\nof order. Our algorithm will guarantee innovation even\\\\nin this case (provided the field is large), by selecting a\\\\nrandom linear combination of the next unseen packets\\\\nof the receivers. However, the algorithm of [28] may\\\\nnot work well here, as it may pick packets that have\\\\nalready been seen, which could cause non-innovative\\\\ntransmissions.\\\\nThe compatibility of our algorithm with random cod-\\\\n\\\\n\\\\f16\\\\n\\\\ning makes it particularly useful from an implementation\\\\nperspective. With random coding, each receiver only\\\\nneeds to inform the sender the set of packets it has\\\\nseen. There is no need to convey the exact knowledge\\\\nspace. This can be done simply by generating a TCP-like\\\\ncumulative ACK upon seeing a packet. Thus, the ACK\\\\nformat is the same as in traditional ARQ-based schemes.\\\\nOnly its interpretation is different.\\\\nWe next present the formal description and analysis\\\\nof the queue update algorithm.\\\\n3) The queuing module: The algorithm works with\\\\nthe RREF bases of the receivers knowledge spaces. The\\\\ncoefficient vectors are with respect to the current queue\\\\ncontents and not the original packet stream.\\\\nAlgorithm 2 (b)\\\\n1. Initialize matrices B1 , B2 , . . . , Bn to the empty\\\\nmatrix. These matrices will hold the bases of the\\\\nincremental knowledge spaces of the receivers.\\\\n2. Incorporate new arrivals: Suppose there are a new\\\\narrivals. Add the new packets to the end of the\\\\nqueue. Append a all-zero columns on the right to\\\\neach Bj for the new packets.\\\\n3. Transmission: If the queue is empty, do nothing;\\\\nelse compute g using the coding module and\\\\ntransmit it.\\\\n4. Incorporate channel state feedback:\\\\nFor every receiver j = 1 to n, do:\\\\nIf receiver j received the transmission, include the\\\\ncoefficient vector of g in terms of the current queue\\\\ncontents, as a new row in Bj . Perform Gaussian\\\\nelimination.\\\\n5. Separate out packets that all receivers have seen:\\\\nUpdate the following sets and bases:\\\\nSj := Set of packets corresponding to the pivot\\\\ncolumns of Bj\\\\n := n S \\\\nS\\\\nj=1 j\\\\nNew Bj := Sub-matrix of current Bj obtained by\\\\n and corresponding pivot\\\\nexcluding columns in S\\\\nrows.\\\\n .\\\\n6. Update the queue: Drop the packets in S\\\\n7. Go back to step 2 for the next slot.\\\\n4) Connecting the physical and virtual queue sizes:\\\\nThe following theorem describes the asymptotic growth\\\\nof the expected physical queue size under our new\\\\nqueuing rule.\\\\nTheorem 6: For Algorithm 2 (b), the physical queue\\\\nsize at the sender is upper-bounded by the sum of\\\\nthe virtual queue sizes, i.e., the sum of the degrees-offreedom backlog between the sender and the receivers.\\\\nHence, the expected size of the physical queue in steady\\\\n\\\\nstate for Algorithm 2 (b) is O\\\\n\\\\n\\\\u0010\\\\n\\\\n1\\\\n1\\\\n\\\\n\\\\u0011\\\\n\\\\n.\\\\n\\\\nIn the rest of this section, we will prove the above\\\\nresult. Now, in order to relate the queue size to the\\\\nbacklog in number of degrees of freedom, we will need\\\\nthe following notation:\\\\nS(t) := Set of packets arrived at sender till the end of\\\\nslot t\\\\nV (t) := Senders knowledge space after incorporating\\\\n|S(t)|\\\\nthe arrivals in slot t. This is simply equal to Fq\\\\nVj (t) := Receiver j s knowledge space at the end of slot\\\\nt. It is a subspace of V (t).\\\\nSj (t) := Set of packets receiver j has seen till end of\\\\nslot t\\\\nWe will now formally argue that Algorithm 2 (b)\\\\nindeed implements the drop-when-seen rule in spite of\\\\nthe incremental implementation. In any slot, the columns\\\\nof Bj are updated as follows. When new packets are\\\\nappended to the queue, new columns are added to Bj\\\\non the right. When packets are dropped from the queue,\\\\ncorresponding columns are dropped from Bj . There is\\\\nno rearrangement of columns at any point. This implies\\\\nthat a one-to-one correspondence is always maintained\\\\nbetween the columns of Bj and the packets currently\\\\nin the queue. Let Uj (t) be the row space of Bj at\\\\ntime t. Thus, if (u1 , u2 , . . . , uQ(t) ) is any vector in\\\\nUj (t), it corresponds to a linear combination of the form\\\\nPQ(t)\\\\nth packet in the queue at\\\\ni=1 ui pi , where pi is the i\\\\ntime t. The following theorem connects the incremental\\\\nknowledge space Uj (t) to the cumulative knowledge\\\\nspace Vj (t).\\\\nTheorem 7: In Algorithm 2 (b), for each receiver j ,\\\\nat the end of slot t, for any u  Uj (t), the linear\\\\nPQ(t)\\\\ncombination\\\\ni=1 ui pi is known to the receiver j ,\\\\nwhere pi denotes the ith packet in the queue at time\\\\nt.\\\\nProof: We will use induction on t. For t = 0,\\\\nthe system is completely empty and the statement is\\\\nvacuously true. Let us now assume that the statement is\\\\ntrue at time (t  1). Consider the operations in slot t. A\\\\nnew row is added to Bj only if the corresponding linear\\\\ncombination has been successfully received by receiver\\\\nj . Hence, the statement is still true. Row operations\\\\ninvolved in Gaussian elimination do not alter the row\\\\nspace. Finally, when some of the pivot columns are\\\\ndropped along with the corresponding pivot rows in\\\\nstep 5, this does not affect the linear combinations to\\\\nwhich the remaining rows correspond because the pivot\\\\ncolumns have a 0 in all rows except the pivot row.\\\\nHence, the three operations that are performed between\\\\nslot (t  1) and slot t do not affect the property that\\\\n\\\\n\\\\f17\\\\n\\\\n=\\\\n=\\\\n=\\\\n\\\\n=\\\\n\\\\nPn\\\\n\\\\n\\\\u0002\\\\n\\\\n\\\\u0003\\\\n\\\\ndim[V (t)]  dim[Vj (t)] , which is the sum of the\\\\nvirtual queue sizes.\\\\nFinally, we can find the asymptotic behavior of the\\\\nphysical queue size in steady state under Algorithm 2\\\\n(b). Since\\\\u0010 the \\\\u0011expected virtual queue sizes themselves\\\\n1\\\\nare all O 1\\\\nfrom Equation (2), we obtain the stated\\\\nresult.\\\\n5) The coding module: We now present a coding\\\\nmodule that is compatible with the drop-when-seen\\\\nqueuing algorithm in the sense that it always forms a\\\\nlinear combination using packets that are currently in the\\\\nqueue maintained by the queuing module. In addition,\\\\nwe show that the coding module satisfies the innovation\\\\nguarantee property.\\\\nLet {u1 , u2 , . . . , um } be the set of indices of the next\\\\nunseen packets of the receivers, sorted in ascending order\\\\n(In general, m  n, since the next unseen packet may be\\\\nthe same for some receivers). Exclude receivers whose\\\\nnext unseen packets have not yet arrived at the sender.\\\\nLet R(ui ) be the set of receivers whose next unseen\\\\npacket is pui . We now present the coding module to\\\\nselect the linear combination for transmission.\\\\n1) Loop over next unseen packets\\\\nFor j = 1 to m, do:\\\\nAll receivers in R(uj ) have seen packets pui\\\\nk\\\\nX\\\\nk\\\\nfor i < j . Now, r  R(uj ), find yr :=\\\\n|A|  | i=1 Ai | \\\\n(|A|  |Ai |)\\\\n(15)\\\\nPj1\\\\ni=1\\\\ni=1 i Wr (pui ), where Wr (pui ) is the witness\\\\nfor\\\\nreceiver r seeing pui . Pick j  Fq such that\\\\nProof:\\\\nj is different from the coefficient of puj in yr\\\\n|A|  | ki=1 Ai |\\\\nfor each r  R(uj ).\\\\nP\\\\n|A  (ki=1 Ai )c | (since the Ai s are subsets of A)\\\\n2) Compute the transmit packet: g := m\\\\ni=1 i pui\\\\nk\\\\nc\\\\nIt is easily seen that this coding module is compatible\\\\n|A  (i=1 Ai )| (by De Morgans law)\\\\nwith\\\\nthe drop-when-seen algorithm. Indeed, it does not\\\\nk\\\\nc\\\\n| i=1 (A  Ai )| (distributivity)\\\\nuse\\\\nany\\\\npacket that has been seen by all receivers in\\\\nk\\\\nX\\\\nc\\\\nthe linear combination. It only uses packets that at least\\\\n|A  Ai | (union bound)\\\\none receiver has not yet seen. The queue update module\\\\ni=1\\\\nk\\\\nretains precisely such packets in the queue. The next\\\\nX\\\\n(|A|  |Ai |)\\\\ntheorem presents a useful property of the coding module.\\\\n\\\\nthe vectors in the row space of Bj correspond to linear\\\\ncombinations that are known at receiver j . This proves\\\\nthe theorem.\\\\nIf a packet corresponds to a pivot column in Bj ,\\\\nthe corresponding pivot row is a linear combination of\\\\nthe packet in question with packets that arrived after it.\\\\nFrom the above theorem, receiver j knows this linear\\\\ncombination which means it has seen the packet. This\\\\nleads to the following corollary.\\\\nCorollary 3: If a packet corresponds to a pivot column in Bj , then it has been seen by receiver j .\\\\n (t) consists of those packets in\\\\nThus, in step 5, S\\\\nthe queue that all receivers have seen by the end of\\\\nslot t. In other words, the algorithm retains only those\\\\npackets that have not yet been seen by all receivers. Even\\\\nthough the algorithm works with an incremental version\\\\nof the knowledge spaces, namely Uj (t), it maintains\\\\nthe queue in the same way as if it was working with\\\\nthe cumulative version Vj (t). Thus, the incremental\\\\napproach is equivalent to the cumulative approach.\\\\nWe will require the following lemma to prove the main\\\\ntheorem.\\\\nLemma 7: Let A1 , A2 , . . . , Ak be subsets of a set A.\\\\nThen, for k  1,\\\\n\\\\nj=1\\\\n\\\\ni=1\\\\n\\\\nNow, we are ready to prove Theorem 6.\\\\nProof of Theorem 6: Since the only packets in the\\\\nqueue at any point are those that not all receivers have\\\\nseen, we obtain the following expression for the physical\\\\nqueue size at the sender at the end of slot t:\\\\nQ(t) = |S(t)|  | nj=1 Sj (t)|\\\\n\\\\nIf we apply Lemma 7 to the sets S(t) and Sj (t), j =\\\\n1, 2, . . . , n then the left hand side of inequality (15)\\\\nbecomes the sender queue size Q(t) given above. Now,\\\\n|Sj (t)| = dim[Vj (t)], using Corollary 1. Hence the\\\\nright hand side of inequality (15) can be rewritten as\\\\n\\\\nTheorem 8: If the field size is at least n, then the\\\\ncoding module picks a linear combination that will cause\\\\nany receiver to see its next unseen packet upon successful\\\\nreception.\\\\nProof: First we show that a suitable choice always\\\\nexists for j that satisfies the requirement in step 1. For\\\\nr  R(u1 ), yr = 0. Hence, as long as 1 6= 0, the\\\\ncondition is satisfied. So, pick 1 = 1. Since at least one\\\\nreceiver is in R(u1 ), we have that for j > 1, |R(uj )| \\\\n(n  1). Even if each yr for r  R(uj ) has a different\\\\ncoefficient for puj , that covers only (n1) different field\\\\nelements. If q  n, then there is a choice left in Fq for\\\\nj .\\\\n\\\\n\\\\f18\\\\n\\\\nNow, we have to show that the condition given in step\\\\n1 implies that the receivers will be able to see their next\\\\nunseen packet. Indeed, for all j from 1 to m, and for\\\\nall r  R(uj ), receiver r knows yr , since it is a linear\\\\ncombination of witnesses of r . Hence, if r successfully\\\\nreceives g, it can compute (g  yr ). Now, g and yr have\\\\nthe same coefficient for all packets with index less than\\\\nuj , and a different coefficient for puj . Hence, (g  yr )\\\\nwill involve puj and only packets with index beyond uj .\\\\nThis means r can see puj and this completes the proof.\\\\nTheorem 5 implies that seeing an unseen packet\\\\ncorresponds to receiving an unknown degree of freedom.\\\\nThus, Theorem 8 essentially says that the innovation\\\\nguarantee property is satisfied and hence the scheme is\\\\nthroughput optimal.\\\\nThis theorem is closely related to the result derived\\\\nin [28] that computes the minimum field size needed to\\\\nguarantee innovation. The difference is that our result\\\\nuses the framework of seen packets to make a more\\\\ngeneral statement by specifying not only that innovation\\\\nis guaranteed, but also that packets will be seen in\\\\norder with this deterministic coding scheme. This means\\\\npackets will be dropped in order at the sender.\\\\nV. OVERHEAD\\\\nIn this section, we comment on the overhead required\\\\nfor Algorithms 1 and 2 (b). There are several types of\\\\noverhead.\\\\nA. Amount of feedback\\\\nOur scheme assumes that every receiver feeds back\\\\none bit after every slot, indicating whether an erasure\\\\noccurred or not. In comparison, the drop-when-decoded\\\\nscheme requires feedback only when packets get decoded. However, in that case, the feedback may be more\\\\nthan one bit  the receiver will have to specify the list\\\\nof all packets that were decoded, since packets may\\\\nget decoded in groups. In a practical implementation\\\\nof the drop-when-seen algorithm, TCP-like cumulative\\\\nacknowledgments can be used to inform the sender\\\\nwhich packets have been seen.\\\\nB. Identifying the linear combination\\\\nBesides transmitting a linear combination of packets,\\\\nthe sender must also embed information that allows the\\\\nreceiver to identify what linear combination has been\\\\nsent. This involves specifying which packets have been\\\\ninvolved in the combination, and what coefficients were\\\\nused for these packets.\\\\n\\\\n1) Set of packets involved: The baseline algorithm\\\\nuses all packets in the queue for the linear combination. The queue is updated in a first-in-first-out (FIFO)\\\\nmanner, i.e., no packet departs before all earlier packets\\\\nhave departed. This is a consequence of the fact that\\\\nthe receiver signals successful decoding only when the\\\\nvirtual queue becomes empty5 . The FIFO rule implies\\\\nthat specifying the current contents of the queue in\\\\nterms of the original stream boils down to specifying\\\\nthe sequence number of the head-of-line packet and the\\\\nlast packet in the queue in every transmission.\\\\nThe drop-when-seen algorithm does not use all packets from the queue, but only at most n packets from the\\\\nqueue (the next unseen packet of each receiver). This set\\\\ncan be specified by listing the sequence number of these\\\\nn packets.\\\\nNow, in both cases, the sequence number of the\\\\noriginal stream cannot be used as it is, since it grows\\\\nunboundedly with time. However, we can avoid this\\\\nproblem using the fact that the queue contents are\\\\nupdated in a FIFO manner (This is also true of our\\\\ndrop-when-seen scheme  the coding module guarantees\\\\nthat packets will be seen in order, thereby implying a\\\\nFIFO rule for the senders queue.). The solution is to\\\\nexpress the sequence number relative to an origin that\\\\nalso advances with time, as follows. If the sender is\\\\ncertain that the receivers estimate of the senders queue\\\\nstarts at a particular point, then both the sender and\\\\nreceiver can reset their origin to that point, and then\\\\ncount from there.\\\\nFor the baseline case, the origin can be reset to the\\\\ncurrent HOL packet, whenever the receiver sends feedback indicating successful decoding. The idea is that if\\\\nthe receiver decoded in a particular slot, that means it had\\\\na successful reception in that slot. Therefore, the sender\\\\ncan be certain that the receiver must have received the\\\\nlatest update about the queue contents and is therefore in\\\\nsync with the sender. Thus, the sender and receiver can\\\\nreset their origin. Note that since the decoding epochs of\\\\ndifferent receivers may not be synchronized, the sender\\\\nwill have to maintain a different origin for each receiver\\\\nand send a different sequence number to each receiver,\\\\nrelative to that receivers origin. This can be done simply\\\\nby concatenating the sequence number for each receiver\\\\nin the header.\\\\nTo determine how many bits are needed to represent\\\\nthe sequence number, we need to find out what range of\\\\nvalues it can take. In the baseline scheme, the sequence\\\\nnumber range will be proportional to the busy period\\\\n5\\\\n\\\\nAs mentioned earlier in Remark 1, we assume that the sender\\\\nchecks whether any packets have been newly decoded, only when\\\\nthe virtual queue becomes empty.\\\\n\\\\n\\\\f19\\\\n\\\\nof the virtual queue, since this determines how often\\\\nthe origin is reset. Thus, the overhead in bits for each\\\\nreceiver will be proportional\\\\u0010 to the logarithm\\\\nof the\\\\n\\\\u0011\\\\n1\\\\nexpected busy period, i.e., O log2 1\\\\n.\\\\nFor the drop-when-seen scheme, the origin can be\\\\nreset whenever the receiver sends feedback indicating\\\\nsuccessful reception. Thus, the origin advances a lot\\\\nmore frequently than in the baseline scheme.\\\\n2) Coefficients used: The baseline algorithm uses a\\\\nrandom linear coding scheme. Here, potentially all packets in the queue get combined in a linear combination.\\\\nSo, in the worst case, the sender would have to send\\\\none coefficient for every packet in the queue. If the\\\\nqueue has m packets, this would require m log2 q bits,\\\\nwhere\\\\n\\\\u0010 q is\\\\u0011 the field size. In expectation, this would be\\\\nlog2 q\\\\nO (1)\\\\nbits. If the receiver knows the pseudorandom\\\\n2\\\\nnumber generator used by the sender, then it would be\\\\nsufficient for the sender to send the current state of\\\\nthe generator and the size of the queue. Using this,\\\\nthe receiver can generate the coefficients used by the\\\\nsender in the coding process. The new drop-when-seen\\\\nalgorithm uses a coding module which combines the next\\\\nunseen packet of each receiver. Thus, the overhead for\\\\nthe coefficients is at most n log2 q bits, where n is the\\\\nnumber of receivers. It does not depend on the load factor\\\\n at all.\\\\nC. Overhead at sender\\\\nWhile Algorithm 2 (b) saves in buffer space, it requires\\\\nthe sender to store the basis matrix of each receiver,\\\\nand update them in every slot based on feedback. However, storing a row of the basis matrix requires much\\\\nless memory than storing a packet, especially for long\\\\npackets. Thus, there is an overall saving in memory. The\\\\nupdate of the basis matrix simply involves one step of\\\\nthe Gaussian elimination algorithm.\\\\nD. Overhead at receiver\\\\nThe receiver will have to store the coded packets till\\\\nthey are decoded. It will also have to decode the packets.\\\\nFor this, the receiver can perform a Gaussian elimination\\\\nafter every successful reception. Thus, the computation\\\\nfor the matrix inversion associated with decoding can be\\\\nspread over time.\\\\nVI. D ECODING\\\\n\\\\nDELAY\\\\n\\\\nWith the coding module of Section IV-C5, although\\\\na receiver can see the next unseen packet in every\\\\nsuccessful reception, this does not mean the packet will\\\\nbe decoded immediately. In general, the receiver will\\\\n\\\\nhave to collect enough equations in the unknown packets\\\\nbefore being able to decode them, resulting in a delay.\\\\nWe consider two notions of delay in this paper:\\\\nDefinition 10 (Decoding Delay): The decoding delay\\\\nof a packet with respect to a receiver is the time that\\\\nelapses between the arrival of the packet at the sender\\\\nand the decoding of the packet by the receiver under\\\\nconsideration.\\\\nAs discussed in Section I, some applications can make\\\\nuse of a packet only if all prior packets have been\\\\ndecoded. In other words, the application will accept\\\\npackets only up to the front of contiguous knowledge.\\\\nThis motivates the following stronger notion of delay.\\\\nDefinition 11 (Delivery Delay): The delivery delay of\\\\na packet with respect to a receiver is the time that elapses\\\\nbetween the arrival of the packet at the sender and the\\\\ndelivery of the packet by the receiver to the application,\\\\nwith the constraint that packets may be delivered only\\\\nin order.\\\\nIt follows from these definitions that the decoding\\\\ndelay is always less than or equal to the delivery delay.\\\\nUpon decoding the packets, the receiver will place them\\\\nin a reordering buffer until they are delivered to the\\\\napplication.\\\\nIn this section, we study the expectation of these\\\\ndelays for an arbitrary packet. It can be shown using\\\\nergodic theory that the long term average of the delay\\\\nexperienced by the packets in steady state converges to\\\\nthis expectation with high probability. We focus on the\\\\nasymptotic growth of the expected delay as   1.\\\\nThe section is organized as follows. We first study\\\\nthe delivery delay behavior of Algorithms 1 and 2(b),\\\\nand provide an upper bound on the asymptotic expected\\\\ndelivery delay for any policy that satisfies the innovation guarantee property. We then present a generic\\\\nlower bound on the expected decoding delay. Finally,\\\\nwe present a new coding module for the case of three\\\\nreceivers which not only guarantees innovation, but also\\\\naims to minimize the delivery delay. We conjecture\\\\nthat this algorithm achieves a delivery delay whose\\\\nasymptotic growth matches that of the lower bound. This\\\\nbehavior is verified through simulations.\\\\nA. An upper bound on delivery delay\\\\nWe now present the upper bound on delay for policies that satisfy the innovation guarantee property. The\\\\narguments leading to this bound are presented below.\\\\nTheorem 9: The expected delivery delay of a packet\\\\nfor any coding module\\\\u0010 that satisfies\\\\nthe innovation\\\\n\\\\u0011\\\\n1\\\\nguarantee property is O (1)2 .\\\\n\\\\n\\\\f20\\\\n\\\\nLoad factor ()\\\\n3000\\\\n.94 .945\\\\n\\\\n2500\\\\n\\\\n.955\\\\n\\\\n.96\\\\n\\\\n.965\\\\n\\\\n.97\\\\n\\\\n.975\\\\n\\\\n.98\\\\n\\\\nTime till next decoding event (Simulation)\\\\nBusy period upper bound (Simulation)\\\\n[(1)]/[(1)2] (from Eqn. (3))\\\\n2\\\\n\\\\n0.37/(1)\\\\n\\\\n2000\\\\n\\\\nNumber of time slots\\\\n\\\\nFor any policy that satisfies the innovation guarantee\\\\nproperty, the virtual queue size evolves according to the\\\\nMarkov chain in Figure 2. The analysis of Algorithm 1 in\\\\nSection IV-A therefore applies to any coding algorithm\\\\nthat guarantees innovation.\\\\nAs explained in that section, the event of a virtual\\\\nqueue becoming empty translates to successful decoding\\\\nat the corresponding receiver, since the number of equations now matches the number of unknowns involved.\\\\nThus, an arbitrary packet that arrives at the sender will\\\\nget decoded by receiver j at or before the next emptying\\\\nof the j th virtual queue. In fact, it will get delivered to the\\\\napplication at or before the next emptying of the virtual\\\\nqueue. This is because, when the virtual queue is empty,\\\\nevery packet that arrived at the sender gets decoded.\\\\nThus, the front of contiguous knowledge advances to the\\\\nlast packet that the sender knows.\\\\nThe above discussion implies that Equation (3) gives\\\\nan upper bound on the expected delivery delay of an\\\\narbitrary packet. We thus obtain the result stated above.\\\\nWe next study the decoding delay of Algorithm 2 (b).\\\\nWe define the decoding event to be the event that all\\\\nseen packets get decoded. Since packets are always seen\\\\nin order, the decoding event guarantees that the front of\\\\ncontiguous knowledge will advance to the front of seen\\\\npackets.\\\\nWe use the term leader to refer to the receiver which\\\\nhas seen the maximum number of packets at the given\\\\npoint in time. Note that there can be more than one leader\\\\nat the same time. The following theorem characterizes\\\\nsufficient conditions for the decoding event to occur.\\\\nTheorem 10: The decoding event occurs in a slot at\\\\na particular receiver if in that slot:\\\\n(a) The receiver has a successful reception which\\\\nresults in an empty virtual queue at the sender;\\\\nOR\\\\n(b) The receiver has a successful reception and the\\\\nreceiver was a leader at the beginning of the slot.\\\\nProof: Condition (a) implies that the receiver has\\\\nseen all packets that have arrived at the sender up to\\\\nthat slot. Each packet at the sender is an unknown and\\\\neach seen packet corresponds to a linearly independent\\\\nequation. Thus, the receiver has received as many equations as the number of unknowns, and can decode all\\\\npackets it has seen.\\\\nSuppose condition (b) holds. Let pk be the next\\\\nunseen packet of the receiver in question. The senders\\\\ntransmitted linear combination will involve only the next\\\\nunseen packets of all the receivers. Since the receiver\\\\nwas a leader at the beginning of the slot, the senders\\\\ntransmission will not involve any packet beyond pk ,\\\\n\\\\n.95\\\\n\\\\n1500\\\\n\\\\n1000\\\\n\\\\n500\\\\n\\\\n0\\\\n15\\\\n\\\\n20\\\\n\\\\n25\\\\n\\\\n30\\\\n\\\\n35\\\\n\\\\n40\\\\n\\\\n45\\\\n\\\\n50\\\\n\\\\n1/(1)\\\\n\\\\nFig. 4. Delay to decoding event and upper bound for 2 receiver case,\\\\n1\\\\nas a function of (1)\\\\n. The corresponding values of  are shown on\\\\nthe top of the figure.\\\\n\\\\nsince the next unseen packet of all other receivers is\\\\neither pk or some earlier packet. After subtracting the\\\\nsuitably scaled witnesses of already seen packets from\\\\nsuch a linear combination, the leading receiver will end\\\\nup with a linear combination that involves only pk . Thus\\\\nthe leader not only sees pk , but also decodes it. In fact,\\\\nnone of the senders transmissions so far would have\\\\ninvolved any packet beyond pk . Hence, once pk has\\\\nbeen decoded, pk1 can also be decoded. This procedure\\\\ncan be extended to all unseen packets, and by induction,\\\\nwe can show that all unseen packets will be decoded.\\\\nThe upper bound proved in Theorem 9 is based on the\\\\nemptying of the virtual queues. This corresponds only to\\\\ncase (a) in Theorem 10. The existence of case (b) shows\\\\nthat in general, the decoding delay will be strictly smaller\\\\nthan the upper bound. A natural question is whether this\\\\ndifference is large enough to cause a different asymptotic\\\\nbehavior, i.e., does Algorithm 2 (b) achieve a delay that\\\\nasymptotically has a smaller exponent of growth than the\\\\nupper bound as   1? We conjecture that this is not the\\\\ncase, i.e.,\\\\nthe decoding delay for Algorithm 2 (b) is\\\\n\\\\u0010 that \\\\u0011\\\\n1\\\\nalso  (1)\\\\n, although the constant of proportionality\\\\n2\\\\nwill be smaller. For the two receiver case, based on our\\\\nsimulations, this fact seems to be true. Figure 4 shows\\\\nthe growth of the decoding delay averaged over a large\\\\n1\\\\n. The resulting\\\\nnumber of packets, as a function of (1)\\\\n0.37\\\\ncurve seems to be close to the curve (1)\\\\n2 , implying\\\\na quadratic growth. The value of  ranges from 0.95 to\\\\n0.98, while  is fixed to be 0.5. The figure also shows\\\\nthe upper bound based on busy period measurements.\\\\nThis curve agrees with the formula in Equation (3) as\\\\nexpected.\\\\n\\\\n\\\\f21\\\\n\\\\nU\\\\n\\\\nB. The lower bound\\\\nLemma 8: The\\\\nper-packet delay is lower\\\\n\\\\u0010 expected\\\\n\\\\u0011\\\\n1\\\\nbounded by  1\\\\nProof: The expected per-packet delay for the single\\\\nreceiver case is clearly a lower bound for the corresponding quantity at one of the receivers in a multiple-receiver\\\\nsystem. We will compute this lower bound in this section.\\\\nFigure 2 shows the Markov chain for the queue size\\\\nin the single receiver case. If  =  < 1, then the\\\\nchain is positive recurrent and the steady state expected\\\\n\\\\u0010\\\\n\\\\u0011\\\\n1\\\\n=\\\\n\\\\nqueue size can be computed to be (1)\\\\n(1)\\\\n1\\\\n(see Equation (1)). Now, if  < 1, then the system is\\\\nstable and Littles law can be applied to show that the\\\\nexpected \\\\u0010\\\\nper-packet\\\\ndelay in the single receiver system\\\\n\\\\u0011\\\\n1\\\\nis also  1\\\\n.\\\\nC. An alternate coding module for better delay\\\\nIn this section, we present a new coding module for\\\\nthe case of three receivers that significantly improves\\\\nthe delay performance compared to Algorithm 2 (b). In\\\\nparticular, we obtain 100% throughput and conjecture\\\\nthat the algorithm simultaneously achieves asymptotically optimal decoding delay by meeting the lower bound\\\\nof Lemma VI-B. The asymptotics here are in the realm of\\\\nthe load factor  tending to 1 from below, while keeping\\\\neither the arrival rate  or the channel quality parameter\\\\n fixed at a number less than 1.\\\\nWe introduce a new notion of packets that a node has\\\\nheard of.\\\\nDefinition 12 (Heard of a packet): A node is said to\\\\nhave heard of a packet if it knows some linear combination involving that packet.\\\\nThe new coding module\\\\nOur coding module works in the Galois field of size\\\\n3. At the beginning of every slot, the module has to\\\\ndecide what linear combination to transmit. Since there\\\\nis full feedback, the module is fully aware of the current\\\\nknowledge space of each of the three receivers. The\\\\ncoding algorithm is as follows:\\\\n1) Initialize L = 1, N = 2, D = 3, m = 0.\\\\n2) Compute the following sets for all receivers i =\\\\n1, 2, 3.\\\\nHi := Set of packets heard of by receiver i\\\\nDi := Set of", "Discussion": " packets decoded by receiver i\\\\n3) Define a universe set U consisting of packets p1\\\\nto pm , and also pm+1 if it has arrived. Compute\\\\nthe following sets6 (See Figure 5):\\\\n6\\\\nNotation: The subscripts N and D are simply indices. For\\\\nexample, DN is simply that Di for which i = N .\\\\n\\\\nS6\\\\nS5\\\\nDN\\\\n\\\\nDD\\\\nS4\\\\n\\\\nS1\\\\n\\\\nS2\\\\n\\\\nS3\\\\n\\\\nHD\\\\n\\\\nFig. 5.\\\\n\\\\nSets used by the coding module\\\\n\\\\nS1 = DN  DD\\\\nS2 = DN  (HD \\\\\\\\DD )\\\\n S3 = DN \\\\\\\\HD\\\\n S4 = DD \\\\\\\\DN\\\\n S5 = (HD \\\\\\\\DD )\\\\\\\\DN\\\\n S6 = U \\\\\\\\(HD  DN )\\\\n4) The coding module picks a linear combination\\\\ndepending on which of these sets pm+1 falls in,\\\\nas follows:\\\\nCase 1  pm+1 has not arrived: Check if both S2\\\\nand S4 are non-empty. If they are, pick the oldest\\\\npacket from each, and send their sum. If not, try\\\\nthe pair of sets S3 and S4 . If neither of these pairs\\\\nof sets work, then send the oldest packet in S5 if it\\\\nis non-empty. If not, try S6 , S2 , S3 and S4 in that\\\\norder. If all of these are empty, then send nothing.\\\\nCase 2  pm+1  S1 : This is identical to case 1,\\\\nexcept that pm+1 must also be added to the linear\\\\ncombination that case 1 suggests.\\\\nCase 3  pm+1  S2 : Send pm+1 added to\\\\nanother packet. The other packet is chosen to be\\\\nthe oldest packet in the first non-empty set in the\\\\nfollowing list, tested in that order: S4 , S5 , S6 . (In\\\\nthe case where pm+1  S2 , if the other packet p is\\\\nchosen from S5 , then both the chosen packets are\\\\nin HD \\\\\\\\DD . Therefore, the receiver D might know\\\\none (but not both) of (pm+1 + p) or (pm+1 + 2p).\\\\nHence, the coefficient for p in the transmitted\\\\ncombination must be selected to be either 1 or 2,\\\\nin such a way that the resulting linear combination\\\\nis innovative to receiver D .)\\\\nCase 4  pm+1  S3 : Send pm+1 added to another\\\\npacket. The other packet is chosen to be the oldest\\\\npacket in the first non-empty set in the following\\\\nlist, tested in that order: S4 , S5 , S6 .\\\\nCase 5  pm+1  S4 : Send pm+1 added to another\\\\npacket. The other packet is chosen to be the oldest\\\\npacket in the first non-empty set in the following\\\\nlist, tested in that order: S2 , S3 , S6 .\\\\nCase 6  All other cases: Send pm+1 as it is.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\f22\\\\n\\\\n5) Transmit the chosen linear combination and collect the feedback from all receivers. Using the\\\\nfeedback, update the sets Hi and Di for all the\\\\nreceivers.\\\\n6) Set the new value of m to be the maximum of\\\\nthe ranks of the three receivers. Identify the set\\\\nof receivers that have decoded all packets from 1\\\\nto m. If there is no such receiver, assign L, N\\\\nand D arbitrarily and go to step 3. (We show in\\\\nTheorem 11 that there will always be at least one\\\\nsuch receiver.)\\\\nIf there is more than one such receiver, pick the one\\\\nwith the lowest index to be L. Compute the unsolved set Ti := Hi \\\\\\\\Di for the other two receivers.\\\\nIf exactly one of them has a non-empty unsolved\\\\nset, pick that receiver to be D (for deficit), and\\\\nthe other one to be N (for no deficit7 ). If neither\\\\nhas an unsolved set or if both have an unsolved\\\\nset, assign D and N arbitrarily. (We show in\\\\nTheorem 12 that at most one of them will have a\\\\nnon-empty unsolved set.) Go to step 3.\\\\nD. Properties of the coding module\\\\nThe above algorithm aims to guarantee innovation\\\\nusing as little mixing of packets as possible. In this\\\\nsection, we state and prove some key properties of\\\\nthe coding module, including the innovation guarantee\\\\nproperty. In what follows, we use the notation m(t) to\\\\ndenote the maximum rank among the three receivers at\\\\nthe beginning of slot t.\\\\nLemma 9: For any t > 0, the transmission in any slot\\\\nfrom 1 to t does not involve a packet with index beyond\\\\nm(t) + 1.\\\\nProof: The proof is by induction on the slot number.\\\\nBasis step: If anything is sent in slot 1, it has to be p1 ,\\\\nsince all the sets except S6 are empty. Thus, as m(1) =\\\\n0, the statement holds.\\\\nInduction hypothesis: Suppose no transmission up to and\\\\nincluding slot t has involved packets beyond pm(t)+1 .\\\\nInduction step: Then at the beginning of slot (t + 1), the\\\\nsets S1 to S5 cannot contain packets beyond pm(t)+1 .\\\\nAlong with the definition of S6 and the fact that m(t +\\\\n1)  m(t), this statement implies that S1 to S6 cannot\\\\ncontain any packet with index beyond m(t + 1) + 1.\\\\nThe coding module combines pm(t+1)+1 with up to\\\\n2 other packets from these sets. Thus, the resulting\\\\ntransmission will not involve any packet with index\\\\nbeyond m(t + 1) + 1.\\\\n7\\\\nIf Hi \\\\\\\\Di is not empty, this indicates a deficit of equations\\\\ncompared to the unknowns involved in them.\\\\n\\\\nTheorem 11: At the beginning of any slot t > 0, at\\\\nleast one receiver has decoded all packets from p1 to\\\\npm(t) .\\\\nProof: The proof is by induction on the slot number.\\\\nBasis step: Since m(1) = 0, the statement is trivially\\\\ntrue for t = 1.\\\\nInduction hypothesis: Suppose at the beginning of slot t,\\\\nthere is a receiver R that has decoded all packets from\\\\np1 to pm(t) .\\\\nInduction step: We need to show that the statement holds\\\\nat the beginning of slot (t + 1). Clearly, m(t)  m(t +\\\\n1)  m(t) + 1 (The rank cannot jump by more than 1\\\\nper slot).\\\\nIf m(t + 1) = m(t), then the statement clearly holds,\\\\nas R has already decoded packets from p1 to pm(t) . If\\\\nm(t + 1) = m(t) + 1, then let R be the receiver with\\\\nthat rank. From Lemma 9, all transmissions up to and\\\\nincluding the one in slot t, have involved packets with\\\\nindex 1 to m(t)+1. This means R has m(t+1) linearly\\\\nindependent equations in the unknowns p1 to pm(t+1) .\\\\nThus, R can decode these packets and this completes\\\\nthe proof.\\\\nDefinition 13 (Leader): In the context of this coding\\\\nmodule, the node that has decoded all packets from p1\\\\nto pm(t) at the beginning of slot t is called the leader. If\\\\nthere is more than one such node, then any one of them\\\\nmay be picked.\\\\nNote that the node labeled L in the algorithm corresponds to the leader. The other two nodes are called\\\\nnon-leaders. We now present another useful feature of\\\\nthe coding module.\\\\nLemma 10: From any receivers perspective, the\\\\ntransmitted linear combination involves at most two\\\\nundecoded packets in any slot.\\\\nProof: The module mixes at most two packets with\\\\neach other, except in case 2 where sometimes three\\\\npackets are mixed. Even in case 2, one of the packets,\\\\nnamely pm+1 , has already been decoded by both nonleaders, as it is in S1 . From the leaders perspective, there\\\\nis only one unknown packet that could be involved in any\\\\ntransmission, namely, pm+1 (from Lemma 9). Thus, in\\\\nall cases, no more than two undecoded packets are mixed\\\\nfrom any receivers point of view.\\\\nStructure of the knowledge space: The above property\\\\nleads to a nice structure for the knowledge space of the\\\\nreceivers. In order to explain this structure, we define\\\\nthe following relation with respect to a specific receiver.\\\\nThe ground set G of the relation contains all packets that\\\\nhave arrived at the sender so far, along with a fictitious\\\\nall-zero packet that is known to all receivers even before\\\\ntransmission begins. Note that the relation is defined with\\\\nrespect to a specific receiver. Two packets px  G and\\\\n\\\\n\\\\f23\\\\n\\\\npy  G are defined to be related to each other if the\\\\nreceiver knows at least one of px + py and px + 2py .\\\\nLemma 11: The relation defined above is an equivalence relation.\\\\nProof: A packet added with two times the same\\\\npacket gives 0 which is trivially known to the receiver.\\\\nHence, the relation is reflexive. The relation is symmetric\\\\nbecause addition is a commutative operation. For any\\\\npx , py , pz in G, if a receiver knows px + py and py +\\\\npz , then it can compute either px + pz or px + 2pz\\\\nby canceling out the py , for  = 1 or 2 and  = 1 or\\\\n2. Therefore the relation is also transitive and is thus an\\\\nequivalence relation.\\\\nThe relation defines a partition on the ground set,\\\\nnamely the equivalence classes, which provide a structured abstraction for the knowledge of the node. The reason we include a fictitious all-zero packet in the ground\\\\nset is that it allows us to represent the decoded packets\\\\nwithin the same framework. It can be seen that the class\\\\ncontaining the all-zero packet is precisely the set of\\\\ndecoded packets. Packets that have not been involved\\\\nin any of the successfully received linear combinations\\\\nso far will form singleton equivalence classes. These\\\\ncorrespond to the packets that the receiver has not heard\\\\nof. All other classes contain the packets that have been\\\\nheard of but not decoded. Packets in the same class are\\\\nequivalent in the sense that revealing any one of them\\\\nwill reveal the entire class to the receiver.\\\\nTheorem 12: At the beginning of any slot t > 0, at\\\\nleast one of the two non-leaders has an empty unsolved\\\\nset, i.e., has Hi = Di .\\\\nProof: Initially, every receiver has an empty unsolved set (Hi \\\\\\\\Di ). It becomes non-empty only when\\\\na receiver receives a mixture involving two undecoded\\\\npackets. It can be verified that this happens only in two\\\\nsituations:\\\\n1) When case 4 occurs, and pm+1  S3 is mixed with\\\\na packet from S6 ; or\\\\n2) When case 5 occurs, and pm+1  S4 is mixed with\\\\na packet from S6 .\\\\nEven in these cases, only one receiver develops an\\\\nunsolved set because, from the other two receivers\\\\nperspective, the mixture involves one decoded packet and\\\\none new packet.\\\\nThe receiver that develops an unsolved set, say node\\\\nj , is labeled D in step 6, and HD \\\\\\\\DD now contains\\\\ntwo packets. Let the slot in which this happens for the\\\\nfirst time be t1 . Now, at least one of these two packets\\\\nis in S2 because, as argued above, each of the other two\\\\nreceivers has decoded one of these packets. So, no matter\\\\nwhich of the other two receivers is labeled N, one of\\\\nthese two packets has already been decoded by N.\\\\n\\\\nWe will now prove by contradiction that neither of the\\\\nother two nodes can develop an unsolved set, as long as\\\\nnode j s unsolved set is not empty. In other words, node\\\\nj will continue to be labeled as D, until its unsolved\\\\nset is fully decoded.\\\\nSuppose one of the other nodes, say node i (i 6= j ),\\\\nindeed develops an unsolved set while HD \\\\\\\\DD is still\\\\nnon-empty. Let t2 be the slot when this happens. Thus,\\\\nfrom slot t1 + 1 to slot t2 , node j is labeled D . We\\\\ntrack the possible changes to HD \\\\\\\\DD in terms of its\\\\nconstituent equivalence classes, during this time. Only\\\\nthree possible types of changes could happen:\\\\n1) Addition of new class: A new equivalence class\\\\nwill be added to HD \\\\\\\\DD if case 4 occurs, and\\\\npm+1  S3 is mixed with a packet from S6 . In\\\\nthis case, the new class will again start with two\\\\npackets just as above, and at least one of them will\\\\nbe in S2 .\\\\n2) Decoding of existing class: An existing equivalence class could get absorbed into the class of decoded packets if an innovative linear combination\\\\nis revealed about the packets in the class, allowing\\\\nthem to be decoded.\\\\n3) Expansion of existing class: If a linear combination\\\\ninvolves a packet in an existing class and a new\\\\nunheard of packet, then the new packet will simply\\\\njoin the class.\\\\nIn every class, at least one of the initial two packets\\\\nis in S2 when it is formed. The main observation is that\\\\nduring the period up to t2 , this remains true till the class\\\\ngets decoded. The reason is as follows. Up to slot t2 ,\\\\nnode j is still called D. Even if the labels L and N\\\\nget interchanged, at least one of the initial pair of packets\\\\nwill still be in DN , and therefore in S2 . The only way\\\\nthe classs contribution to S2 can become empty is if the\\\\nclass itself gets decoded by D .\\\\nThis means, as long as there is at least one class, i.e.,\\\\nas long as HD \\\\\\\\DD is non-empty, S2 will also be nonempty. In particular, S2 will be non-empty at the start of\\\\nslot t2 .\\\\nBy assumption, node i developed an unsolved set in\\\\nslot t2 . Then, node i could not have been a leader at\\\\nthe beginning of slot t2  a leader can never develop\\\\nan unsolved set, as there is only one undecoded packet\\\\nthat could ever be involved in the transmitted linear\\\\ncombination, namely pm+1 (Lemma 9). Therefore, for\\\\nnode i to develop an unsolved set, it has to first be a\\\\nnon-leader, i.e., N at the start of slot t2 . In addition,\\\\ncase 5 must occur, and pm+1  S4 must get mixed\\\\nwith a packet from S6 during t2 . But this could not\\\\nhave happened, as we just showed that S2 is nonempty. Hence, in case 5, the coding module would have\\\\n\\\\n\\\\f24\\\\n\\\\npreferred S2 to S6 , thus leading to a contradiction.\\\\nOnce j s unsolved set is solved, the system returns\\\\nto the initial state of all unsolved sets being empty. The\\\\nsame argument applies again, and this proves that a node\\\\ncannot develop an unsolved set while another already has\\\\na non-empty unsolved set.\\\\nInnovation guarantee: Next, we prove that the coding\\\\nmodule provides the innovation guarantee.\\\\nTheorem 13: The transmit linear combination computed by the coding module is innovative to all receivers\\\\nthat have not decoded everything that the sender knows.\\\\nProof: Since the maximum rank is m, any deficit\\\\nbetween the sender and any receiver will show up within\\\\nthe first (m + 1) packets. Thus, it is sufficient to check\\\\nwhether U \\\\\\\\Di is non-empty, while deciding whether\\\\nthere is a deficit between the sender and receiver i.\\\\nConsider the leader node. It has decoded packets p1 to\\\\npm (by Theorem 11). If pm+1 has not yet arrived at the\\\\nsender, then the guarantee is vacuously true. If pm+1 has\\\\narrived, then the transmission involves this packet in all\\\\nthe cases, possibly combined with one or two packets\\\\nfrom p1 to pm , all of which the leader has already\\\\ndecoded. Hence, the transmission will reveal pm+1 , and\\\\nin particular, will be innovative.\\\\nNext, consider node N. If there is a packet in U \\\\\\\\DN ,\\\\nthen at least one of S4 , S5 and S6 will be non-empty. Let\\\\nus consider the coding module case by case.\\\\nCase 1  Suppose S4 is empty, then the module considers S5 and S6 before anything else, thereby ensuring\\\\ninnovation. Suppose S4 is not empty, then a packet from\\\\nS4 is mixed with a packet from S2 or S3 if available.\\\\nSince S2 and S3 have already been decoded by N, this\\\\nwill reveal the packet from S4 . If both S2 and S3 are\\\\nempty, then S5 , S6 and S4 are considered in that order.\\\\nTherefore, in all cases, if there is a deficit, an innovative\\\\npacket will be picked.\\\\nCase 2  This is identical to case 1, since pm+1 has\\\\nalready been decoded by N.\\\\nCase 3 and 4  pm+1 has already been decoded by N,\\\\nand the other packet is picked from S4 , S5 or S6 , thus\\\\nensuring innovation.\\\\nCase 5 and 6  In these cases, pm+1 has not yet been\\\\ndecoded by N, and is involved in the transmission.\\\\nSince N has no unsolved set (Theorem 12), innovation\\\\nis ensured.\\\\nFinally, consider node D. If there is a packet in\\\\nU \\\\\\\\DD , then at least one of S2 , S3 , S5 and S6 will be\\\\nnon-empty. Again, we consider the coding module case\\\\nby case.\\\\n\\\\nCase 1  If S4 is empty, the coding module considers\\\\nS5 , S6 , S2 or S3 and reveals a packet from the first nonempty set. If S4 is not empty, then then a packet from S4\\\\nis mixed with a packet from S2 or S3 if available. Since\\\\nS4 has already been decoded by D, this will reveal a\\\\npacket from S2 or S3 respectively. If both S2 and S3 are\\\\nempty, then S5 and S6 are considered. Thus, innovation\\\\nis ensured.\\\\nCase 2  This is identical to case 1, since pm+1 has\\\\nalready been decoded by D.\\\\nCase 3  In this case, pm+1  HD \\\\\\\\DD . There are four\\\\npossibilities:\\\\n1) If it is mixed with a packet from S4 , then since D\\\\nhas already all packets in S4 , it will decode pm+1 .\\\\n2) If instead it is mixed with a packet, say p from\\\\nS5 , then since both packets have been heard of, it\\\\nis possible that D already knows at most one of\\\\np + pm+1 and 2p + pm+1 . Then, as outlined in\\\\nstep 4 of the algorithm (case 3), the coefficient of\\\\np is chosen so as to guarantee innovation.\\\\n3) If it is mixed with a packet from S6 , then innovation is ensured because the packet in S6 has not\\\\neven been heard of.\\\\n4) If it is not mixed with any other packet, then also\\\\ninnovation is ensured, since pm+1 has not yet been\\\\ndecoded.\\\\nCase 4  The exact same reasoning as in Case 3 holds\\\\nhere, except that the complication of picking the correct\\\\ncoefficient in possibility number 2 above, does not arise.\\\\nCase 5  In this case, pm+1 has already been decoded.\\\\nThe module considers S2 , S3 and S6 . There is no need\\\\nto consider S5 because, if S5 is non-empty, then so is\\\\nS2 . This fact follows from the arguments in the proof of\\\\nTheorem 12.\\\\nCase 6  In all the other cases, pm+1 has not been\\\\ndecoded, and will therefore be innovative.\\\\nE. Delay performance of the new coding module\\\\nWe now study the delay experienced by an arbitrary\\\\narrival before it gets decoded by one of the receivers.\\\\nWe consider a system where  is fixed at 0.5. The value\\\\nof  is varied from 0.9 to 0.99 in steps of 0.01. We\\\\nplot the expected decoding delay and delivery delay per\\\\npacket,\\\\naveraged\\\\nacross the three receivers, as a function\\\\n\\\\u0010\\\\n\\\\u0011\\\\n1\\\\nof 1\\\\nin Figure 6. We also plot the log of the same\\\\nquantities in Figure 7. The value of the delay is averaged\\\\nover 106 time slots for the first five points and 2  106\\\\ntime slots for the next three points and 5  106 for the\\\\nlast two points.\\\\nFigure 6 shows that the growth of the average decoding delay as well as the average delivery delay are\\\\n\\\\n\\\\f25\\\\n300\\\\n\\\\nVII. A PPLICATIONS\\\\nDecoding delay\\\\nDelivery delay\\\\n\\\\nDelay (in slots)\\\\n\\\\n250\\\\n\\\\n200\\\\n\\\\n150\\\\n\\\\n100\\\\n\\\\n50\\\\n\\\\n0\\\\n0\\\\n\\\\n20\\\\n\\\\n40\\\\n\\\\n60\\\\n\\\\n80\\\\n\\\\n100\\\\n\\\\n1/(1)\\\\n\\\\nFig. 6. Decoding and delivery delay for the coding module in Section\\\\nVI-C\\\\n6\\\\nDecoding delay\\\\nDelivery delay\\\\n\\\\n5.5\\\\n\\\\nloge(Delay)\\\\n\\\\n5\\\\n4.5\\\\n4\\\\n3.5\\\\n3\\\\n2.5\\\\n2\\\\n2\\\\n\\\\n2.5\\\\n\\\\n3\\\\n\\\\n3.5\\\\n\\\\n4\\\\n4.5\\\\nloge(1/(1))\\\\n\\\\n5\\\\n\\\\n5.5\\\\n\\\\n6\\\\n\\\\nFig. 7. Log plot of the delay for the coding module in Section VI-C\\\\n\\\\n\\\\u0010\\\\n\\\\n\\\\u0011\\\\n\\\\n1\\\\nlinear in 1\\\\nas  approaches 1. Figure 7 confirms\\\\nthis behavior  we can see that the slopes on the plot\\\\nof the logarithm of these quantities is indeed close to 1.\\\\nThis observation leads to the following conjecture:\\\\nConjecture 1: For the newly proposed coding module,\\\\nthe expected decoding delay per packet, as well as the\\\\nexpected delivery delay per packet from\\\\na\\\\u0011 particular\\\\n\\\\u0010\\\\n1\\\\n, which is\\\\nreceivers point of view grow as O 1\\\\nasymptotically optimal.\\\\nThis conjecture, if true, implies that such feedbackbased coding for delay also simplifies the queue management at the sender. If the sender simply follows a\\\\ndrop-when-decoded strategy, then by Littles theorem,\\\\nthe expected queue size of undecoded packets will\\\\nbe\\\\n\\\\u0010\\\\n\\\\u0011\\\\n1\\\\nproportional to the expected decoding delay O 1 ,\\\\nwhich is asymptotically optimal.\\\\n\\\\nAND FURTHER EXTENSIONS\\\\n\\\\nAlthough we have presented the algorithm in the\\\\ncontext of a single packet erasure broadcast channel, we\\\\nbelieve the main ideas in the scheme are quite robust\\\\nand can be applied to more general topologies. The\\\\nscheme readily extends to a tandem network of broadcast\\\\nlinks (with no mergers) if the intermediate nodes use\\\\nthe witness packets in place of the original packets.\\\\nWe expect that it will also extend to other topologies\\\\nwith suitable modifications. In addition, we believe the\\\\nproposed scheme will also be robust to delayed or\\\\nimperfect feedback, just like conventional ARQ. Such\\\\na generalization can lead to a TCP-like protocol for\\\\nsystems that use network coding [36].\\\\nWe have assumed the erasures to be independent\\\\nand identically distributed across receivers. However, the\\\\nanalysis for Algorithm 2 (b) will hold even if we allow\\\\nadversarial erasures. This is because, the guarantee that\\\\nthe physical queue size tracks the backlog in degrees\\\\nof freedom is not a probabilistic guarantee, but a combinatorial guarantee on the instantaneous value of the\\\\nqueue sizes. Note that, while the erasures can be chosen\\\\nadversarially, we will require the adversary to guarantee\\\\na certain minimum long-term connection rate from the\\\\nsender to every receiver, so that the virtual queues can\\\\nthemselves be stabilized.\\\\nFrom a theoretical point of view, our results mean\\\\nthat any stability results or queue size bounds in terms\\\\nof virtual queues can be translated to corresponding\\\\nresults for the physical queues. In addition, results from\\\\ntraditional queuing theory about M/G/1 queues or a\\\\nJackson network type of result [8] can be extended to\\\\nthe physical queue size in coded networks, as opposed to\\\\njust the backlog in degrees of freedom. From a practical\\\\npoint of view, if the memory at the sender has to be\\\\nshared among several different flows, then this reduction\\\\nin queue occupancy will prove quite useful in getting\\\\nstatistical multiplexing benefits.\\\\nFor instance, one specific scenario where our results\\\\ncan be immediately applied is the multicast switch with\\\\nintra-flow network coding, studied in [30]. The multicast\\\\nswitch has broadcast-mode links from each input to\\\\nall the outputs. Erasures occur because the scheduler\\\\nmay require that only some outputs can receive the\\\\ntransmission, as the others are scheduled to receive a\\\\ndifferent transmission from some other input. In this\\\\ncase, there is no need for explicit feedback, since the\\\\nsender can track the states of knowledge of the receivers\\\\nsimply using the scheduling configurations from the past.\\\\nThe results stated in [30] in terms of the virtual queues\\\\ncan thus be extended to the physical queues as well.\\\\n\\\\n\\\\f26\\\\n\\\\nAnother important extension that needs to be investigated in the future, is the extension of the coding scheme\\\\nfor optimizing decoding and delivery delay to the case\\\\nof more than three receivers. This problem is particularly\\\\nimportant for real-time data streaming applications.\\\\nVIII. C ONCLUSIONS\\\\nIn this work, we have presented a completely online\\\\napproach to network coding based on feedback, which\\\\ndoes not compromise on throughput and yet, provides\\\\nbenefits in terms of queue occupancy at the sender and\\\\ndecoding delay at the receivers.\\\\nThe notion of seen packets introduced in this work,\\\\nallows the application of tools and results from traditional queuing theory in contexts that involve coding\\\\nacross packets. Using this notion, we proposed the dropwhen-seen algorithm, which allows the physical queue\\\\nsize to track the backlog in degrees of freedom, thereby\\\\nreducing the amount of storage used at the sender.\\\\nComparing the results in Theorem 1 and Theorem 6,\\\\nwe see that the newly proposed Algorithm 2 (b) gives\\\\na significant improvement in the expected queue size at\\\\nthe sender, compared to Algorithm 1.\\\\nFor the three receiver case, we have proposed a new\\\\ncoding scheme that makes use of feedback to dynamically adapt the code in order\\\\u0010 to ensure\\\\nlow decoding\\\\n\\\\u0011\\\\n1\\\\ndelay. As argued earlier,  1\\\\nis an asymptotic\\\\nlower bound on the decoding delay and the stronger\\\\nnotion of delivery delay in the limit of the load factor\\\\napproaching capacity (  1). We conjecture that our\\\\nscheme achieves this lower bound. If true, this implies\\\\nthe asymptotic optimality of our coding module in terms\\\\nof both decoding delay and delivery delay. We have\\\\nverified this conjecture through simulations.\\\\nIn summary, we believe that the proper combination\\\\nof feedback and coding in erasure networks presents a\\\\nwide range of benefits in terms of throughput, queue\\\\nmanagement and delay. Our work is a step towards\\\\nrealizing these benefits.\\\\nR EFERENCES\\\\n[1] M. Luby, LT codes, in Proceedings of IEEE Symposium on\\\\nFoundations of Computer Science (FOCS), November 2002, pp.\\\\n271282.\\\\n[2] A. Shokrollahi, Raptor codes, in Proceedings of IEEE International Symposium on Information Theory (ISIT), July 2004.\\\\n[3] P. Pakzad, C. Fragouli, and A. Shokrollahi, Coding schemes\\\\nfor line networks, in Proceedings of IEEE International Symposium on Information Theory (ISIT), 2005.\\\\n[4] R. Gummadi and R. S. Sreenivas, Relaying a fountain code\\\\nacross multiple nodes, in Proceedings of IEEE Information\\\\nTheory Workshop, 2008.\\\\n\\\\n[5] R. Ahlswede, N. Cai, S.-Y. R. Li, and R. W. Yeung, Network\\\\ninformation flow, IEEE Trans. on Information Theory, vol. 46,\\\\npp. 12041216, 2000.\\\\n[6] R. Koetter and M. M edard, An algebraic approach to network\\\\ncoding, IEEE/ACM Trans. Netw., vol. 11, no. 5, pp. 782795,\\\\n2003.\\\\n[7] D. S. Lun, Efficient operation of coded packet networks, PhD\\\\nThesis, Massachusetts Institute of Technology, Dept. of EECS,\\\\nJune 2006.\\\\n[8] D. S. Lun, M. M edard, R. Koetter, and M. Effros, On coding\\\\nfor reliable communication over packet networks, Physical\\\\nCommunication, vol. 1, no. 1, pp. 3  20, 2008.\\\\n[9] E. Martinian, Dynamic information and constraints in source\\\\nand channel coding, PhD Thesis, Massachusetts Institute of\\\\nTechnology, Dept. of EECS, Sep. 2004.\\\\n[10] A. Sahai, Why delay and block length are not the same thing\\\\nfor channel coding with feedback, in Proc. of UCSD Workshop\\\\non Information Theory and its Applications. Invited Paper, Feb.\\\\n2006.\\\\n[11] C. T. K. Ng, M. M edard, and A. Ozdaglar, Cross-layer\\\\noptimization in wireless networks under different packet delay\\\\nmetrics, in Proceedings of IEEE INFOCOM, 2009.\\\\n[12] S. Sanghavi, Intermediate performance of rateless codes, in\\\\nProceedings of IEEE Information Theory Workshop (ITW),\\\\nSeptember 2007.\\\\n[13] A. Beimel, S. Dolev, and N. Singer, RT oblivious erasure correcting, in Proceedings of IEEE Information Theory Workshop\\\\n(ITW), October 2004.\\\\n[14] A. Albanese, J. Bl omer, J. Edmonds, M. Luby, and M. Sudan,\\\\nPriority encoding transmission, IEEE Trans. on Information\\\\nTheory, vol. 42, pp. 17371744, November 1996.\\\\n[15] D. Silva and F. R. Kschischang, Rank-metric codes for priority encoding transmission with network coding, in Canadian\\\\nWorkshop on Information Theory, June 2007, p. 8184.\\\\n[16] J. M. Walsh and S. Weber, A concatenated network coding\\\\nscheme for multimedia transmission, in Proc. of NetCod, 2008.\\\\n[17] T. Ho and H. Viswanathan, Dynamic algorithms for multicast\\\\nwith intra-session network coding, in 43rd Allerton Annual\\\\nConference on Communication, Control and Computing, 2005.\\\\n[18] A. Eryilmaz and D. S. Lun, Control for inter-session network\\\\ncoding, in Proc. of NetCod, 2007.\\\\n[19] M. Artin, Algebra. Englewood Cliffs, NJ: Prentice-Hall, 1991.\\\\n[20] B. Shrader and A. Ephremides, On the queueing delay of\\\\na multicast erasure channel, in IEEE Information Theory\\\\nWorkshop (ITW), October 2006.\\\\n[21] , A queueing model for random linear coding, in\\\\nIEEE Military Communications Conference (MILCOM), October 2007.\\\\n[22] Y. E. Sagduyu and A. Ephremides, On broadcast stability\\\\nregion in random access through network coding, in 44th\\\\nAllerton Annual Conference on Communication, Control and\\\\nComputing, September 2006.\\\\n[23] , On network coding for stable multicast communication,\\\\nin IEEE Military Communications Conference (MILCOM), October 2007.\\\\n[24] J. Lacan and E. Lochin, On-the-fly coding to enable\\\\nfull reliability without retransmission, ISAE, LAASCNRS, France, Tech. Rep., 2008. [Online]. Available:\\\\nhttp://arxiv.org/pdf/0809.4576\\\\n[25] P. Larsson and N. Johansson, Multi-user ARQ, in Proceedings\\\\nof IEEE Vehicular Technology Conference (VTC) - Spring, May\\\\n2006, pp. 20522057.\\\\n[26] M. Jolfaei, S. Martin, and J. Mattfeldt, A new efficient selective\\\\nrepeat protocol for point-to-multipoint communication, in Proceedings of IEEE International Conference on Communications\\\\n(ICC), May 1993, pp. 11131117 vol.2.\\\\n\\\\n\\\\f27\\\\n\\\\n[27] S. Yong and L. B. Sung, XOR retransmission in multicast error\\\\nrecovery, in Proceedings of IEEE International Conference on\\\\nNetworks (ICON), 2000, pp. 336340.\\\\n[28] P. Larsson, Multicast multiuser ARQ, in Proceedings of\\\\nIEEE Wireless Communications and Networking Conference\\\\n(WCNC), 2008, pp. 19851990.\\\\n[29] D. Nguyen, T. Tran, T. Nguyen, and B. Bose, Wireless broadcast using network coding, IEEE Transactions on Vehicular\\\\nTechnology, vol. 58, no. 2, pp. 914925, February 2009.\\\\n[30] J. K. Sundararajan, M. M edard, M. Kim, A. Eryilmaz, D. Shah,\\\\nand R. Koetter, Network coding in a multicast switch, in\\\\nProceedings of IEEE INFOCOM, 2007.\\\\n[31] L. Keller, E. Drinea, and C. Fragouli, Online broadcasting with\\\\nnetwork coding, in Proc. of NetCod, 2008.\\\\n[32] M. Durvy, C. Fragouli, and P. Thiran, Towards reliable broadcasting using ACKs, in Proceedings of IEEE International\\\\nSymposium on Information Theory (ISIT), 2007.\\\\n[33] C. Fragouli, D. S. Lun, M. M edard, and P. Pakzad, On\\\\nfeedback for network coding, in Proc. of 2007 Conference on\\\\nInformation Sciences and Systems (CISS 2007), March 2007.\\\\n[34] J. J. Hunter, Mathematical Techniques of Applied Probability,\\\\nVol. 2, Discrete Time Models: Techniques and Applications.\\\\nNY: Academic Press, 1983.\\\\n[35] H. Takagi, Queueing Analysis, Vol. 3: Discrete-Time Systems.\\\\nAmsterdam: Elseviser Science B. V., 1993.\\\\n[36] J. K. Sundararajan, D. Shah, M. M edard, M. Mitzenmacher,\\\\nand J. Barros, Network coding meets TCP, in Proceedings of\\\\nIEEE INFOCOM, 2009.\\\\n\\\\nA PPENDIX A\\\\nD ERIVATION\\\\n\\\\nOF THE FIRST PASSAGE TIME\\\\n\\\\nConsider the Markov chain {Qj (t)} for the virtual\\\\nqueue size, shown in Figure 2. Assume that the Markov\\\\nchain has an initial distribution equal to the steady state\\\\ndistribution (Equivalently, assume that the Markov chain\\\\nhas reached steady state.). We use the same notation as\\\\nin Section IV-A.\\\\nDefine Nm := inf{t  1 : Qj (t) = m}. We are\\\\ninterested in deriving for k  1, an expression for k,0,\\\\nthe expected first passage time from state k to 0, i.e.,\\\\nk,0 = E[N0 |Qj (0) = k]\\\\n\\\\nDefine for i  1:\\\\nXi := a(i)  d(i)\\\\n\\\\nwhere, a(i) is the indicator function for an arrival in\\\\nslot i, and d(i) is the indicator function for the channel\\\\nP\\\\nbeing on in slot i. Let St := ti=1 Xi . If Qj (t) > 0,\\\\nthen the channel being on in slot t implies that there is a\\\\ndeparture in that slot. Thus the correspondence between\\\\nthe channel being on and a departure holds for all 0 \\\\nt  N0 . This implies that:\\\\nFor t  N0 , Qj (t) = Qj (0) + St\\\\nThus, N0 can be redefined as the smallest t  1 such\\\\nthat St reaches Qj (0). Thus, N0 is a valid stopping\\\\nrule for the Xi s which are themselves IID, and have a\\\\n\\\\nmean E[X] = (  ). We can find E[N0 ] using Walds\\\\nequality:\\\\nE[SN0 |Qj (0) = k] = E[N0 |Qj (0) = k]  E[X]\\\\n\\\\ni.e.,  k = E[N0 |Qj (0) = k]  (  )\\\\nwhich gives:\\\\nk,0 = E[N0 |Qj (0) = k] =\\\\n\\\\nk\\\\n\\\\n\\\\nA PPENDIX B\\\\nP ROOF OF L EMMA 4\\\\nProof: For any z  V  ni=1Ui , there is a x  V\\\\nand y  ni=1 Ui such that z = x + y . Now, for each i,\\\\ny  Ui . Thus, z = x + y implies that z  ni=1 [V  Ui ].\\\\nTherefore, V  ni=1 Ui  ni=1 [V  Ui ].\\\\nNow, let w  ni=1 V  Ui . Then for each i, there\\\\nis a xi  V and yi  Ui such that w = xi + yi . But,\\\\nw = xi +yi = xj +yj means that xi xj = yi yj . Now,\\\\n(xi  xj )  V and (yi  yj )  (U1 + U2 + . . . + Un ).\\\\nBy hypothesis, these two vector spaces have only 0 in\\\\ncommon. Thus, xi  xj = yi  yj = 0. All the xi s are\\\\nequal to a common x  V and all the yi s are equal to\\\\na common y which belongs to all the Ui s. This means,\\\\nw can be written as the sum of a vector in V and a\\\\nvector in ni=1 Ui , thereby proving that ni=1 [V  Ui ] \\\\nV  ni=1 Ui .\\\\nA PPENDIX C\\\\nP ROOF OF L EMMA 5\\\\nProof: Statement 1 follows from the fact that B is\\\\na subset of B  C . Hence, if A  (B  C) is empty, so\\\\nis A  B .\\\\nFor statement 2, we need to show that (A  B)  C =\\\\n{0}. Consider any element x  (A  B)  C . Since it\\\\nis in A  B , there exist unique a  A and b  B such\\\\nthat x = a + b. Now, since b  B and x  C , it follows\\\\nthat a = x  c is in B  C . It is also in A. Since A is\\\\nindependent of B  C , a must be 0. Hence, x = b. But\\\\nthis means x  B . Since it is also in C , it must be 0,\\\\nas B and C are independent. This shows that the only\\\\nelement in (A  B)  C is 0.\\\\nStatement 3 can be proved as follows.\\\\nx  A  (B  C)\\\\n unique a  A, d  B  C s.t. x = a + d\\\\n unique a  A, b  B, c  C s.t. x = a + b + c\\\\n unique e  A  B, c  C s.t. x = e + c\\\\nx  (A  B)  C\\\\n\\\\n\\\\f\"}\\n'"}, "key_words": {"Abstract": [["network coding performed", 0.6651], ["network coding", 0.6508], ["network coding batch", 0.6343], ["shown network coding", 0.6208], ["network coded studied", 0.6124]], "Introduction": [["network coding realtime", 0.5635], ["queue size sender", 0.5532], ["streaming applications packet", 0.5459], ["queue size delay", 0.5351], ["notions ndecoding delay", 0.5334]], "Related Work": [["coding packets", 0.62], ["coding packets nthe", 0.6141], ["coding packet", 0.6136], ["packet transmits coded", 0.6029], ["coded packets", 0.5987]], "Methodology": [["receiver virtual queue", 0.6196], ["virtual queues correspond", 0.6041], ["queue coding", 0.5901], ["virtual queues", 0.5699], ["algorithm physical queue", 0.5655]], "Experiment": [["linear combination pk", 0.5602], ["packet corresponds linearly", 0.5461], ["linear combination packets", 0.5193], ["packet corresponds pivot", 0.5093], ["packet knows linear", 0.5071]], "Discussion": [["nthese packets decoded", 0.6785], ["involves decoded packet", 0.6529], ["coded packet", 0.6478], ["packets decoded", 0.6426], ["decoded packets", 0.6396]]}, "summarization": {"Abstract": "feedback-based online network coding can be performed in a completely online manner without the need for batch-based coding. The benefits of using feedback in a network coded system are studied. The paper is presented at the Xiv:0904.1730v1.", "Introduction": "nof feedback on queue size at the sender and decoding delay at the receivers is studied. Strategies for adaptive coding based on feedback are presented to minimize the queue size and delay. The paper is a step towards low-delay, highthroughput solutions based on network coding.", "Related Work": "reliable communication over a network of packet channels is a well-studied problem. We present three approaches to solve the problem. They include digital fountain codes, random linear network coding and priority encoding transmission. The schemes would generate a continuous stream of packets in real-time.", "Methodology": "We use a random coding scheme with a drop-when-decoded rule for queue update. We assume that each slot is served independently of the others. We also assume that the queue update rule can track the size of the virtual queue. The algorithm is based on Bernoulli arrival and receipt times.", "Experiment": "receiver's number of packets seen is equal to the dimension of its knowledge space. Remark 2: Remark proposes a coding algorithm for decoded packets. The algorithm uses a linear combination to see the next unseen packet of a packet. The coding algorithm is based on a simple XOR rule.", "Discussion": "define a universe set U consisting of packets p1 and pm+1 if they have arrived. Identify the set of receivers that have decoded all packets from 1 to m. Compute a linear combination of the sets and send them to each receiver. We show in step 3 that each receiver will have a non-empty set."}}