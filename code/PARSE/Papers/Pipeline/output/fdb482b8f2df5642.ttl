@prefix askg-data: <https://www.anu.edu.au/data/scholarly/> .
@prefix askg-onto: <https://www.anu.edu.au/onto/scholarly#> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .
@prefix domo: <http://example.org/domo/> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

askg-data:Paper-fdb482b8f2df5642 a askg-onto:Paper ;
    rdfs:label "fdb482b8f2df5642"@en ;
    dc:title "fdb482b8f2df5642"^^xsd:string ;
    askg-onto:hasSection askg-data:Paper-fdb482b8f2df5642-Section-1,
        askg-data:Paper-fdb482b8f2df5642-Section-10,
        askg-data:Paper-fdb482b8f2df5642-Section-11,
        askg-data:Paper-fdb482b8f2df5642-Section-12,
        askg-data:Paper-fdb482b8f2df5642-Section-13,
        askg-data:Paper-fdb482b8f2df5642-Section-14,
        askg-data:Paper-fdb482b8f2df5642-Section-15,
        askg-data:Paper-fdb482b8f2df5642-Section-16,
        askg-data:Paper-fdb482b8f2df5642-Section-2,
        askg-data:Paper-fdb482b8f2df5642-Section-3,
        askg-data:Paper-fdb482b8f2df5642-Section-4,
        askg-data:Paper-fdb482b8f2df5642-Section-5,
        askg-data:Paper-fdb482b8f2df5642-Section-6,
        askg-data:Paper-fdb482b8f2df5642-Section-7,
        askg-data:Paper-fdb482b8f2df5642-Section-8,
        askg-data:Paper-fdb482b8f2df5642-Section-9 .

askg-data:Entity-%09exti rdfs:label "${	ext{I}}$"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%0A________%0A________cal_l_%0Dm_cvae rdfs:label """
        {
        cal L}_{\rm CVAE}"""@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%0Amathcall_mathrmioub rdfs:label """
\\mathcal{L}_{\\mathrm{Ioub}}"""@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%C2%B5 rdfs:label "µ"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%C2%B5_k_%CF%83k rdfs:label "(µ k, σk)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%C2%B5post_%CF%83post rdfs:label "(µpost, σpost)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%C2%B5prior_%CF%83prior rdfs:label "(µprior, σprior)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%CE%B1 rdfs:label "α"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%CE%BB1__%CE%BB2__03 rdfs:label "λ1 = λ2 = 0.3"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-%CE%BE rdfs:label "ξ"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%CF%83 rdfs:label "σ"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%CF%83_k__%C2%B5_k rdfs:label "σ k + µ k"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%CF%88 rdfs:label "Ψ"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-%CF%88s__s_2__1e6 rdfs:label "Ψ(s) = √s 2 + 1e−6"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-0 rdfs:label "0"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-006s rdfs:label "0.06s"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-019 rdfs:label ".019"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-025 rdfs:label ".025"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-039 rdfs:label ".039"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-043 rdfs:label ".043"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-051 rdfs:label ".051"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-066 rdfs:label ".066"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-1 rdfs:label "1"@en ;
    askg-onto:entityType "Score"@en .

askg-data:Entity-12 rdfs:label "12"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-13_hours rdfs:label "13 hours"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-186202 rdfs:label "186–202"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-18_algorithms rdfs:label "18 algorithms"@en ;
    askg-onto:entityType "Algorithm"@en .

askg-data:Entity-18_competing_algorithms rdfs:label "18 competing algorithms"@en ;
    askg-onto:entityType "Algorithm"@en .

askg-data:Entity-1e-4 rdfs:label "1e-4"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-201112541259_1998 rdfs:label "20(11):1254–1259, 1998"@en ;
    askg-onto:entityType "Article"@en .

askg-data:Entity-2016 rdfs:label "2016"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-22272238 rdfs:label "2227–2238"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-22742285 rdfs:label "2274–2285"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-236819823 rdfs:label "23(6):819–823"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-24 rdfs:label "24"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-275 rdfs:label "275"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-28062813 rdfs:label "2806–2813"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-2_12 rdfs:label "[2, 12]"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-30 rdfs:label "30"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-32 rdfs:label "32"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-352__352 rdfs:label "352 × 352"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-3d_domain rdfs:label "3D domain"@en ;
    askg-onto:entityType "Domain"@en .

askg-data:Entity-3d_instance_segmentation rdfs:label "3D Instance Segmentation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-3d_meshes_deformation rdfs:label "3D meshes deformation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-40101489_-_1506_2000 rdfs:label "40(10):1489 - 1506, 2000"@en ;
    askg-onto:entityType "Article"@en .

askg-data:Entity-42044216 rdfs:label "4204–4216"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-5 rdfs:label "5"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-54 rdfs:label "[54]"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-5527755284 rdfs:label "55277–55284"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-57065722 rdfs:label "5706–5722"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-6 rdfs:label "6"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-6_10 rdfs:label "[6, 10]"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-7 rdfs:label "7"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-855 rdfs:label ".855"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-86376385_2019 rdfs:label "86:376–385, 2019"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-864 rdfs:label ".864"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-867 rdfs:label ".867"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-875 rdfs:label ".875"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-884 rdfs:label ".884"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-886 rdfs:label ".886"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-891 rdfs:label ".891"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-897 rdfs:label ".897"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-9 rdfs:label "9"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-901 rdfs:label ".901"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-903 rdfs:label ".903"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-914 rdfs:label ".914"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-919 rdfs:label ".919"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-92-109 rdfs:label "92-109"@en ;
    askg-onto:entityType "Score"@en .

askg-data:Entity-920 rdfs:label ".920"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-930 rdfs:label ".930"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-934 rdfs:label ".934"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-938 rdfs:label ".938"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-951 rdfs:label ".951"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-967 rdfs:label ".967"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-a_function_based_on_ir rdfs:label "a function based on I^{r}"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-a_multiple_noisy_labeling_perspective rdfs:label "A Multiple Noisy Labeling Perspective"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-a_new_state-of-the-art rdfs:label "a new state-of-the-art"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-aaron_c rdfs:label "Aaron C."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ablation rdfs:label "Ablation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-abs190204601 rdfs:label "abs/1902.04601"@en ;
    askg-onto:entityType "Article"@en .

askg-data:Entity-abubakar_abid rdfs:label "Abubakar Abid"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-acknowledgments rdfs:label "Acknowledgments"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-acm_icimcs rdfs:label "ACM ICIMCS"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-acrv rdfs:label "ACRV"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-adam_method rdfs:label "Adam method"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-adaptive_fusion rdfs:label "Adaptive Fusion"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-adaptive_threshold rdfs:label "adaptive threshold"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-adrien_ali_taga rdfs:label "Adrien Ali Taga"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-advances_in_biomaterials rdfs:label "Advances in Biomaterials"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-afnet rdfs:label "AFNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-afnet_ctmf_mmci_pcf_tanet_cpfp_dmra_uc-net rdfs:label "AFNet CTMF MMCI PCF TANet CPFP DMRA UC-Net"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-aixuan_li rdfs:label "Aixuan Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-akshaya_mishra rdfs:label "Akshaya Mishra"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-alex_kendall rdfs:label "Alex Kendall"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-algorithms rdfs:label "algorithms"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ali_eslami rdfs:label "Ali Eslami"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ambiguity rdfs:label "Ambiguity"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-amirhossein_habibian rdfs:label "Amirhossein Habibian"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-an_uncertain_future rdfs:label "An Uncertain Future"@en ;
    askg-onto:entityType "Article"@en .

askg-data:Entity-analytical_technique rdfs:label "Analytical Technique"@en ;
    askg-onto:entityType "Technology"@en .

askg-data:Entity-andreas_m rdfs:label "Andreas M."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-andrew_achkar rdfs:label "Andrew Achkar"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-andrew_zisserman rdfs:label "Andrew Zisserman"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-andy_song rdfs:label "Andy Song"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-angle rdfs:label "angle"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-anisotropic_center-surround_difference rdfs:label "anisotropic center-surround difference"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-annotation rdfs:label "annotation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-annotations rdfs:label "annotations"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-annotators rdfs:label "annotators"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-another_convolutional_layer rdfs:label "another convolutional layer"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-anton_s rdfs:label "Anton S."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-approximate_inference rdfs:label "Approximate Inference"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-architecture rdfs:label "Architecture"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-area_chairs rdfs:label "Area Chairs"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-artificial_intelligence rdfs:label "Artificial Intelligence"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-arxiv190800733 rdfs:label "arXiv:1908.00733"@en ;
    askg-onto:entityType "Article"@en,
        "Publication"@en .

askg-data:Entity-arxiv_e-prints rdfs:label "arXiv e-prints"@en ;
    askg-onto:entityType "Platform"@en .

askg-data:Entity-attention rdfs:label "attention"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-attention-aware_cross-level_combination_blocks rdfs:label "attention-aware cross-level combination blocks"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-augedgt rdfs:label "AugedGT"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-augmented_training_data rdfs:label "augmented training data"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-australia_research_council_centre_of_excellence_for_robotics_vision rdfs:label "Australia Research Council Centre of Excellence for Robotics Vision"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-australian_national_university rdfs:label "Australian National University"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-auto-encoding_variational_bayes rdfs:label "Auto-Encoding Variational Bayes"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-auxiliary_component rdfs:label "auxiliary component"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-auxiliary_posterior_q%CF%86zx_y rdfs:label "auxiliary posterior Qφ(z|*X, Y*)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-background rdfs:label "background"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-base_learning_rate rdfs:label "base learning rate"@en ;
    askg-onto:entityType "Rate"@en .

askg-data:Entity-basnet rdfs:label "BASNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-baumgartner rdfs:label "Baumgartner"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-bayesian_segnet rdfs:label "Bayesian SegNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-becker rdfs:label "Becker"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-behavior_research_methods rdfs:label "Behavior Research Methods"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-benchmark_datasets rdfs:label "benchmark datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-bernardino_romera-paredes rdfs:label "Bernardino Romera-Paredes"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-better_performance rdfs:label "better performance"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-bias rdfs:label "bias"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-bilinear_upsampling_operation rdfs:label "bilinear upsampling operation"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-binary_foreground_map_evaluation rdfs:label "Binary Foreground Map Evaluation"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-binary_prediction rdfs:label "binary prediction"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-binary_predictions rdfs:label "binary predictions"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-binary_version rdfs:label "binary version"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-bing_li rdfs:label "Bing Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-biochemistry rdfs:label "Biochemistry"@en ;
    askg-onto:entityType "Research Area"@en .

askg-data:Entity-bjrn_ommer rdfs:label "Bjrn Ommer"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-bmvc rdfs:label "BMVC"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-bo_li rdfs:label "Bo Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-bo_ren rdfs:label "Bo Ren"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-bootstrap_learning rdfs:label "bootstrap learning"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-boundary-aware_salient_object_detection rdfs:label "Boundary-Aware Salient Object Detection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-bowen_liu rdfs:label "Bowen Liu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-branches rdfs:label "branches"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-brostow_clment_godard rdfs:label "Brostow Clment Godard"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-c1_m rdfs:label "c1 M"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-c_times rdfs:label "C times"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-caffe rdfs:label "Caffe"@en ;
    askg-onto:entityType "Software"@en .

askg-data:Entity-cambridge_centre_for_medical_materials rdfs:label "Cambridge Centre for Medical Materials"@en ;
    askg-onto:entityType "Research Group"@en .

askg-data:Entity-carl_doersch rdfs:label "Carl Doersch"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-cdcp_acsd_lbe_dcmc_mdsf rdfs:label "CDCP ACSD LBE DCMC MDSF"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-center-dark_channel_prior rdfs:label "center-dark channel prior"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-changqing_zhang rdfs:label "Changqing Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-channel_size_k rdfs:label "channel size K"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-channel_size_of_s_das_m rdfs:label "channel size of S das M"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-chao_gao rdfs:label "Chao Gao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-chen_et_al rdfs:label "Chen et al."@en ;
    askg-onto:entityType "Author"@en .

askg-data:Entity-cheng_gong rdfs:label "Cheng Gong"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-chenggang_yan rdfs:label "Chenggang Yan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-chenyang_huang rdfs:label "Chenyang Huang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-chi_zhang rdfs:label "Chi Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-chris_mccarthy rdfs:label "Chris McCarthy"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-christian_f rdfs:label "Christian F."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-christian_rupprecht rdfs:label "Christian Rupprecht"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-chunbiao_zhu rdfs:label "Chunbiao Zhu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-chunping_hou rdfs:label "Chunping Hou"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-clemens_meyer rdfs:label "Clemens Meyer"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-clinical_trials rdfs:label "Clinical Trials"@en ;
    askg-onto:entityType "Study"@en .

askg-data:Entity-cnn rdfs:label "CNN"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-cnns rdfs:label "CNNs"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-co-salient_object_detection rdfs:label "Co-salient Object Detection"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-co-sod rdfs:label "Co-SOD"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-color_components_in_the_linear_color_space_after_gamma_function_been_removed_from_the_original_color_space rdfs:label "color components in the linear color space after Gamma function been removed from the original color space"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-color_image rdfs:label "color image"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-complementary-aware_fusion_block rdfs:label "complementary-aware fusion block"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-complex_image rdfs:label "complex image"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-complex_vision_tasks rdfs:label "complex vision tasks"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-computer_vision_problems rdfs:label "computer vision problems"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-computing_depth rdfs:label "computing depth"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-conditional_counterpart_cvae rdfs:label "conditional counterpart CVAE"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-conditional_generative_model rdfs:label "conditional generative model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-conditional_probabilistic_rgb-d_saliency_prediction_model rdfs:label "conditional probabilistic RGB-D saliency prediction model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-conditional_variational_autoencoder_cvae rdfs:label "Conditional Variational Autoencoder (CVAE)"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-conditioning_variable_x rdfs:label "conditioning variable X"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-consensus_of_predictions rdfs:label "consensus of predictions"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-consistent_prediction rdfs:label "consistent prediction"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-consistent_result rdfs:label "consistent result"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-contrast_and_depth-guided-background_prior rdfs:label "contrast and depth-guided-background prior"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-contrast_prior rdfs:label "contrast prior"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-contrastive_variational_autoencoder rdfs:label "Contrastive Variational Autoencoder"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-convolutional_layer rdfs:label "convolutional layer"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-convolutional_layer_of_kernel_size_1__1 rdfs:label "convolutional layer of kernel size 1 × 1"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-convolutional_neural_network rdfs:label "Convolutional Neural Network"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-corpus rdfs:label "Corpus"@en ;
    askg-onto:entityType "Corpus"@en .

askg-data:Entity-corr rdfs:label "CoRR"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-courville_ishaan_gulrajani rdfs:label "Courville Ishaan Gulrajani"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-covid-19 rdfs:label "COVID-19"@en ;
    askg-onto:entityType "Disease"@en .

askg-data:Entity-cpfp_and_dmra rdfs:label "CPFP and DMRA"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-crispr rdfs:label "CRISPR"@en ;
    askg-onto:entityType "Concept"@en,
        "Technology"@en .

askg-data:Entity-crispr-cas9 rdfs:label "CRISPR-Cas9"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-crispr_research rdfs:label "CRISPR research"@en ;
    askg-onto:entityType "Research Area"@en .

askg-data:Entity-cross-level_fusion_models rdfs:label "cross-level fusion models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-crossview_transfer rdfs:label "crossview transfer"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-cs_nankai_university rdfs:label "CS, Nankai University"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-d rdfs:label "D"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-d0 rdfs:label "D0"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-d0i rdfs:label "D0i"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-d_klq_%09extphizxyp_%09hetazx rdfs:label "D_{KL}(Q_{	extphi}(z|X,Y)||P_{	heta}(z|X))"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-daan_wierstra rdfs:label "Daan Wierstra"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-dacheng_tao rdfs:label "Dacheng Tao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-dan_su rdfs:label "Dan Su"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-dapeng_du rdfs:label "Dapeng Du"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-data61 rdfs:label "Data61"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-data_analysis rdfs:label "Data Analysis"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-data_labeling_process rdfs:label "data labeling process"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-data_source_selection rdfs:label "data source selection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-dataset_creators rdfs:label "dataset creators"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-david_feng rdfs:label "David Feng"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-david_vzquez rdfs:label "David Vzquez"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-decoder rdfs:label "decoder"@en ;
    askg-onto:entityType "Model"@en,
        "Technique"@en .

askg-data:Entity-decoder_part rdfs:label "decoder part"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-deep_conditional_generative_models rdfs:label "Deep Conditional Generative Models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-deep_convolutional_encoder-decoder_architectures rdfs:label "Deep Convolutional Encoder-Decoder Architectures"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-deep_convolutional_neural_network_based_models rdfs:label "deep convolutional neural network based models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-deep_fusion rdfs:label "deep fusion"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-deep_generative_models rdfs:label "Deep Generative Models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-deep_models rdfs:label "Deep Models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-deep_neural_network rdfs:label "deep neural network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-deep_part-object_relationships rdfs:label "Deep Part-Object Relationships"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-deep_saliency_frameworks rdfs:label "deep saliency frameworks"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-deep_unsupervised_saliency_detection rdfs:label "Deep Unsupervised Saliency Detection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-deforming_3d_mesh_models rdfs:label "Deforming 3D Mesh Models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-denseaspp_module rdfs:label "DenseASPP module"@en ;
    askg-onto:entityType "Technology"@en .

askg-data:Entity-depth-aware_salient_object_detection_and_segmentation rdfs:label "Depth-aware salient object detection and segmentation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-depth-correctionnet rdfs:label "Depth-CorrectionNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-depth-induced_multi-scale_recurrent_attention_network rdfs:label "Depth-induced Multi-scale Recurrent Attention Network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-depth_confidence_analysis rdfs:label "depth confidence analysis"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-depth_correction_net rdfs:label "depth correction net"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-depth_cues rdfs:label "depth cues"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-depth_data_noise_reduction rdfs:label "depth data noise reduction"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-depth_enhanced_saliency_detection rdfs:label "Depth enhanced saliency detection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-depth_images rdfs:label "depth images"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-depth_information rdfs:label "depth information"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-depth_map rdfs:label "depth map"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-depth_map_d0 rdfs:label "depth map D0"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-depth_saliency rdfs:label "Depth saliency"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-depth_sensing_technologies rdfs:label "depth sensing technologies"@en ;
    askg-onto:entityType "Technology"@en .

askg-data:Entity-depth_sensors rdfs:label "depth sensors"@en ;
    askg-onto:entityType "Technology"@en .

askg-data:Entity-depthinduced_multi-scale_rgb-d_saliency_detection_network rdfs:label "depthinduced multi-scale RGB-D saliency detection network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-deterministic_feature_s_d rdfs:label "deterministic feature S d"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-deterministic_features_s_d_i rdfs:label "deterministic features S d i"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-deterministic_learning_pipeline rdfs:label "deterministic learning pipeline"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-deterministic_saliency_features_s_d rdfs:label "deterministic saliency features S d"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-device rdfs:label "Device"@en ;
    askg-onto:entityType "Device"@en .

askg-data:Entity-di rdfs:label "Di"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-diederik_p_kingma rdfs:label "Diederik P Kingma"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-difference_of_motion_modes rdfs:label "difference of motion modes"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-different_modality_information_fusion rdfs:label "different modality information fusion"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-different_predictions rdfs:label "different predictions"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-disagreement rdfs:label "disagreement"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-disease rdfs:label "Disease"@en ;
    askg-onto:entityType "Disease"@en .

askg-data:Entity-distribution_estimation_problem rdfs:label "distribution estimation problem"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-distribution_of_saliency_maps rdfs:label "distribution of saliency maps"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-diverse_annotations rdfs:label "diverse annotations"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-diverse_saliency_predictions rdfs:label "diverse saliency predictions"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-diversity_of_prediction rdfs:label "Diversity of prediction"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-diversityuncertainty rdfs:label "diversity/uncertainty"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-dkl rdfs:label "DKL"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-dklq%CF%86zx_y rdfs:label "DKL(Qφ(z|*X, Y*))"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-dp1 rdfs:label "DP1"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-dp2 rdfs:label "DP2"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-dyer rdfs:label "Dyer"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-e%CE%BE rdfs:label "Eξ"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-e-measure_and_f-measure_curves rdfs:label "E-measure and F-measure curves"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-e-measure_performance_boost rdfs:label "E-measure performance boost"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-e_z%09extsim_q_%09extphizxy rdfs:label "E_{z	extsim Q_{	extphi}(z|X,Y)}"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-each_module rdfs:label "each module"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-early-fusion_model rdfs:label "early-fusion model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-early-fusion_models rdfs:label "early-fusion models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-eccvw rdfs:label "ECCVW"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-edge-aware_saliency_detection rdfs:label "edge-aware saliency detection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-edge_guidance_network_for_salient_object_detection rdfs:label "Edge guidance network for salient object detection"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-effective_in_rgb-d_saliency_detection rdfs:label "effective in RGB-D saliency detection"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-effectiveness_of_our_approach rdfs:label "effectiveness of our approach"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-effectiveness_of_uc-net rdfs:label "effectiveness of UC-Net"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-egnet rdfs:label "EGNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-eight_deep_models rdfs:label "eight deep models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-eight_deep_rgb-d_saliency_detection_models rdfs:label "eight deep RGB-D saliency detection models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-ekaterina_sutter rdfs:label "Ekaterina Sutter"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-emmanuelle_charpentier rdfs:label "Emmanuelle Charpentier"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-encoder_of_saliencynet rdfs:label "encoder of SaliencyNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-encoding_error rdfs:label "encoding error"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-encoding_error_dklq%CF%86zx_y_p%CE%B8zx rdfs:label "encoding error DKL(Qφ(z|*X, Y* )||Pθ(z|X))"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ender_konukoglu rdfs:label "Ender Konukoglu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-enhanced-alignment_measure rdfs:label "Enhanced-alignment Measure"@en ;
    askg-onto:entityType "Measure"@en .

askg-data:Entity-enhanced_alignment_measure rdfs:label "Enhanced alignment measure"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-epoch rdfs:label "epoch"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-eq rdfs:label "Eq"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-ernst_niebur rdfs:label "Ernst Niebur"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-es_d rdfs:label "ES D"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-existing_approaches rdfs:label "Existing approaches"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-existing_rgb_saliency_detection_model rdfs:label "existing RGB saliency detection model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-exploiting_global_priors rdfs:label "Exploiting Global Priors"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-f%CE%B2 rdfs:label "Fβ"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-f-measure_curves rdfs:label "F-measure curves"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-f-measure_performance_boost rdfs:label "F-measure performance boost"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-f3net rdfs:label "F3Net"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-factors rdfs:label "factors"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-fangfang_liang rdfs:label "Fangfang Liang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-faruk_ahmed rdfs:label "Faruk Ahmed"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-feature rdfs:label "Feature"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-feature_expanding_module rdfs:label "Feature Expanding module"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-feature_figure_5 rdfs:label "feature Figure 5"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-feature_generation rdfs:label "feature generation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-feature_map rdfs:label "feature map"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-feature_maps rdfs:label "feature maps"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-features rdfs:label "features"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-federico_tombari rdfs:label "Federico Tombari"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-feng_liu rdfs:label "Feng Liu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-field rdfs:label "Field"@en ;
    askg-onto:entityType "Domain"@en .

askg-data:Entity-figure_1 rdfs:label "Figure 1"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-figure_6 rdfs:label "Figure 6"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-figure_7 rdfs:label "Figure 7"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-figure_8 rdfs:label "Figure 8"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-final_ground_truth_saliency_map rdfs:label "final ground truth saliency map"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-first-order_derivatives_of_the_saliency_map rdfs:label "first-order derivatives of the saliency map"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-first_click_attention rdfs:label "First Click Attention"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-first_stage_of_the_vgg16_network rdfs:label "first stage of the VGG16 network"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-fluid_pyramid_integration_framework rdfs:label "fluid pyramid integration framework"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-forecasting rdfs:label "Forecasting"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-foreground rdfs:label "foreground"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-four_sequential_convolutional_layers rdfs:label "four sequential convolutional layers"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-four_testing_datasets rdfs:label "four testing datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-francesco_visin rdfs:label "Francesco Visin"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-francisco_estrada rdfs:label "Francisco Estrada"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-frequency-tuned_salient_region_detection rdfs:label "Frequency-tuned salient region detection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-full_loss rdfs:label "full loss"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-fully_connected_layers rdfs:label "fully connected layers"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-functional_mri rdfs:label "Functional MRI"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-gabriel_j rdfs:label "Gabriel J."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-gaussian_distribution rdfs:label "Gaussian distribution"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-gaussian_distribution_p%CE%B8zx rdfs:label "Gaussian distribution Pθ(z|X)"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-gaussian_latent_space rdfs:label "Gaussian latent space"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ge_li rdfs:label "Ge Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-generative_shape_proposal_network rdfs:label "Generative Shape Proposal Network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-genetic_engineering rdfs:label "genetic engineering"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-genetics rdfs:label "genetics"@en ;
    askg-onto:entityType "Research Field"@en .

askg-data:Entity-geometric_and_semantic_information rdfs:label "geometric and semantic information"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-gp rdfs:label "GP"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-gradients rdfs:label "gradients"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-gray_saliency_map rdfs:label "gray saliency map"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-gregory_d rdfs:label "Gregory D."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ground_truth rdfs:label "ground truth"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ground_truth_acquisition rdfs:label "ground truth acquisition"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-ground_truth_saliency_map rdfs:label "ground truth saliency map"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ground_truth_y rdfs:label "ground truth Y"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-gspn rdfs:label "GSPN"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-gt_map_y rdfs:label "GT map Y"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-gt_saliency_map_generation rdfs:label "GT saliency map generation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-gt_y rdfs:label "GT Y"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-guangling_sun rdfs:label "Guangling Sun"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-guibas rdfs:label "Guibas"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-guolei_sun rdfs:label "Guolei Sun"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-hadap_sunil rdfs:label "Hadap Sunil"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-hager rdfs:label "Hager"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-haibin_ling rdfs:label "Haibin Ling"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-han_et_al rdfs:label "Han et al"@en ;
    askg-onto:entityType "Research Group"@en .

askg-data:Entity-handcrafted_feature_based_models rdfs:label "Handcrafted Feature based Models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-hangke_song rdfs:label "Hangke Song"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-harikrishna_mulam rdfs:label "Harikrishna Mulam"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-harvard_university rdfs:label "Harvard University"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-he_wang rdfs:label "He Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-height_above_ground rdfs:label "height above ground"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-hidden_image rdfs:label "hidden image"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-hide-and-seek rdfs:label "Hide-and-Seek"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-hide_and_seek_principle rdfs:label "hide and seek principle"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-hiding_technique rdfs:label "hiding technique"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-high_accuracy_predictions rdfs:label "high accuracy predictions"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-honglak_lee rdfs:label "Honglak Lee"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-horizontal_disparity rdfs:label "horizontal disparity"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-hotker_urs_j rdfs:label "Hotker, Urs J."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-houwen_peng rdfs:label "Houwen Peng"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-httpsgithubcomjingzhang617ucnet rdfs:label "https://github.com/JingZhang617/UCNet"@en ;
    askg-onto:entityType "Platform"@en .

askg-data:Entity-huaizu_jiang rdfs:label "Huaizu Jiang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-huan_du rdfs:label "Huan Du"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-huchuan_lu rdfs:label "Huchuan Lu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-huiling_wang rdfs:label "Huiling Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-human_annotation_uncertainty rdfs:label "human annotation uncertainty"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-human_annotations rdfs:label "human annotations"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-human_consensus rdfs:label "human consensus"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-human_labeling rdfs:label "human labeling"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-human_motion_prediction rdfs:label "human motion prediction"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-human_perceptual_uncertainty rdfs:label "human perceptual uncertainty"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-human_vision_system rdfs:label "human vision system"@en ;
    askg-onto:entityType "System"@en .

askg-data:Entity-human_visual_perception rdfs:label "human visual perception"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-i rdfs:label "I"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-i_lr rdfs:label "I lr"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-i_lr_i_lg_and_i_lb rdfs:label "I lr, I lg and I lb"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-icip rdfs:label "ICIP"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-icme rdfs:label "ICME"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-icml rdfs:label "ICML"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-ieee_access rdfs:label "IEEE Access"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-ieee_cvprw rdfs:label "IEEE CVPRW"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-ieee_iccvw rdfs:label "IEEE ICCVW"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-ieee_spl rdfs:label "IEEE SPL"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-ieee_tcyb rdfs:label "IEEE TCYB"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-ieee_tnnls rdfs:label "IEEE TNNLS"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-ieee_tpami rdfs:label "IEEE TPAMI"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-igu_v rdfs:label "Ig(*u, v*)"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-ii rdfs:label "Ii"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-ii_di rdfs:label "{Ii, Di}"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ijcai rdfs:label "IJCAI"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-ilr rdfs:label "I^{lr}"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-image rdfs:label "Image"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-image_background rdfs:label "image background"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-image_background_modeling rdfs:label "image background modeling"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-image_classification rdfs:label "Image Classification"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-image_segmentation rdfs:label "image segmentation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-imagenet rdfs:label "ImageNet"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-images rdfs:label "images"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-images_with_complex_background rdfs:label "images with complex background"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-images_with_complicated_context rdfs:label "images with complicated context"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-imaging_technology rdfs:label "Imaging Technology"@en ;
    askg-onto:entityType "Technology"@en .

askg-data:Entity-inception_institute_of_artificial_intelligence_iiai rdfs:label "Inception Institute of Artificial Intelligence (IIAI)"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-inference_time rdfs:label "inference time"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-inferred_gravity_direction rdfs:label "inferred gravity direction"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-input_data_x rdfs:label "input data X"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-input_image_size rdfs:label "input image size"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-input_observation rdfs:label "input observation"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-input_rgb-d_image_pair_x rdfs:label "input RGB-D image pair X"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-intensity_of_rgb_image_ig rdfs:label "intensity of RGB image Ig"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-interactive_image_segmentation rdfs:label "Interactive Image Segmentation"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-interclass_distinction_and_intra-class_similarity rdfs:label "interclass distinction and intra-class similarity"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ir rdfs:label "I^{r}"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-iro_laina rdfs:label "Iro Laina"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-iterative_and_cooperative_top-down_and_bottom-up_inference_network rdfs:label "Iterative and Cooperative Top-Down and Bottom-Up Inference Network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-iterative_hiding_technique rdfs:label "iterative hiding technique"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-jacob_walker rdfs:label "Jacob Walker"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-james_y rdfs:label "James Y."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jeffrey_de_fauw rdfs:label "Jeffrey De Fauw"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jennifer_doudna rdfs:label "Jennifer Doudna"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jia_bei rdfs:label "Jia Bei"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jia_li rdfs:label "Jia Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jiandong_tian rdfs:label "Jiandong Tian"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jiangjian_xiao rdfs:label "Jiangjian Xiao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jianjun_lei rdfs:label "Jianjun Lei"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jianqiang_ren rdfs:label "Jianqiang Ren"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jiawei_zhang rdfs:label "Jiawei Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jingfan_guo rdfs:label "Jingfan Guo"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jingjing_li rdfs:label "Jingjing Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jingyi_yu rdfs:label "Jingyi Yu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jingzhang617 rdfs:label "JingZhang617"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jinwei_ye rdfs:label "Jinwei Ye"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jitendra_malik rdfs:label "Jitendra Malik"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jl-dcf rdfs:label "JL-DCF"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-joint_learning_and_densely-cooperative_fusion_framework_for_rgb-d_salient_object_detection rdfs:label "Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-joseph_r rdfs:label "Joseph R."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jufeng_yang rdfs:label "Jufeng Yang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jun_wei rdfs:label "Jun Wei"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jungong_han rdfs:label "Jungong Han"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-junwei_han rdfs:label "Junwei Han"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-justin_eichel rdfs:label "Justin Eichel"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-k__32 rdfs:label "K = 32"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-k__m_channel_feature_map_s_sd rdfs:label "K + M channel feature map S sd"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-k__m_dimensional_variable_r rdfs:label "K + M dimensional variable r"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-k_dimensional_vector rdfs:label "K dimensional vector"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-kai_zhao rdfs:label "Kai Zhao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-karen_simonyan rdfs:label "Karen Simonyan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-kerem_can_tezcan rdfs:label "Kerem Can Tezcan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-keren_fu_fu rdfs:label "Keren Fu Fu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-khoschy rdfs:label "Khoschy"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-kihyuk_sohn rdfs:label "Kihyuk Sohn"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-kingma rdfs:label "Kingma"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-kl-divergence rdfs:label "KL-Divergence"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-klaus_maier-hein rdfs:label "Klaus Maier-Hein"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-krishna_chaitanya rdfs:label "Krishna Chaitanya"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-krishna_kumar_singh rdfs:label "Krishna Kumar Singh"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-kuiyuan_yang rdfs:label "Kuiyuan Yang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-kun_yu rdfs:label "Kun Yu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-kundan_kumar rdfs:label "Kundan Kumar"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-label_generation_technique rdfs:label "label generation technique"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-labeling rdfs:label "labeling"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-laiyun_qing rdfs:label "Laiyun Qing"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-large-scale_benchmarks rdfs:label "large-scale benchmarks"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-large-scale_image_recognition rdfs:label "Large-Scale Image Recognition"@en ;
    askg-onto:entityType "Research Area"@en .

askg-data:Entity-larger_is_better rdfs:label "larger is better"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-lars_petersson rdfs:label "Lars Petersson"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-late-fusion_models rdfs:label "late-fusion models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-late-fusion_network rdfs:label "late-fusion network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-latent_gaussian_variable_z rdfs:label "latent Gaussian variable z"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-latent_representation rdfs:label "latent representation"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-latent_representations_with_sharp_samples rdfs:label "latent representations with sharp samples"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-latent_space rdfs:label "latent space"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-latent_variable_model rdfs:label "Latent Variable Model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-latentnet_n_%C2%B5prior_diag%CF%83_2prior rdfs:label "LatentNet N (µprior*, diag*(σ 2prior))"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-lcvae rdfs:label "LCVAE"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-lcvae__%CE%BB1ldepth__%CE%BB2lsmooth rdfs:label "LCVAE + λ1LDepth + λ2LSmooth"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-ldepth rdfs:label "LDepth"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-le_zhang rdfs:label "Le Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-learning_in_an_uncertain_world rdfs:label "Learning in an Uncertain World"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-learning_rate rdfs:label "learning rate"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-learning_structured_output_representation rdfs:label "Learning Structured Output Representation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-learning_variations_in_human_motion rdfs:label "Learning Variations in Human Motion"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ledsam rdfs:label "Ledsam"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-lee_honglak rdfs:label "Lee Honglak"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-left-right_consistency rdfs:label "Left-Right Consistency"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-leonidas_j rdfs:label "Leonidas J."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-lhm_cdb_desm rdfs:label "LHM CDB DESM"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-li rdfs:label "Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-li_et_al rdfs:label "Li et al"@en ;
    askg-onto:entityType "Research Group"@en .

askg-data:Entity-li_yi rdfs:label "Li Yi"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-liang_zhao rdfs:label "Liang Zhao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-liangqiong_qu rdfs:label "Liangqiong Qu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-light_field rdfs:label "light field"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-lijuan_duan rdfs:label "Lijuan Duan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-limin_wang rdfs:label "Limin Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-lin-zhuo_chen rdfs:label "Lin-Zhuo Chen"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-lin_gao rdfs:label "Lin Gao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ling_ge rdfs:label "Ling Ge"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-lioub rdfs:label "LIoub"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-local_background_enclosure rdfs:label "Local background enclosure"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-log_likelihood_py rdfs:label "log likelihood P(Y)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-loss rdfs:label "loss"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-loss_function rdfs:label "loss function"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-low-resolution_grayscale_images rdfs:label "Low-Resolution Grayscale Images"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-low_dimensional_gaussian_latent_variable_z rdfs:label "low dimensional Gaussian latent variable z"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-lsal rdfs:label "Lsal"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-lsl rdfs:label "Lsl"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-lu_yu rdfs:label "Lu Yu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-m-head_based_method rdfs:label "M-head based method"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-m-head_based_models rdfs:label "M-head based models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-m2 rdfs:label "M2"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-machine_learning rdfs:label "Machine Learning"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-mae rdfs:label "MAE"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-magnitude_of_d0_and_ig rdfs:label "magnitude of D0 and Ig"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-majority_voting_mechanism rdfs:label "majority voting mechanism"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-majority_voting_of_puv rdfs:label "majority voting of Pu,v"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-majority_voting_strategy rdfs:label "majority voting strategy"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-maoke_yang rdfs:label "Maoke Yang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-martial_hebert rdfs:label "Martial Hebert"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-martin_jagersand rdfs:label "Martin Jagersand"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-masood_dehghan rdfs:label "Masood Dehghan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-mass_spectrometry rdfs:label "Mass Spectrometry"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-mathieu_salzmann rdfs:label "Mathieu Salzmann"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-max_welling rdfs:label "Max Welling"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-maximilian_baust rdfs:label "Maximilian Baust"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-maximum_epoch rdfs:label "maximum epoch"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-mc-dropout_based_models rdfs:label "MC-dropout based models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-mean_absolute_error rdfs:label "Mean Absolute Error"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-mean_e%CE%BE rdfs:label "mean Eξ"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-mean_f%CE%B2 rdfs:label "mean Fβ"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-mean_f-measure rdfs:label "mean F-measure"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-mean_of_the_multiple_predictions rdfs:label "mean of the multiple predictions"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-measure rdfs:label "Measure"@en ;
    askg-onto:entityType "Measure"@en .

askg-data:Entity-mechanism rdfs:label "mechanism"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-medical_image_segmentation_model rdfs:label "medical image segmentation model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-mehrtash_harandi rdfs:label "Mehrtash Harandi"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-menglong_zhu rdfs:label "Menglong Zhu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-mh1 rdfs:label "MH1"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-mh2 rdfs:label "MH2"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-miao_zhang rdfs:label "Miao Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-michael_ying_yang rdfs:label "Michael Ying Yang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-microsoft_kinect rdfs:label "Microsoft Kinect"@en ;
    askg-onto:entityType "Device"@en .

askg-data:Entity-minhyuk_sung rdfs:label "Minhyuk Sung"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-mix-and-match_perturbation rdfs:label "Mix-and-Match Perturbation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-mixed_feature_s_msd rdfs:label "mixed feature S msd"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-modeling_inherent_ambiguities_of_an_image rdfs:label "modeling inherent ambiguities of an image"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-models rdfs:label "Models"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-module rdfs:label "Module"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-modules rdfs:label "modules"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-mohammad_sadegh_aliakbarian rdfs:label "Mohammad Sadegh Aliakbarian"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-momentum_09 rdfs:label "momentum 0.9"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-mt-vae rdfs:label "MT-VAE"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-muehlematter rdfs:label "Muehlematter"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-multi-head rdfs:label "Multi-head"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-multi-head_models rdfs:label "Multi-head models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-multi-modal_fusion_network rdfs:label "Multi-modal fusion network"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-multi-modality_fusion rdfs:label "multi-modality fusion"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-multi-scale_cross-modal_feature_fusion rdfs:label "multi-scale cross-modal feature fusion"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-multi-scale_multi-path_network rdfs:label "multi-scale multi-path network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-multiple_cues_fusion rdfs:label "multiple cues fusion"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-multiple_hypotheses rdfs:label "Multiple Hypotheses"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-multiple_labels rdfs:label "multiple labels"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-multiple_saliency_maps rdfs:label "multiple saliency maps"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-multiple_salient_objects rdfs:label "multiple salient objects"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-multiscale_discriminative_saliency_fusion rdfs:label "multiscale discriminative saliency fusion"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-multiview_fusion rdfs:label "multiview fusion"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-n_0_001 rdfs:label "N (0, 0.01)"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-nassir_navab rdfs:label "Nassir Navab"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-national_institutes_of_health rdfs:label "National Institutes of Health"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-national_key_research_and_development_program_of_china rdfs:label "National Key Research and Development Program of China"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-natural_images rdfs:label "Natural Images"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-natural_science_foundation_of_china rdfs:label "Natural Science Foundation of China"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-need_intensity rdfs:label "need intensity"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-network_performance rdfs:label "network performance"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-network_structure rdfs:label "network structure"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-network_training_pipeline rdfs:label "Network training pipeline"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-neural_network_architecture rdfs:label "neural network architecture"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-neurips rdfs:label "NeurIPS"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-neurocomputing rdfs:label "Neurocomputing"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-neuroscience rdfs:label "Neuroscience"@en ;
    askg-onto:entityType "Research Area"@en .

askg-data:Entity-new_label_generation rdfs:label "New label generation"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-nian_liu rdfs:label "Nian Liu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-nianyi_li rdfs:label "Nianyi Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ningning_wang rdfs:label "Ningning Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-nju2 rdfs:label "NJU2"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-non-local_deep_features rdfs:label "Non-Local Deep Features"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-normal_distribution rdfs:label "Normal Distribution"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-northwestern_polytechnical_university rdfs:label "Northwestern Polytechnical University"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-number rdfs:label "number"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-nvidia_geforce_rtx_gpu rdfs:label "NVIDIA GeForce RTX GPU"@en ;
    askg-onto:entityType "Equipment"@en .

askg-data:Entity-object-level_visual_saliency_detection rdfs:label "Object-level visual saliency detection"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-object_detection_and_segmentation rdfs:label "object detection and segmentation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-occlusion_aware_unsupervised_learning_of_optical_flow rdfs:label "Occlusion Aware Unsupervised Learning of Optical Flow"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-oisin_mac_aodha rdfs:label "Oisin Mac Aodha"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-olaf_ronneberger rdfs:label "Olaf Ronneberger"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-olivio_donati rdfs:label "Olivio Donati"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-on_x_and_y_directions rdfs:label "on −→x and −→y directions"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-one-channel_saliency_map rdfs:label "one-channel saliency map"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-one_image rdfs:label "one image"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-original_and_channel_of_image rdfs:label "original and channel of image"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-original_red_channel_of_image_i rdfs:label "original red channel of image I"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-our_framework rdfs:label "our framework"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-our_reported_results rdfs:label "our reported results"@en ;
    askg-onto:entityType "Result"@en .

askg-data:Entity-our_solution rdfs:label "our solution"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ours1 rdfs:label "Ours(1)"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-ours2 rdfs:label "Ours(2)"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-output rdfs:label "output"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-output_channel_sizes_of_k_k2_1 rdfs:label "output channel sizes of K, K/2, 1"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-overt_and_covert_shifts_of_visual_attention rdfs:label "overt and covert shifts of visual attention"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-p_%09extomegayxz rdfs:label "P_{	extomega}(Y|X,z)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-p_1__p_c rdfs:label "P 1*, ..., P* C"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-p_bc rdfs:label "P_{b}^{c}"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-p_c_bu_v rdfs:label "P c b(u, v)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-p_gmjv rdfs:label "P_{g}^{mjv}"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-p_mjv_b rdfs:label "P mjv b"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-p_mjv_bu_v rdfs:label "P mjv b(*u, v*)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-pablo_arbelaez rdfs:label "Pablo Arbelaez"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-paper rdfs:label "paper"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-parameter_set_%CE%B8 rdfs:label "parameter set θ"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-part rdfs:label "part"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-patrick_esser rdfs:label "Patrick Esser"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-pc rdfs:label "PC"@en ;
    askg-onto:entityType "Device"@en .

askg-data:Entity-peipei_song rdfs:label "Peipei Song"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-peng_wang rdfs:label "Peng Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-performance_improvement rdfs:label "performance improvement"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-phiseg rdfs:label "PHiSeg"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-pierre-marc_jodoin rdfs:label "Pierre-Marc Jodoin"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-pixel rdfs:label "pixel"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-pixels_local_surface_normal rdfs:label "pixels local surface normal"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-pixelvae rdfs:label "PixelVAE"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-point_cloud rdfs:label "Point Cloud"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-point_cloud_instance_segmentation rdfs:label "point cloud instance segmentation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-point_estimation rdfs:label "point estimation"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-position_in_z rdfs:label "position in z"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-possible_ranking_of_1_2__k__m rdfs:label "possible ranking of 1, 2*, ..., K* + M"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-posterior_distribution rdfs:label "posterior distribution"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-posterior_net_parameter_set rdfs:label "posterior net parameter set"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-pr rdfs:label "PR"@en ;
    askg-onto:entityType "Article"@en .

askg-data:Entity-prediction rdfs:label "prediction"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-prediction-net rdfs:label "Prediction-Net"@en ;
    askg-onto:entityType "Framework"@en,
        "Model"@en .

askg-data:Entity-predictions_from_the_rgb_and_depth_branch_adaptively rdfs:label "predictions from the RGB and depth branch adaptively"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-prior_and_fluid_pyramid_integration rdfs:label "Prior and Fluid Pyramid Integration"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-prior_distribution rdfs:label "prior distribution"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-prior_on_gaussian_latent_variables rdfs:label "prior on Gaussian latent variables"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-probabilistic_framework rdfs:label "probabilistic framework"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-probabilistic_model rdfs:label "probabilistic model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-probabilistic_model_selection rdfs:label "probabilistic model selection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-probabilistic_rgb-d_saliency_detection_model rdfs:label "probabilistic RGB-D saliency detection model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-probabilistic_u-net rdfs:label "Probabilistic U-Net"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-process rdfs:label "process"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-progressively_complementarityaware_fusion_network rdfs:label "Progressively complementarityaware fusion network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-proposed_solution rdfs:label "proposed solution"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-provided_annotation_y rdfs:label "provided annotation Y"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-puv rdfs:label "Pu,v"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-qiang_zhang rdfs:label "Qiang Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-qibin_hou rdfs:label "Qibin Hou"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-qijun_zhao rdfs:label "Qijun Zhao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-qingxiong_yang rdfs:label "Qingxiong Yang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-qingyang_tan rdfs:label "Qingyang Tan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-qu_et_al rdfs:label "Qu et al"@en ;
    askg-onto:entityType "Author"@en .

askg-data:Entity-qualitative_comparisons rdfs:label "Qualitative Comparisons"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-radhakrishna_achanta rdfs:label "Radhakrishna Achanta"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ran_ju rdfs:label "Ran Ju"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-random_dropout rdfs:label "random dropout"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-random_sample rdfs:label "random sample"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-rastogi_akash rdfs:label "Rastogi Akash"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-raw_depth rdfs:label "raw depth"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-raw_depth_data rdfs:label "raw depth data"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-receptive_field rdfs:label "receptive field"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-reconstruction_loss rdfs:label "reconstruction loss"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-reconstruction_residuals rdfs:label "reconstruction residuals"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-recurrent_learning_framework rdfs:label "recurrent learning framework"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-refined_depth rdfs:label "refined depth"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-refined_depth_data rdfs:label "refined depth data"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-refined_depth_image_d0i rdfs:label "refined depth image D0i"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-refined_depth_information rdfs:label "refined depth information"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-regenerative_medicine rdfs:label "regenerative medicine"@en ;
    askg-onto:entityType "Research Area"@en .

askg-data:Entity-regression_analysis rdfs:label "Regression Analysis"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-regularization_loss rdfs:label "regularization loss"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-research_concepts rdfs:label "research concepts"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-results rdfs:label "Results"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-reviewers rdfs:label "reviewers"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-rezende rdfs:label "Rezende"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-rgb-d rdfs:label "RGB-D"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-rgb-d_benchmark_datasets rdfs:label "RGB-D benchmark datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_data rdfs:label "RGB-D data"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_datasets rdfs:label "RGB-D datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_image_pair_x rdfs:label "RGB-D image pair X"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_image_pairs rdfs:label "RGB-D image pairs"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_image_x rdfs:label "RGB-D image X"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_input_xi rdfs:label "RGB-D input Xi"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_related_dense_prediction_models rdfs:label "RGB-D related dense prediction models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-rgb-d_saliency_dataset rdfs:label "RGB-D saliency dataset"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_saliency_datasets rdfs:label "RGB-D saliency datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_saliency_detection_benchmark_datasets rdfs:label "RGB-D saliency detection benchmark datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_saliency_detection_datasets rdfs:label "RGB-D saliency detection datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_saliency_detection_method rdfs:label "RGB-D saliency detection method"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-rgb-d_saliency_detection_methods rdfs:label "RGB-D saliency detection methods"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-rgb-d_saliency_detection_model rdfs:label "RGB-D saliency detection model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-rgb-d_saliency_detection_models rdfs:label "RGB-D saliency detection models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-rgb-d_scene_recognition rdfs:label "RGB-D Scene Recognition"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-rgb_image_ig rdfs:label "RGB image Ig"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-rgb_image_ii rdfs:label "RGB image Ii"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb_images rdfs:label "RGB images"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-rgb_sod rdfs:label "RGB SOD"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-rich_semantic_and_geometric_information rdfs:label "rich semantic and geometric information"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-richard_hartley rdfs:label "Richard Hartley"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-roberto_cipolla rdfs:label "Roberto Cipolla"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ronggang_wang rdfs:label "Ronggang Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-rongrong_ji rdfs:label "Rongrong Ji"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ross_girshick rdfs:label "Ross Girshick"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-runmin_cong rdfs:label "Runmin Cong"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-s%CE%B1 rdfs:label "Sα"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-s-measure rdfs:label "S-measure"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-s-measure_performance_boost rdfs:label "S-measure performance boost"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-s1 rdfs:label "S1"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-s_d rdfs:label "S d"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-s_dand_s_s rdfs:label "S dand S s"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-s_dchannel rdfs:label "S dchannel"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-s_s rdfs:label "S s"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-s_sand rdfs:label "S sand"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-s_sd rdfs:label "S sd"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-sabine_susstrunk rdfs:label "Sabine Susstrunk"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-saeed_anwar rdfs:label "Saeed Anwar"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-saliency-based_search_mechanism rdfs:label "saliency-based search mechanism"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-saliency-based_visual_attention rdfs:label "saliency-based visual attention"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_analysis rdfs:label "saliency analysis"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-saliency_annotations rdfs:label "saliency annotations"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_annotator rdfs:label "saliency annotator"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-saliency_consensus_process rdfs:label "saliency consensus process"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-saliency_detection_community rdfs:label "saliency detection community"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_detection_network rdfs:label "saliency detection network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-saliency_detection_problems rdfs:label "saliency detection problems"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_detection_task rdfs:label "saliency detection task"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_distribution rdfs:label "saliency distribution"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_evolution rdfs:label "saliency evolution"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_feature_map rdfs:label "saliency feature map"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_feature_map_s rdfs:label "saliency feature map S"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-saliency_feature_maps_sd_i rdfs:label "saliency feature maps Sd i"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-saliency_labeling_process rdfs:label "saliency labeling process"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-saliency_map_prediction_pi rdfs:label "saliency map prediction Pi"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-saliency_model rdfs:label "saliency model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-saliency_of_superpixel rdfs:label "saliency of superpixel"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-saliency_prediction_framework rdfs:label "saliency prediction framework"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-saliency_prediction_network rdfs:label "saliency prediction network"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-saliency_prediction_p rdfs:label "saliency prediction P"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-saliency_preservation rdfs:label "Saliency Preservation"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-salient_features rdfs:label "Salient Features"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-salient_object_hiding_technique rdfs:label "salient object hiding technique"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-sampling rdfs:label "Sampling"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-saurabh_gupta rdfs:label "Saurabh Gupta"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-scale_of_latent_space rdfs:label "scale of latent space"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-scale_of_the_gaussian_latent_space_k rdfs:label "scale of the Gaussian latent space K"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-scanpaths rdfs:label "scanpaths"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-scenario rdfs:label "scenario"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-scene_analysis rdfs:label "scene analysis"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-scene_understanding rdfs:label "Scene Understanding"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-schawkat rdfs:label "Schawkat"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-scribble_annotations rdfs:label "Scribble Annotations"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-se_df rdfs:label "SE DF"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-second_row_of_fig rdfs:label "second row of Fig"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-semantic_guided_depth_correction_network rdfs:label "semantic guided depth correction network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-semantic_segmentation rdfs:label "Semantic Segmentation"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-seminal_work rdfs:label "seminal work"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-separating_conspicuous_objects_from_the_background rdfs:label "separating conspicuous objects from the background"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-separation_of_salient_objects rdfs:label "separation of salient objects"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-shakir_mohamed rdfs:label "Shakir Mohamed"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shang-hua_gao rdfs:label "Shang-Hua Gao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shao-ping_lu rdfs:label "Shao-Ping Lu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shaodi_you rdfs:label "Shaodi You"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shaozi_li rdfs:label "Shaozi Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shape-guided_image_generation rdfs:label "shape-guided image generation"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-shechtman_eli rdfs:label "Shechtman Eli"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-sheila_hemami rdfs:label "Sheila Hemami"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shengfeng_he rdfs:label "Shengfeng He"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shihong_xia rdfs:label "Shihong Xia"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shivanthan_a rdfs:label "Shivanthan A."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-shuhui_wang rdfs:label "Shuhui Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-simon_kohl rdfs:label "Simon Kohl"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-single_gt rdfs:label "single GT"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-single_gt_saliency_map rdfs:label "single GT saliency map"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-single_gt_saliency_map_yi rdfs:label "single GT saliency map Yi"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-single_prediction rdfs:label "single prediction"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-single_salient_object rdfs:label "single salient object"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-six_challenging_benchmark_datasets rdfs:label "six challenging benchmark datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-six_rgbd_saliency_datasets rdfs:label "six RGBD saliency datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-six_standard_and_challenging_benchmark_datasets rdfs:label "six standard and challenging benchmark datasets"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-size rdfs:label "Size"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-smaller_is_better rdfs:label "smaller is better"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-smooth_1_loss rdfs:label "smooth `1 loss"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-smoothness_term rdfs:label "smoothness term"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-sota_rgb-d_models rdfs:label "SOTA RGB-D models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-standard rdfs:label "standard"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-standard_normal_distribution rdfs:label "standard normal distribution"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-static_images rdfs:label "Static Images"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-statistical_method rdfs:label "Statistical Method"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-statistical_methods rdfs:label "Statistical Methods"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-stephen_gould rdfs:label "Stephen Gould"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-stereo_cameras rdfs:label "stereo cameras"@en ;
    askg-onto:entityType "Technology"@en .

askg-data:Entity-stereopsis rdfs:label "stereopsis"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-stereoscopic_images rdfs:label "stereoscopic images"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-stereoscopic_saliency_model rdfs:label "Stereoscopic saliency model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-stochastic rdfs:label "stochastic"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-stochastic_backpropagation rdfs:label "Stochastic Backpropagation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-stochastic_characteristic rdfs:label "stochastic characteristic"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-stochastic_feature_s_s rdfs:label "stochastic feature S s"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-stochastic_features_s rdfs:label "stochastic features S"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-stochastic_features_s_s_i rdfs:label "stochastic features S s i"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-street_scenes rdfs:label "Street Scenes"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-strengths_and_weaknesses rdfs:label "strengths and weaknesses"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-structure-measure rdfs:label "Structure-measure"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-structure_measure rdfs:label "Structure measure"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-structured_saliency_detection_model rdfs:label "structured saliency detection model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-sunkavalli_kalyan rdfs:label "Sunkavalli Kalyan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-superpixelwise_variational_autoencoder rdfs:label "Superpixelwise Variational Autoencoder"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-supervae rdfs:label "SuperVAE"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-system rdfs:label "System"@en ;
    askg-onto:entityType "System"@en .

askg-data:Entity-table_1 rdfs:label "Table 1"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-tao_li rdfs:label "Tao Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-task rdfs:label "Task"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-ten_handcrafted_conventional_methods rdfs:label "ten handcrafted conventional methods"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-ten_leading_handcrafted_feature-based_models rdfs:label "ten leading handcrafted feature-based models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-testing rdfs:label "testing"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-the_closest_of_the_multiple_predictions rdfs:label "the closest of the multiple predictions"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-the_content_of_image rdfs:label "the content of image"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-the_superiority_of_learning_the_distribution_of_saliency_maps rdfs:label "the superiority of learning the distribution of saliency maps"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-theory rdfs:label "Theory"@en ;
    askg-onto:entityType "Theory"@en .

askg-data:Entity-thierry_baccino rdfs:label "Thierry Baccino"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-this_research rdfs:label "this research"@en ;
    askg-onto:entityType "Study"@en .

askg-data:Entity-three-stream_attention-aware_network rdfs:label "Three-stream Attention-aware Network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-three_11_convolutional_layers rdfs:label "Three 1×1 convolutional layers"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-three_channels rdfs:label "three channels"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-tissue_engineering rdfs:label "tissue engineering"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-training_batch_size rdfs:label "training batch size"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-training_image rdfs:label "training image"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-translate-to-recognize_networks rdfs:label "Translate-to-Recognize Networks"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-two-dimensional_gaussian_noise_map rdfs:label "two-dimensional Gaussian noise map"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-two_parts rdfs:label "two parts"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-u_v rdfs:label "(u, v)"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-uc-_net rdfs:label "UC- Net"@en ;
    askg-onto:entityType "Framework"@en .

askg-data:Entity-ucla rdfs:label "UCLA"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-ucnet rdfs:label "UCNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-ucsf rdfs:label "UCSF"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-uncertain_future_forecast rdfs:label "uncertain future forecast"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-uncertainty rdfs:label "uncertainty"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-uncertainty_in_medical_image_segmentation rdfs:label "Uncertainty in Medical Image Segmentation"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-uncertainty_network rdfs:label "uncertainty network"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-university_of_california rdfs:label "University of California"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-university_of_cambridge rdfs:label "University of Cambridge"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-unsupervised_monocular_depth_estimation rdfs:label "Unsupervised Monocular Depth Estimation"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-userspecific_saliency_detection rdfs:label "userspecific saliency detection"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-vae_algorithms rdfs:label "VAE algorithms"@en ;
    askg-onto:entityType "Algorithm"@en .

askg-data:Entity-vae_performance rdfs:label "VAE performance"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-variable rdfs:label "Variable"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-variational_autoencoder_vae rdfs:label "variational autoencoder (VAE)"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-variational_u-net rdfs:label "Variational U-Net"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-very_deep_convolutional_networks rdfs:label "Very Deep Convolutional Networks"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-vgg16 rdfs:label "VGG16"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-vgg16_parameters rdfs:label "VGG16 parameters"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-video_salient_object_detection rdfs:label "video salient object detection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-vijay_badrinarayanan rdfs:label "Vijay Badrinarayanan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-villegas_ruben rdfs:label "Villegas Ruben"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-vr rdfs:label "VR"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-vsod rdfs:label "VSOD"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-wang_et_al rdfs:label "Wang et al"@en ;
    askg-onto:entityType "Research Group"@en .

askg-data:Entity-wang_zhao rdfs:label "Wang Zhao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-weakly-supervised_object_and_action_localization rdfs:label "Weakly-supervised Object and Action Localization"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-weakly-supervised_salient_object_detection rdfs:label "Weakly-Supervised Salient Object Detection"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-wei_ji rdfs:label "Wei Ji"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-wei_ma rdfs:label "Wei Ma"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-wei_xu rdfs:label "Wei Xu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-weights rdfs:label "Weights"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-weihua_xiong rdfs:label "Weihua Xiong"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-weiming_hu rdfs:label "Weiming Hu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-wenhui_zhou rdfs:label "Wenhui Zhou"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-wenjing_geng rdfs:label "Wenjing Geng"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-wenmin_wang rdfs:label "Wenmin Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xi rdfs:label "Xi"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-xi_and_yi rdfs:label "Xi and Yi"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-xin_yu rdfs:label "Xin Yu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xinchen_yan rdfs:label "Xinchen Yan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xingxing_wei rdfs:label "Xingxing Wei"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xuan-yi_li rdfs:label "Xuan-Yi Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xuebin_qin rdfs:label "Xuebin Qin"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xuelong_li rdfs:label "Xuelong Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xueqing_li rdfs:label "Xueqing Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yan_xinchen rdfs:label "Yan Xinchen"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yandong_tang rdfs:label "Yandong Tang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yang_wang rdfs:label "Yang Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yi rdfs:label "Yi"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-yi_liu rdfs:label "Yi Liu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yi_yang rdfs:label "Yi Yang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yohanandan_adrian_g rdfs:label "Yohanandan, Adrian G."@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yong_jae_lee rdfs:label "Yong Jae Lee"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yongri_piao rdfs:label "Yongri Piao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yu-kun_lai rdfs:label "Yu-Kun Lai"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yu_ji rdfs:label "Yu Ji"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yuanhua_qiao rdfs:label "Yuanhua Qiao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yujie_geng rdfs:label "Yujie Geng"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yumer_ersin rdfs:label "Yumer Ersin"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yun_liu rdfs:label "Yun Liu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yupeng_cheng rdfs:label "Yupeng Cheng"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yuqi_guo rdfs:label "Yuqi Guo"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yuzhen_niu rdfs:label "Yuzhen Niu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-z_k rdfs:label "z k"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-zhao rdfs:label "Zhao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zhengxing_sun rdfs:label "Zhengxing Sun"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zhenheng_yang rdfs:label "Zhenheng Yang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zhi_cai rdfs:label "Zhi Cai"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zhi_liu rdfs:label "Zhi Liu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zhiming_luo rdfs:label "Zhiming Luo"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zhiwei_li rdfs:label "Zhiwei Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zichen_zhang rdfs:label "Zichen Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zou rdfs:label "Zou"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Paper-fdb482b8f2df5642-Section-1 a askg-onto:Section ;
    rdfs:label "Section 1"@en ;
    domo:Text "Uc-Net: Uncertainty Inspired Rgb-D Saliency Detection Via Conditional Variational Autoencoders"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-1-Paragraph-11 ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:level "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-1-Paragraph-11 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "Jing Zhang1,4,5 Deng-Ping Fan2,6,∗ Yuchao Dai3Saeed Anwar1,5 Fatemeh Sadat Saleh1,4 Tong Zhang1 Nick Barnes1 1 Australian National University 2 CS, Nankai University 3 Northwestern Polytechnical University 4 ACRV 5 Data61 6Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-1-Paragraph-11-Sentence-111 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-1-Paragraph-11-Sentence-111 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Jing Zhang1,4,5 Deng-Ping Fan2,6,∗ Yuchao Dai3Saeed Anwar1,5 Fatemeh Sadat Saleh1,4 Tong Zhang1 Nick Barnes1 1 Australian National University 2 CS, Nankai University 3 Northwestern Polytechnical University 4 ACRV 5 Data61 6Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE"@en ;
    askg-onto:inSentence "Jing Zhang1,4,5 Deng-Ping Fan2,6,∗ Yuchao Dai3Saeed Anwar1,5 Fatemeh Sadat Saleh1,4 Tong Zhang1 Nick Barnes1 1 Australian National University 2 CS, Nankai University 3 Northwestern Polytechnical University 4 ACRV 5 Data61 6Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-acrv,
        askg-data:Entity-australian_national_university,
        askg-data:Entity-cs_nankai_university,
        askg-data:Entity-data61,
        askg-data:Entity-deng-ping_fan,
        askg-data:Entity-fatemeh_sadat_saleh,
        askg-data:Entity-inception_institute_of_artificial_intelligence_iiai,
        askg-data:Entity-institution,
        askg-data:Entity-jing_zhang,
        askg-data:Entity-nick_barnes,
        askg-data:Entity-northwestern_polytechnical_university,
        askg-data:Entity-organization,
        askg-data:Entity-person,
        askg-data:Entity-saeed_anwar,
        askg-data:Entity-tong_zhang,
        askg-data:Entity-yuchao_dai .

askg-data:Paper-fdb482b8f2df5642-Section-10 a askg-onto:Section ;
    rdfs:label "Section 10"@en ;
    domo:Text "3.4. Objective Function"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-102,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-103,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-105,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-106,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-107,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108 ;
    askg-onto:index "10"^^xsd:int ;
    askg-onto:level "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "At this stage, our loss function is composed of two parts i.e. LCVAE and LDepth. Furthermore, we propose to use the smoothness loss [9] as a regularizer to achieve edgeaware saliency detection, based on the assumption of interclass distinction and intra-class similarity. Following [56], we define first-order derivatives of the saliency map in the smoothness term as"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101-Sentence-1011,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101-Sentence-1012,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101-Sentence-1013,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101-Sentence-1014 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101-Sentence-1011 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "At this stage, our loss function is composed of two parts i.e."@en ;
    askg-onto:inSentence "At this stage, our loss function is composed of two parts i.e."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-loss_function,
        askg-data:Entity-two_parts .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101-Sentence-1012 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "LCVAE and LDepth."@en ;
    askg-onto:inSentence "LCVAE and LDepth."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-lcvae,
        askg-data:Entity-ldepth,
        askg-data:Entity-model .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101-Sentence-1013 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Furthermore, we propose to use the smoothness loss [9] as a regularizer to achieve edgeaware saliency detection, based on the assumption of interclass distinction and intra-class similarity."@en ;
    askg-onto:inSentence "Furthermore, we propose to use the smoothness loss [9] as a regularizer to achieve edgeaware saliency detection, based on the assumption of interclass distinction and intra-class similarity."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-edge-aware_saliency_detection,
        askg-data:Entity-interclass_distinction_and_intra-class_similarity,
        askg-data:Entity-regularizer,
        askg-data:Entity-smoothness_loss .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-101-Sentence-1014 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Following [56], we define first-order derivatives of the saliency map in the smoothness term as"@en ;
    askg-onto:inSentence "Following [56], we define first-order derivatives of the saliency map in the smoothness term as"^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-first-order_derivatives_of_the_saliency_map,
        askg-data:Entity-smoothness_term .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-102 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "$$\\mathcal{L}_{\\text{Smooth}}=\\sum_{u,v}\\sum_{d\\in\\overrightarrow{x},\\overrightarrow{y}}\\Psi(|\\partial_{d}P_{u,v}|e^{-\\alpha|\\partial_{d}I_{g}(u,v)|}),\\tag{5}$$"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-102-Sentence-1021 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-102-Sentence-1021 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "$$\\mathcal{L}_{\\text{Smooth}}=\\sum_{u,v}\\sum_{d\\in\\overrightarrow{x},\\overrightarrow{y}}\\Psi(|\\partial_{d}P_{u,v}|e^{-\\alpha|\\partial_{d}I_{g}(u,v)|}),\\tag{5}$$"@en ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-103 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 3"@en ;
    domo:Text "where Ψ is defined as Ψ(s) = √s 2 + 1e−6, Pu,v is the predicted saliency map at position (u, v), and Ig(*u, v*) is the image intensity, d indexes over partial derivative on −→x and −→y directions. We set α = 10 following [56]."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-103-Sentence-1031,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-103-Sentence-1032 ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-103-Sentence-1031 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "where Ψ is defined as Ψ(s) = √s 2 + 1e−6, Pu,v is the predicted saliency map at position (u, v), and Ig(*u, v*) is the image intensity, d indexes over partial derivative on −→x and −→y directions."@en ;
    askg-onto:inSentence "where Ψ is defined as Ψ(s) = √s 2 + 1e−6, Pu,v is the predicted saliency map at position (u, v), and Ig(*u, v*) is the image intensity, d indexes over partial derivative on −→x and −→y directions."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-,
        askg-data:Entity-%CF%88,
        askg-data:Entity-%CF%88s__s_2__1e6,
        askg-data:Entity-igu_v,
        askg-data:Entity-on_x_and_y_directions,
        askg-data:Entity-puv,
        askg-data:Entity-u_v .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-103-Sentence-1032 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "We set α = 10 following [56]."@en ;
    askg-onto:inSentence "We set α = 10 following [56]."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%CE%B1,
        askg-data:Entity-10 .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 4"@en ;
    domo:Text "Both the smoothness loss (Eq. (5)) and the boundary IOU loss (Eq. (3)) need intensity Ig. We convert the RGB image I to a gray-scale intensity image Ig as [60]:"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104-Sentence-1041,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104-Sentence-1042,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104-Sentence-1043,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104-Sentence-1044 ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104-Sentence-1041 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Both the smoothness loss (Eq."@en ;
    askg-onto:inSentence "Both the smoothness loss (Eq."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-metric,
        askg-data:Entity-smoothness_loss .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104-Sentence-1042 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "(5)) and the boundary IOU loss (Eq."@en ;
    askg-onto:inSentence "(5)) and the boundary IOU loss (Eq."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-boundary_iou_loss,
        askg-data:Entity-metric .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104-Sentence-1043 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "(3)) need intensity Ig."@en ;
    askg-onto:inSentence "(3)) need intensity Ig."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ig,
        askg-data:Entity-need_intensity .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-104-Sentence-1044 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "We convert the RGB image I to a gray-scale intensity image Ig as [60]:"@en ;
    askg-onto:inSentence "We convert the RGB image I to a gray-scale intensity image Ig as [60]:"^^xsd:string ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-105 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 5"@en ;
    domo:Text "$$Ig=0.2126\\times I^{lr}+0.7152\\times I^{lg}+0.0722\\times I^{lb},\\tag{6}$$"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-105-Sentence-1051 ;
    askg-onto:index "5"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-105-Sentence-1051 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "$$Ig=0.2126\\times I^{lr}+0.7152\\times I^{lg}+0.0722\\times I^{lb},\\tag{6}$$"@en ;
    askg-onto:inSentence "$$Ig=0.2126\\times I^{lr}+0.7152\\times I^{lg}+0.0722\\times I^{lb},\\tag{6}$$"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ig .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-106 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 6"@en ;
    domo:Text "where I lr, I lg and I lb represent the color components in the linear color space after Gamma function been removed from the original color space. I lr is achieved via:"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-106-Sentence-1061,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-106-Sentence-1062 ;
    askg-onto:index "6"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-106-Sentence-1061 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "where I lr, I lg and I lb represent the color components in the linear color space after Gamma function been removed from the original color space."@en ;
    askg-onto:inSentence "where I lr, I lg and I lb represent the color components in the linear color space after Gamma function been removed from the original color space."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-color_components_in_the_linear_color_space_after_gamma_function_been_removed_from_the_original_color_space,
        askg-data:Entity-i_lr_i_lg_and_i_lb .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-106-Sentence-1062 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "I lr is achieved via:"@en ;
    askg-onto:inSentence "I lr is achieved via:"^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-i_lr .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-107 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 7"@en ;
    domo:Text "$$I^{lr}=\\left\\{\\begin{array}{cc}\\frac{I^{r}}{12.92},&I^{r}\\leq0.04045,\\\\ \\left(\\frac{I^{r}+0.055}{1.055}\\right)^{2.4},&I^{r}>0.04045.\\end{array}\\right.\\tag{7}$$ $I^{r}$ is the original and channel of image $I$ and we where I ris the original red channel of image I, and we compute I gand I bin the same way as Eq. (7)."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-107-Sentence-1071,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-107-Sentence-1072 ;
    askg-onto:index "7"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-107-Sentence-1071 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "$$I^{lr}=\\left\\{\\begin{array}{cc}\\frac{I^{r}}{12.92},&I^{r}\\leq0.04045,\\\\ \\left(\\frac{I^{r}+0.055}{1.055}\\right)^{2.4},&I^{r}>0.04045.\\end{array}\\right.\\tag{7}$$ $I^{r}$ is the original and channel of image $I$ and we where I ris the original red channel of image I, and we compute I gand I bin the same way as Eq."@en ;
    askg-onto:inSentence "$$I^{lr}=\\left\\{\\begin{array}{cc}\\frac{I^{r}}{12.92},&I^{r}\\leq0.04045,\\\\ \\left(\\frac{I^{r}+0.055}{1.055}\\right)^{2.4},&I^{r}>0.04045.\\end{array}\\right.\\tag{7}$$ $I^{r}$ is the original and channel of image $I$ and we where I ris the original red channel of image I, and we compute I gand I bin the same way as Eq."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-a_function_based_on_ir,
        askg-data:Entity-i,
        askg-data:Entity-ilr,
        askg-data:Entity-ir,
        askg-data:Entity-original_and_channel_of_image,
        askg-data:Entity-original_red_channel_of_image_i .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-107-Sentence-1072 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "(7)."@en ;
    askg-onto:inSentence "(7)."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-organization,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 8"@en ;
    domo:Text "4As the GT map Y ∈ {0, 1}, we produce series of binary predictions with each one representing annotation from one saliency annotator. With smoothness loss LSmooth, depth loss LDepth and CVAE loss LCVAE, our final loss function is defined as: Lsal = LCVAE + λ1LDepth + λ2LSmooth. (8) In our experiments, we set λ1 = λ2 = 0.3. Training details: We set channel size of S das M = 32, and scale of latent space as K = 8. We trained our model using Pytorch, and initialized the encoder of SaliencyNet and DepthCorrectionNet with VGG16 parameters pre-trained on ImageNet. Weights of new layers were initialized with N (0, 0.01), and bias was set as constant. We used the Adam method with momentum 0.9 and decreased the learning rate 10% after each epoch. The base learning rate was initialized as 1e-4. The whole training took 13 hours with training batch size 6 and maximum epoch 30 on a PC with an NVIDIA GeForce RTX GPU. For input image size 352 × 352, the inference time is 0.06s on average."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1081,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-10810,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1082,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1083,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1084,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1085,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1086,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1087,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1088,
        askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1089 ;
    askg-onto:index "8"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1081 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "4As the GT map Y ∈ {0, 1}, we produce series of binary predictions with each one representing annotation from one saliency annotator."@en ;
    askg-onto:inSentence "4As the GT map Y ∈ {0, 1}, we produce series of binary predictions with each one representing annotation from one saliency annotator."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-binary_predictions,
        askg-data:Entity-gt_map_y,
        askg-data:Entity-saliency_annotator .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-10810 a askg-onto:Sentence ;
    rdfs:label "Sentence 10"@en ;
    domo:Text "For input image size 352 × 352, the inference time is 0.06s on average."@en ;
    askg-onto:inSentence "For input image size 352 × 352, the inference time is 0.06s on average."^^xsd:string ;
    askg-onto:index "10"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-006s,
        askg-data:Entity-352__352,
        askg-data:Entity-inference_time,
        askg-data:Entity-input_image_size .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1082 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "With smoothness loss LSmooth, depth loss LDepth and CVAE loss LCVAE, our final loss function is defined as: Lsal = LCVAE + λ1LDepth + λ2LSmooth."@en ;
    askg-onto:inSentence "With smoothness loss LSmooth, depth loss LDepth and CVAE loss LCVAE, our final loss function is defined as: Lsal = LCVAE + λ1LDepth + λ2LSmooth."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-lcvae__%CE%BB1ldepth__%CE%BB2lsmooth,
        askg-data:Entity-lsal .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1083 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "(8) In our experiments, we set λ1 = λ2 = 0.3."@en ;
    askg-onto:inSentence "(8) In our experiments, we set λ1 = λ2 = 0.3."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%CE%BB1__%CE%BB2__03,
        askg-data:Entity-experiments .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1084 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Training details: We set channel size of S das M = 32, and scale of latent space as K = 8."@en ;
    askg-onto:inSentence "Training details: We set channel size of S das M = 32, and scale of latent space as K = 8."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-32,
        askg-data:Entity-8,
        askg-data:Entity-channel_size_of_s_das_m,
        askg-data:Entity-scale_of_latent_space .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1085 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "We trained our model using Pytorch, and initialized the encoder of SaliencyNet and DepthCorrectionNet with VGG16 parameters pre-trained on ImageNet."@en ;
    askg-onto:inSentence "We trained our model using Pytorch, and initialized the encoder of SaliencyNet and DepthCorrectionNet with VGG16 parameters pre-trained on ImageNet."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-encoder_of_saliencynet,
        askg-data:Entity-imagenet,
        askg-data:Entity-model,
        askg-data:Entity-pytorch,
        askg-data:Entity-vgg16_parameters .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1086 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "Weights of new layers were initialized with N (0, 0.01), and bias was set as constant."@en ;
    askg-onto:inSentence "Weights of new layers were initialized with N (0, 0.01), and bias was set as constant."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-bias,
        askg-data:Entity-n_0_001,
        askg-data:Entity-weights .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1087 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "We used the Adam method with momentum 0.9 and decreased the learning rate 10% after each epoch."@en ;
    askg-onto:inSentence "We used the Adam method with momentum 0.9 and decreased the learning rate 10% after each epoch."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-10,
        askg-data:Entity-adam_method,
        askg-data:Entity-epoch,
        askg-data:Entity-learning_rate,
        askg-data:Entity-momentum_09 .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1088 a askg-onto:Sentence ;
    rdfs:label "Sentence 8"@en ;
    domo:Text "The base learning rate was initialized as 1e-4."@en ;
    askg-onto:inSentence "The base learning rate was initialized as 1e-4."^^xsd:string ;
    askg-onto:index "8"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-1e-4,
        askg-data:Entity-base_learning_rate .

askg-data:Paper-fdb482b8f2df5642-Section-10-Paragraph-108-Sentence-1089 a askg-onto:Sentence ;
    rdfs:label "Sentence 9"@en ;
    domo:Text "The whole training took 13 hours with training batch size 6 and maximum epoch 30 on a PC with an NVIDIA GeForce RTX GPU."@en ;
    askg-onto:inSentence "The whole training took 13 hours with training batch size 6 and maximum epoch 30 on a PC with an NVIDIA GeForce RTX GPU."^^xsd:string ;
    askg-onto:index "9"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-13_hours,
        askg-data:Entity-30,
        askg-data:Entity-6,
        askg-data:Entity-maximum_epoch,
        askg-data:Entity-nvidia_geforce_rtx_gpu,
        askg-data:Entity-pc,
        askg-data:Entity-training,
        askg-data:Entity-training_batch_size .

askg-data:Paper-fdb482b8f2df5642-Section-11 a askg-onto:Section ;
    rdfs:label "Section 11"@en ;
    domo:Text "4. Experimental Results"@en ;
    askg-onto:index "11"^^xsd:int ;
    askg-onto:level "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-12 a askg-onto:Section ;
    rdfs:label "Section 12"@en ;
    domo:Text "4.1. Setup"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-12-Paragraph-121 ;
    askg-onto:index "12"^^xsd:int ;
    askg-onto:level "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-12-Paragraph-121 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "Datasets: We perform experiments on six datasets including five widely used RGB-D saliency detection datasets (namely NJU2K [28], NLPR [41], SSB [40], LFSD [35], DES [8]) and one newly released dataset (SIP [18]). Competing Methods: We compare our method with 18 algorithms, including ten handcrafted conventional methods and eight deep RGB-D saliency detection models. Evaluation Metrics: Four evaluation metrics are used, including two widely used: 1) Mean Absolute Error (MAE M); 2) mean F-measure (Fβ) and two recently proposed: 3) Enhanced alignment measure (mean E-measure, Eξ) [15] and 4) Structure measure (S-measure, Sα) [14]."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-12-Paragraph-121-Sentence-1211,
        askg-data:Paper-fdb482b8f2df5642-Section-12-Paragraph-121-Sentence-1212,
        askg-data:Paper-fdb482b8f2df5642-Section-12-Paragraph-121-Sentence-1213 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-12-Paragraph-121-Sentence-1211 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Datasets: We perform experiments on six datasets including five widely used RGB-D saliency detection datasets (namely NJU2K [28], NLPR [41], SSB [40], LFSD [35], DES [8]) and one newly released dataset (SIP [18])."@en ;
    askg-onto:inSentence "Datasets: We perform experiments on six datasets including five widely used RGB-D saliency detection datasets (namely NJU2K [28], NLPR [41], SSB [40], LFSD [35], DES [8]) and one newly released dataset (SIP [18])."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dataset,
        askg-data:Entity-des,
        askg-data:Entity-lfsd,
        askg-data:Entity-nju2k,
        askg-data:Entity-nlpr,
        askg-data:Entity-rgb-d_saliency_detection_datasets,
        askg-data:Entity-sip,
        askg-data:Entity-ssb .

askg-data:Paper-fdb482b8f2df5642-Section-12-Paragraph-121-Sentence-1212 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Competing Methods: We compare our method with 18 algorithms, including ten handcrafted conventional methods and eight deep RGB-D saliency detection models."@en ;
    askg-onto:inSentence "Competing Methods: We compare our method with 18 algorithms, including ten handcrafted conventional methods and eight deep RGB-D saliency detection models."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-18_algorithms,
        askg-data:Entity-eight_deep_rgb-d_saliency_detection_models,
        askg-data:Entity-our_method,
        askg-data:Entity-ten_handcrafted_conventional_methods .

askg-data:Paper-fdb482b8f2df5642-Section-12-Paragraph-121-Sentence-1213 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Evaluation Metrics: Four evaluation metrics are used, including two widely used: 1) Mean Absolute Error (MAE M); 2) mean F-measure (Fβ) and two recently proposed: 3) Enhanced alignment measure (mean E-measure, Eξ) [15] and 4) Structure measure (S-measure, Sα) [14]."@en ;
    askg-onto:inSentence "Evaluation Metrics: Four evaluation metrics are used, including two widely used: 1) Mean Absolute Error (MAE M); 2) mean F-measure (Fβ) and two recently proposed: 3) Enhanced alignment measure (mean E-measure, Eξ) [15] and 4) Structure measure (S-measure, Sα) [14]."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-enhanced_alignment_measure,
        askg-data:Entity-mean_absolute_error,
        askg-data:Entity-mean_f-measure,
        askg-data:Entity-metric,
        askg-data:Entity-structure_measure .

askg-data:Paper-fdb482b8f2df5642-Section-13 a askg-onto:Section ;
    rdfs:label "Section 13"@en ;
    domo:Text "4.2. Performance Comparison"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-132 ;
    askg-onto:index "13"^^xsd:int ;
    askg-onto:level "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "Quantitative Comparison: We report performance of our method and competing methods in Table 1. It shows that our method consistently achieves the best performance on all datasets, especially on SSB [40] and SIP [18], our method achieves significant S-measure, E-measure, and F-measure performance boost and a decrease in MAE by a large margin. We show E-measure and F-measure curves of competing methods and ours in Fig. 7. We observe that our method produces not only stable E-measure and F-measure but also best performance. Qualitative Comparisons: In Fig. 8, we show five images comparing results of our method with one newly released RGB-D saliency detection method (DMRA [61]), and two widely used methods to produce structured outputs, namely M-head [46] and MC-dropout [30] (we will discuss these two methods in detail in the ablation study section)."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1311,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1312,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1313,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1314,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1315,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1316,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1317 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1311 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Quantitative Comparison: We report performance of our method and competing methods in Table 1."@en ;
    askg-onto:inSentence "Quantitative Comparison: We report performance of our method and competing methods in Table 1."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-competing_methods,
        askg-data:Entity-our_method .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1312 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "It shows that our method consistently achieves the best performance on all datasets, especially on SSB [40] and SIP [18], our method achieves significant S-measure, E-measure, and F-measure performance boost and a decrease in MAE by a large margin."@en ;
    askg-onto:inSentence "It shows that our method consistently achieves the best performance on all datasets, especially on SSB [40] and SIP [18], our method achieves significant S-measure, E-measure, and F-measure performance boost and a decrease in MAE by a large margin."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-best_performance,
        askg-data:Entity-dataset,
        askg-data:Entity-e-measure_performance_boost,
        askg-data:Entity-f-measure_performance_boost,
        askg-data:Entity-mae,
        askg-data:Entity-our_method,
        askg-data:Entity-s-measure_performance_boost,
        askg-data:Entity-sip,
        askg-data:Entity-ssb .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1313 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "We show E-measure and F-measure curves of competing methods and ours in Fig."@en ;
    askg-onto:inSentence "We show E-measure and F-measure curves of competing methods and ours in Fig."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-competing_methods,
        askg-data:Entity-e-measure,
        askg-data:Entity-e-measure_and_f-measure_curves,
        askg-data:Entity-f-measure_curves,
        askg-data:Entity-fig,
        askg-data:Entity-ours .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1314 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "7."@en ;
    askg-onto:inSentence "7."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-experiment,
        askg-data:Entity-method,
        askg-data:Entity-research_group,
        askg-data:Entity-study,
        askg-data:Entity-tool,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1315 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "We observe that our method produces not only stable E-measure and F-measure but also best performance."@en ;
    askg-onto:inSentence "We observe that our method produces not only stable E-measure and F-measure but also best performance."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-best_performance,
        askg-data:Entity-e-measure,
        askg-data:Entity-f-measure,
        askg-data:Entity-method .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1316 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "Qualitative Comparisons: In Fig."@en ;
    askg-onto:inSentence "Qualitative Comparisons: In Fig."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-method,
        askg-data:Entity-qualitative_comparisons .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-131-Sentence-1317 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "8, we show five images comparing results of our method with one newly released RGB-D saliency detection method (DMRA [61]), and two widely used methods to produce structured outputs, namely M-head [46] and MC-dropout [30] (we will discuss these two methods in detail in the ablation study section)."@en ;
    askg-onto:inSentence "8, we show five images comparing results of our method with one newly released RGB-D saliency detection method (DMRA [61]), and two widely used methods to produce structured outputs, namely M-head [46] and MC-dropout [30] (we will discuss these two methods in detail in the ablation study section)."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dmra,
        askg-data:Entity-m-head,
        askg-data:Entity-mc-dropout,
        askg-data:Entity-rgb-d_saliency_detection_method .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-132 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "We design both M-head and MC-dropout based structured saliency detection models by replacing CVAE with M-head and MC-dropout respectively. Results in Fig. 8 show that our method can not only produce high accuracy predictions (compared with DMRA [61]), but also diverse predictions (compared with M-head based and MC-dropout based models) for images with complex background (image in the first and last rows)."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-132-Sentence-1321,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-132-Sentence-1322,
        askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-132-Sentence-1323 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-132-Sentence-1321 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "We design both M-head and MC-dropout based structured saliency detection models by replacing CVAE with M-head and MC-dropout respectively."@en ;
    askg-onto:inSentence "We design both M-head and MC-dropout based structured saliency detection models by replacing CVAE with M-head and MC-dropout respectively."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-m-head,
        askg-data:Entity-mc-dropout,
        askg-data:Entity-structured_saliency_detection_model .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-132-Sentence-1322 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Results in Fig."@en ;
    askg-onto:inSentence "Results in Fig."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-fig,
        askg-data:Entity-results .

askg-data:Paper-fdb482b8f2df5642-Section-13-Paragraph-132-Sentence-1323 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "8 show that our method can not only produce high accuracy predictions (compared with DMRA [61]), but also diverse predictions (compared with M-head based and MC-dropout based models) for images with complex background (image in the first and last rows)."@en ;
    askg-onto:inSentence "8 show that our method can not only produce high accuracy predictions (compared with DMRA [61]), but also diverse predictions (compared with M-head based and MC-dropout based models) for images with complex background (image in the first and last rows)."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-diverse_predictions,
        askg-data:Entity-dmra,
        askg-data:Entity-high_accuracy_predictions,
        askg-data:Entity-images_with_complex_background,
        askg-data:Entity-m-head_based_models,
        askg-data:Entity-mc-dropout_based_models,
        askg-data:Entity-method .

askg-data:Paper-fdb482b8f2df5642-Section-14 a askg-onto:Section ;
    rdfs:label "Section 14"@en ;
    domo:Text "4.3. Ablation Study"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1411,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-142,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-143,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-145,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-146,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-148,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-149 ;
    askg-onto:index "14"^^xsd:int ;
    askg-onto:level "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "We carried out eight experiments (shown in Table 2) to thoroughly analyse our framework, including network structure (\"M1\", \"M2\", \"M3\"), probabilistic model selection (\"M4\", \"M5\", \"M6\"), data source selection (\"M7\") and effectiveness of the new label generation technique (\"M8\"). We make the number bold when it's better than ours. Scale of Latent Space: We investigate the influence of the scale of the Gaussian latent space K in our network. In this paper, after parameter tuning, we find K = 8 works best. We show performance with K = 32 as \"M1\". Performance of \"M1\" is worse than our reported results, which indicates that scale of the latent space is an important parameter in our framework. We further carried out more experiments with K ∈ [2, 12], and found relative stable predictions with K ∈ [6, 10]. Effect of DepthCorrectionNet: To illustrate the effectiveness of the proposed DepthCorrectionNet, we remove this branch and feed the concatenation of the RGB image and depth data to the SaliencyNet, shown as \"M2\", which is worse than our method. On DES [8] dataset, we observe the proposed solution achieves around 4% improvement on S-measure, E-measure and F-measure, which demonstrates the effectiveness of the depth correction net. Saliency Consencus Module: To mimic the saliency labeling process, we embed a saliency consensus module during test in our framework (as shown in Fig. 3) to obtain the majority voting of the multiple predictions. We remove it from our framework and test the network performance by random sample from the latent PriorNet Pθ(z|X), and performance is shown in \"M3\", which is the best compared with competing methods. While, with the saliency consensus module embedded, we achieve even better performance, which illustrates effectiveness of the saliency consencus module. VAE vs. **CVAE:** We use CVAE to model labeling variants, and a PosteriorNet is used to estimate parameters for the PriorNet. To test how our model performs with prior of z as a standard normal distribution, and the posterior of z as Pθ(z|X). VAE performance is shown as \"M4\", which is comparable with SOTA RGB-D models. With the CVAE [50] based model proposed, we further boost performance of \"M4\", which proves effectiveness of the our solution."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1411,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14110,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14111,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14112,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14113,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14114,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14115,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14116,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14117,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14118,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1412,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1413,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1414,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1415,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1416,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1417,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1418,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1419 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1411 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "We carried out eight experiments (shown in Table 2) to thoroughly analyse our framework, including network structure (\"M1\", \"M2\", \"M3\"), probabilistic model selection (\"M4\", \"M5\", \"M6\"), data source selection (\"M7\") and effectiveness of the new label generation technique (\"M8\")."@en ;
    askg-onto:inSentence "We carried out eight experiments (shown in Table 2) to thoroughly analyse our framework, including network structure (\"M1\", \"M2\", \"M3\"), probabilistic model selection (\"M4\", \"M5\", \"M6\"), data source selection (\"M7\") and effectiveness of the new label generation technique (\"M8\")."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-data_source_selection,
        askg-data:Entity-experiments,
        askg-data:Entity-framework,
        askg-data:Entity-m1,
        askg-data:Entity-m2,
        askg-data:Entity-m3,
        askg-data:Entity-m4,
        askg-data:Entity-m5,
        askg-data:Entity-m6,
        askg-data:Entity-m7,
        askg-data:Entity-m8,
        askg-data:Entity-network_structure,
        askg-data:Entity-new_label_generation_technique,
        askg-data:Entity-probabilistic_model_selection .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14110 a askg-onto:Sentence ;
    rdfs:label "Sentence 10"@en ;
    domo:Text "Saliency Consencus Module: To mimic the saliency labeling process, we embed a saliency consensus module during test in our framework (as shown in Fig."@en ;
    askg-onto:inSentence "Saliency Consencus Module: To mimic the saliency labeling process, we embed a saliency consensus module during test in our framework (as shown in Fig."^^xsd:string ;
    askg-onto:index "10"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-framework,
        askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-saliency_consensus_module,
        askg-data:Entity-saliency_labeling_process,
        askg-data:Entity-system .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14111 a askg-onto:Sentence ;
    rdfs:label "Sentence 11"@en ;
    domo:Text "3) to obtain the majority voting of the multiple predictions."@en ;
    askg-onto:inSentence "3) to obtain the majority voting of the multiple predictions."^^xsd:string ;
    askg-onto:index "11"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-majority_voting,
        askg-data:Entity-multiple_predictions .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14112 a askg-onto:Sentence ;
    rdfs:label "Sentence 12"@en ;
    domo:Text "We remove it from our framework and test the network performance by random sample from the latent PriorNet Pθ(z|X), and performance is shown in \"M3\", which is the best compared with competing methods."@en ;
    askg-onto:inSentence "We remove it from our framework and test the network performance by random sample from the latent PriorNet Pθ(z|X), and performance is shown in \"M3\", which is the best compared with competing methods."^^xsd:string ;
    askg-onto:index "12"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-competing_methods,
        askg-data:Entity-m3,
        askg-data:Entity-metric,
        askg-data:Entity-model,
        askg-data:Entity-network_performance,
        askg-data:Entity-priornet,
        askg-data:Entity-random_sample .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14113 a askg-onto:Sentence ;
    rdfs:label "Sentence 13"@en ;
    domo:Text "While, with the saliency consensus module embedded, we achieve even better performance, which illustrates effectiveness of the saliency consencus module."@en ;
    askg-onto:inSentence "While, with the saliency consensus module embedded, we achieve even better performance, which illustrates effectiveness of the saliency consencus module."^^xsd:string ;
    askg-onto:index "13"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-performance,
        askg-data:Entity-saliency_consensus_module .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14114 a askg-onto:Sentence ;
    rdfs:label "Sentence 14"@en ;
    domo:Text "VAE vs."@en ;
    askg-onto:inSentence "VAE vs."^^xsd:string ;
    askg-onto:index "14"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-vae .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14115 a askg-onto:Sentence ;
    rdfs:label "Sentence 15"@en ;
    domo:Text "**CVAE:** We use CVAE to model labeling variants, and a PosteriorNet is used to estimate parameters for the PriorNet."@en ;
    askg-onto:inSentence "**CVAE:** We use CVAE to model labeling variants, and a PosteriorNet is used to estimate parameters for the PriorNet."^^xsd:string ;
    askg-onto:index "15"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-labeling_variants,
        askg-data:Entity-posteriornet,
        askg-data:Entity-priornet .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14116 a askg-onto:Sentence ;
    rdfs:label "Sentence 16"@en ;
    domo:Text "To test how our model performs with prior of z as a standard normal distribution, and the posterior of z as Pθ(z|X)."@en ;
    askg-onto:inSentence "To test how our model performs with prior of z as a standard normal distribution, and the posterior of z as Pθ(z|X)."^^xsd:string ;
    askg-onto:index "16"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-model,
        askg-data:Entity-p%CE%B8zx,
        askg-data:Entity-posterior_of_z,
        askg-data:Entity-standard_normal_distribution .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14117 a askg-onto:Sentence ;
    rdfs:label "Sentence 17"@en ;
    domo:Text "VAE performance is shown as \"M4\", which is comparable with SOTA RGB-D models."@en ;
    askg-onto:inSentence "VAE performance is shown as \"M4\", which is comparable with SOTA RGB-D models."^^xsd:string ;
    askg-onto:index "17"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-m4,
        askg-data:Entity-sota_rgb-d_models,
        askg-data:Entity-vae_performance .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-14118 a askg-onto:Sentence ;
    rdfs:label "Sentence 18"@en ;
    domo:Text "With the CVAE [50] based model proposed, we further boost performance of \"M4\", which proves effectiveness of the our solution."@en ;
    askg-onto:inSentence "With the CVAE [50] based model proposed, we further boost performance of \"M4\", which proves effectiveness of the our solution."^^xsd:string ;
    askg-onto:index "18"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-m4,
        askg-data:Entity-our_solution .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1412 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "We make the number bold when it's better than ours."@en ;
    askg-onto:inSentence "We make the number bold when it's better than ours."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-number,
        askg-data:Entity-ours .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1413 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Scale of Latent Space: We investigate the influence of the scale of the Gaussian latent space K in our network."@en ;
    askg-onto:inSentence "Scale of Latent Space: We investigate the influence of the scale of the Gaussian latent space K in our network."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gaussian_latent_space,
        askg-data:Entity-scale_of_the_gaussian_latent_space_k .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1414 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "In this paper, after parameter tuning, we find K = 8 works best."@en ;
    askg-onto:inSentence "In this paper, after parameter tuning, we find K = 8 works best."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-8,
        askg-data:Entity-k .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1415 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "We show performance with K = 32 as \"M1\"."@en ;
    askg-onto:inSentence "We show performance with K = 32 as \"M1\"."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-k__32,
        askg-data:Entity-m1 .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1416 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "Performance of \"M1\" is worse than our reported results, which indicates that scale of the latent space is an important parameter in our framework."@en ;
    askg-onto:inSentence "Performance of \"M1\" is worse than our reported results, which indicates that scale of the latent space is an important parameter in our framework."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-latent_space,
        askg-data:Entity-m1,
        askg-data:Entity-our_framework,
        askg-data:Entity-our_reported_results .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1417 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "We further carried out more experiments with K ∈ [2, 12], and found relative stable predictions with K ∈ [6, 10]."@en ;
    askg-onto:inSentence "We further carried out more experiments with K ∈ [2, 12], and found relative stable predictions with K ∈ [6, 10]."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2_12,
        askg-data:Entity-6_10,
        askg-data:Entity-k .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1418 a askg-onto:Sentence ;
    rdfs:label "Sentence 8"@en ;
    domo:Text "Effect of DepthCorrectionNet: To illustrate the effectiveness of the proposed DepthCorrectionNet, we remove this branch and feed the concatenation of the RGB image and depth data to the SaliencyNet, shown as \"M2\", which is worse than our method."@en ;
    askg-onto:inSentence "Effect of DepthCorrectionNet: To illustrate the effectiveness of the proposed DepthCorrectionNet, we remove this branch and feed the concatenation of the RGB image and depth data to the SaliencyNet, shown as \"M2\", which is worse than our method."^^xsd:string ;
    askg-onto:index "8"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depthcorrectionnet,
        askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-141-Sentence-1419 a askg-onto:Sentence ;
    rdfs:label "Sentence 9"@en ;
    domo:Text "On DES [8] dataset, we observe the proposed solution achieves around 4% improvement on S-measure, E-measure and F-measure, which demonstrates the effectiveness of the depth correction net."@en ;
    askg-onto:inSentence "On DES [8] dataset, we observe the proposed solution achieves around 4% improvement on S-measure, E-measure and F-measure, which demonstrates the effectiveness of the depth correction net."^^xsd:string ;
    askg-onto:index "9"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth_correction_net,
        askg-data:Entity-e-measure,
        askg-data:Entity-f-measure,
        askg-data:Entity-proposed_solution,
        askg-data:Entity-s-measure .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 10"@en ;
    domo:Text "Figure 8. Comparisons of saliency maps. \"MH1\" and \"MH2\" are two predictions from M-head. \"DP1\" and \"DP2\" are predictions of two random MC-dropout during test. \"Ours(1)\" and \"Ours(2)\" are two predictions sampled from our CVAE based model. Different from M-head and MC-dropout, which produce consistent predictions for ambiguous images (5th row), *UC-Net* can produce diverse predictions."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14101,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14102,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14103,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14104,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14105,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14106 ;
    askg-onto:index "10"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14101 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Figure 8."@en ;
    askg-onto:inSentence "Figure 8."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-data_visualization,
        askg-data:Entity-figure_8 .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14102 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Comparisons of saliency maps."@en ;
    askg-onto:inSentence "Comparisons of saliency maps."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-saliency_maps .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14103 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "\"MH1\" and \"MH2\" are two predictions from M-head."@en ;
    askg-onto:inSentence "\"MH1\" and \"MH2\" are two predictions from M-head."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-m-head,
        askg-data:Entity-mh1,
        askg-data:Entity-mh2 .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14104 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "\"DP1\" and \"DP2\" are predictions of two random MC-dropout during test."@en ;
    askg-onto:inSentence "\"DP1\" and \"DP2\" are predictions of two random MC-dropout during test."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dp1,
        askg-data:Entity-dp2,
        askg-data:Entity-mc-dropout .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14105 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "\"Ours(1)\" and \"Ours(2)\" are two predictions sampled from our CVAE based model."@en ;
    askg-onto:inSentence "\"Ours(1)\" and \"Ours(2)\" are two predictions sampled from our CVAE based model."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae_based_model,
        askg-data:Entity-ours1,
        askg-data:Entity-ours2 .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1410-Sentence-14106 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "Different from M-head and MC-dropout, which produce consistent predictions for ambiguous images (5th row), *UC-Net* can produce diverse predictions."@en ;
    askg-onto:inSentence "Different from M-head and MC-dropout, which produce consistent predictions for ambiguous images (5th row), *UC-Net* can produce diverse predictions."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-diverse_predictions,
        askg-data:Entity-uc-net .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1411 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 11"@en ;
    domo:Text "| | | | | | Table 2. Ablation study on RGB\\-D saliency datasets. | | | | | | |--------|--------|---------|------|-----------|--------------------------------------------------------|--------------------------|----|----|----|------| | | Metric | UC\\-Net | M1 | M2 M3 | M4 | M5 | M6 | M7 | M8 | M9 | | 8] | Sα ↑ | .897 | .866 | .893 .905 | | .871 .885 .881 .893 .838 | | | | .866 | | [2 K | Fβ ↑ | .886 | .858 | .887 .884 | | .851 .878 .878 .884 .787 | | | | .812 | | | Eξ ↑ | .930 | .905 | .930 .927 | | .910 .923 .927 .932 .840 | | | | .866 | | NJU2 | M ↓ | .043 | .060 | .046 .045 | | .059 .047 .046 .044 .084 | | | | .075 | | ] | Sα ↑ | .903 | .854 | .893 .900 | | .867 .891 .893 .898 .855 | | | | .872 | | [40 | Fβ ↑ | .884 | .831 | .876 .868 | | .834 .864 .876 .882 .793 | | | | .805 | | SSB | Eξ ↑ | .938 | .894 | .911 .922 | | .907 .921 .931 .934 .854 | | | | .870 | | | M ↓ | .039 | .060 | .043 .047 | | .057 .047 .043 .040 .073 | | | | .068 | | [8] | Sα ↑ | .934 | .876 | .896 .928 | | .897 .911 .896 .918 .811 | | | | .911 | | | Fβ ↑ | .919 | .844 | .868 .902 | | .867 .897 .868 .904 .724 | | | | .843 | | ES D | Eξ ↑ | .967 | .906 | .928 .947 | | .930 .945 .928 .953 .794 | | | | .910 | | | M ↓ | .019 | .035 | .026 .024 | | .033 .024 .026 .023 .065 | | | | .036 | | 1] | Sα ↑ | .920 | .878 | .919 .918 | | .890 .899 .910 .915 .850 | | | | .883 | | [4 | Fβ ↑ | .891 | .846 | .897 .878 | | .845 .875 .867 .889 .759 | | | | .795 | | NLPR | Eξ ↑ | .951 | .911 | .953 .941 | | .924 .937 .933 .951 .841 | | | | .883 | | | M ↓ | .025 | .039 | .024 .029 | | .037 .029 .028 .025 .057 | | | | .045 | | ] [35 | Sα ↑ | .864 | .799 | .847 .862 | | .820 .838 .847 .853 .729 | | | | .823 | | | Fβ ↑ | .855 | .791 | .838 .841 | | .802 .833 .838 .848 .661 | | | | .779 | | LFSD | Eξ ↑ | .901 | .829 | .879 .885 | | .865 .875 .879 .891 .720 | | | | .818 | | | M ↓ | .066 | .101 | .079 .075 | | .093 .079 .079 .073 .145 | | | | .108 | | 8] | Sα ↑ | .875 | .846 | .867 .870 | | .851 .859 .867 .865 .810 | | | | .845 | | [1 | Fβ ↑ | .867 | .837 | .860 .848 | | .821 .853 .860 .855 .751 | | | | .795 | | SIP | Eξ ↑ | .914 | .884 | .908 .901 | | .893 .905 .908 .908 .816 | | | | .852 | | | M ↓ | .051 | .068 | .056 .059 | | .067 .057 .056 .056 .094 | | | | .079 |"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1411-Sentence-14111,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1411-Sentence-14112,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1411-Sentence-14113 ;
    askg-onto:index "11"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1411-Sentence-14111 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "| | | | | | Table 2."@en ;
    askg-onto:inSentence "| | | | | | Table 2."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-triple_data .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1411-Sentence-14112 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Ablation study on RGB\\-D saliency datasets."@en ;
    askg-onto:inSentence "Ablation study on RGB\\-D saliency datasets."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ablation,
        askg-data:Entity-rgb-d_saliency_datasets .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1411-Sentence-14113 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "| | | | | | |--------|--------|---------|------|-----------|--------------------------------------------------------|--------------------------|----|----|----|------| | | Metric | UC\\-Net | M1 | M2 M3 | M4 | M5 | M6 | M7 | M8 | M9 | | 8] | Sα ↑ | .897 | .866 | .893 .905 | | .871 .885 .881 .893 .838 | | | | .866 | | [2 K | Fβ ↑ | .886 | .858 | .887 .884 | | .851 .878 .878 .884 .787 | | | | .812 | | | Eξ ↑ | .930 | .905 | .930 .927 | | .910 .923 .927 .932 .840 | | | | .866 | | NJU2 | M ↓ | .043 | .060 | .046 .045 | | .059 .047 .046 .044 .084 | | | | .075 | | ] | Sα ↑ | .903 | .854 | .893 .900 | | .867 .891 .893 .898 .855 | | | | .872 | | [40 | Fβ ↑ | .884 | .831 | .876 .868 | | .834 .864 .876 .882 .793 | | | | .805 | | SSB | Eξ ↑ | .938 | .894 | .911 .922 | | .907 .921 .931 .934 .854 | | | | .870 | | | M ↓ | .039 | .060 | .043 .047 | | .057 .047 .043 .040 .073 | | | | .068 | | [8] | Sα ↑ | .934 | .876 | .896 .928 | | .897 .911 .896 .918 .811 | | | | .911 | | | Fβ ↑ | .919 | .844 | .868 .902 | | .867 .897 .868 .904 .724 | | | | .843 | | ES D | Eξ ↑ | .967 | .906 | .928 .947 | | .930 .945 .928 .953 .794 | | | | .910 | | | M ↓ | .019 | .035 | .026 .024 | | .033 .024 .026 .023 .065 | | | | .036 | | 1] | Sα ↑ | .920 | .878 | .919 .918 | | .890 .899 .910 .915 .850 | | | | .883 | | [4 | Fβ ↑ | .891 | .846 | .897 .878 | | .845 .875 .867 .889 .759 | | | | .795 | | NLPR | Eξ ↑ | .951 | .911 | .953 .941 | | .924 .937 .933 .951 .841 | | | | .883 | | | M ↓ | .025 | .039 | .024 .029 | | .037 .029 .028 .025 .057 | | | | .045 | | ] [35 | Sα ↑ | .864 | .799 | .847 .862 | | .820 .838 .847 .853 .729 | | | | .823 | | | Fβ ↑ | .855 | .791 | .838 .841 | | .802 .833 .838 .848 .661 | | | | .779 | | LFSD | Eξ ↑ | .901 | .829 | .879 .885 | | .865 .875 .879 .891 .720 | | | | .818 | | | M ↓ | .066 | .101 | .079 .075 | | .093 .079 .079 .073 .145 | | | | .108 | | 8] | Sα ↑ | .875 | .846 | .867 .870 | | .851 .859 .867 .865 .810 | | | | .845 | | [1 | Fβ ↑ | .867 | .837 | .860 .848 | | .821 .853 .860 .855 .751 | | | | .795 | | SIP | Eξ ↑ | .914 | .884 | .908 .901 | | .893 .905 .908 .908 .816 | | | | .852 | | | M ↓ | .051 | .068 | .056 .059 | | .067 .057 .056 .056 .094 | | | | .079 |"@en ;
    askg-onto:inSentence "| | | | | | |--------|--------|---------|------|-----------|--------------------------------------------------------|--------------------------|----|----|----|------| | | Metric | UC\\-Net | M1 | M2 M3 | M4 | M5 | M6 | M7 | M8 | M9 | | 8] | Sα ↑ | .897 | .866 | .893 .905 | | .871 .885 .881 .893 .838 | | | | .866 | | [2 K | Fβ ↑ | .886 | .858 | .887 .884 | | .851 .878 .878 .884 .787 | | | | .812 | | | Eξ ↑ | .930 | .905 | .930 .927 | | .910 .923 .927 .932 .840 | | | | .866 | | NJU2 | M ↓ | .043 | .060 | .046 .045 | | .059 .047 .046 .044 .084 | | | | .075 | | ] | Sα ↑ | .903 | .854 | .893 .900 | | .867 .891 .893 .898 .855 | | | | .872 | | [40 | Fβ ↑ | .884 | .831 | .876 .868 | | .834 .864 .876 .882 .793 | | | | .805 | | SSB | Eξ ↑ | .938 | .894 | .911 .922 | | .907 .921 .931 .934 .854 | | | | .870 | | | M ↓ | .039 | .060 | .043 .047 | | .057 .047 .043 .040 .073 | | | | .068 | | [8] | Sα ↑ | .934 | .876 | .896 .928 | | .897 .911 .896 .918 .811 | | | | .911 | | | Fβ ↑ | .919 | .844 | .868 .902 | | .867 .897 .868 .904 .724 | | | | .843 | | ES D | Eξ ↑ | .967 | .906 | .928 .947 | | .930 .945 .928 .953 .794 | | | | .910 | | | M ↓ | .019 | .035 | .026 .024 | | .033 .024 .026 .023 .065 | | | | .036 | | 1] | Sα ↑ | .920 | .878 | .919 .918 | | .890 .899 .910 .915 .850 | | | | .883 | | [4 | Fβ ↑ | .891 | .846 | .897 .878 | | .845 .875 .867 .889 .759 | | | | .795 | | NLPR | Eξ ↑ | .951 | .911 | .953 .941 | | .924 .937 .933 .951 .841 | | | | .883 | | | M ↓ | .025 | .039 | .024 .029 | | .037 .029 .028 .025 .057 | | | | .045 | | ] [35 | Sα ↑ | .864 | .799 | .847 .862 | | .820 .838 .847 .853 .729 | | | | .823 | | | Fβ ↑ | .855 | .791 | .838 .841 | | .802 .833 .838 .848 .661 | | | | .779 | | LFSD | Eξ ↑ | .901 | .829 | .879 .885 | | .865 .875 .879 .891 .720 | | | | .818 | | | M ↓ | .066 | .101 | .079 .075 | | .093 .079 .079 .073 .145 | | | | .108 | | 8] | Sα ↑ | .875 | .846 | .867 .870 | | .851 .859 .867 .865 .810 | | | | .845 | | [1 | Fβ ↑ | .867 | .837 | .860 .848 | | .821 .853 .860 .855 .751 | | | | .795 | | SIP | Eξ ↑ | .914 | .884 | .908 .901 | | .893 .905 .908 .908 .816 | | | | .852 | | | M ↓ | .051 | .068 | .056 .059 | | .067 .057 .056 .056 .094 | | | | .079 |"^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-019,
        askg-data:Entity-025,
        askg-data:Entity-039,
        askg-data:Entity-043,
        askg-data:Entity-051,
        askg-data:Entity-066,
        askg-data:Entity-855,
        askg-data:Entity-864,
        askg-data:Entity-867,
        askg-data:Entity-875,
        askg-data:Entity-884,
        askg-data:Entity-886,
        askg-data:Entity-891,
        askg-data:Entity-897,
        askg-data:Entity-901,
        askg-data:Entity-903,
        askg-data:Entity-914,
        askg-data:Entity-919,
        askg-data:Entity-920,
        askg-data:Entity-930,
        askg-data:Entity-934,
        askg-data:Entity-938,
        askg-data:Entity-951,
        askg-data:Entity-967,
        askg-data:Entity-e%CE%BE,
        askg-data:Entity-es_d,
        askg-data:Entity-f%CE%B2,
        askg-data:Entity-lfsd,
        askg-data:Entity-m,
        askg-data:Entity-nju2,
        askg-data:Entity-nlpr,
        askg-data:Entity-s%CE%B1,
        askg-data:Entity-sip,
        askg-data:Entity-ssb .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 12"@en ;
    domo:Text "HHA is widely used in RGB-D related dense prediction models [11, 24] to obtain better feature representation. To test if HHA also works in our scenario, we replace depth with HHA, and performance is shown in \"M7\". We observe similar performance achieved with HHA instead of the raw depth data. New Label Generation: To produce diverse predictions, we follow [49] and generate diverse annotations for the training dataset. To illustrate the effectiveness of this strategy, we train with only the SaliencyNet to produce single channel saliency map with RGB-D image as input for simplicity. \"M8\" and \"M9\" represent using the provided training dataset and augmented training data respectively. We observe performance improvement of \"M9\" compared with \"M8\", which indicates effectiveness of the new label generation technique."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14121,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14122,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14123,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14124,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14125,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14126,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14127 ;
    askg-onto:index "12"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14121 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "HHA is widely used in RGB-D related dense prediction models [11, 24] to obtain better feature representation."@en ;
    askg-onto:inSentence "HHA is widely used in RGB-D related dense prediction models [11, 24] to obtain better feature representation."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hha,
        askg-data:Entity-rgb-d_related_dense_prediction_models .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14122 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "To test if HHA also works in our scenario, we replace depth with HHA, and performance is shown in \"M7\"."@en ;
    askg-onto:inSentence "To test if HHA also works in our scenario, we replace depth with HHA, and performance is shown in \"M7\"."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hha,
        askg-data:Entity-m7,
        askg-data:Entity-performance,
        askg-data:Entity-scenario .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14123 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "We observe similar performance achieved with HHA instead of the raw depth data."@en ;
    askg-onto:inSentence "We observe similar performance achieved with HHA instead of the raw depth data."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hha,
        askg-data:Entity-raw_depth_data .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14124 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "New Label Generation: To produce diverse predictions, we follow [49] and generate diverse annotations for the training dataset."@en ;
    askg-onto:inSentence "New Label Generation: To produce diverse predictions, we follow [49] and generate diverse annotations for the training dataset."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-diverse_annotations,
        askg-data:Entity-diverse_predictions,
        askg-data:Entity-label_generation,
        askg-data:Entity-training_dataset .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14125 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "To illustrate the effectiveness of this strategy, we train with only the SaliencyNet to produce single channel saliency map with RGB-D image as input for simplicity."@en ;
    askg-onto:inSentence "To illustrate the effectiveness of this strategy, we train with only the SaliencyNet to produce single channel saliency map with RGB-D image as input for simplicity."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-rgb-d_image,
        askg-data:Entity-saliencynet,
        askg-data:Entity-single_channel_saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14126 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "\"M8\" and \"M9\" represent using the provided training dataset and augmented training data respectively."@en ;
    askg-onto:inSentence "\"M8\" and \"M9\" represent using the provided training dataset and augmented training data respectively."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-augmented_training_data,
        askg-data:Entity-m8,
        askg-data:Entity-m9,
        askg-data:Entity-training_dataset .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-1412-Sentence-14127 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "We observe performance improvement of \"M9\" compared with \"M8\", which indicates effectiveness of the new label generation technique."@en ;
    askg-onto:inSentence "We observe performance improvement of \"M9\" compared with \"M8\", which indicates effectiveness of the new label generation technique."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-label_generation_technique,
        askg-data:Entity-m8,
        askg-data:Entity-m9,
        askg-data:Entity-performance_improvement .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-142 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "Multi-head vs. **CVAE:** Multi-head models [46] generate multiple predictions with different decoders and a shared encoder, and the loss function is always defined as the closest of the multiple predictions. We remove the LatentNet,"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-142-Sentence-1421,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-142-Sentence-1422,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-142-Sentence-1423 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-142-Sentence-1421 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Multi-head vs."@en ;
    askg-onto:inSentence "Multi-head vs."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-model,
        askg-data:Entity-multi-head .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-142-Sentence-1422 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "**CVAE:** Multi-head models [46] generate multiple predictions with different decoders and a shared encoder, and the loss function is always defined as the closest of the multiple predictions."@en ;
    askg-onto:inSentence "**CVAE:** Multi-head models [46] generate multiple predictions with different decoders and a shared encoder, and the loss function is always defined as the closest of the multiple predictions."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-model,
        askg-data:Entity-multi-head_models,
        askg-data:Entity-multiple_predictions,
        askg-data:Entity-the_closest_of_the_multiple_predictions .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-142-Sentence-1423 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "We remove the LatentNet,"@en ;
    askg-onto:inSentence "We remove the LatentNet,"^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-latentnet .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-143 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 3"@en ;
    domo:Text "| | | | | | | | Handcrafted Feature based Models | | | | | | | | Deep Models | | | | | |------------|----------|---------------------|------|------|------|------|------------------------------------|-------------------------|------|------|------------|------|------|------|---------------|---------------------------------------------|------|------|------| | | | Metric LHM CDB DESM | | | GP | | | CDCP ACSD LBE DCMC MDSF | | | SE DF | | | | | AFNet CTMF MMCI PCF TANet CPFP DMRA UC\\-Net | | | | | | | [41] | [36] | [8] | [44] | [66] | [28] | [20] | [10] | [51] | [22] [43] | [54] | [24] | [7] | [5] | [6] | [64] | [61] | Ours | | | Sα ↑ | .514 | .632 | .665 | .527 | .669 | .699 | .695 | .686 | .748 | .664 .763 | .822 | .849 | .858 | .877 | .879 | .878 | .886 | .897 | | | Fβ ↑ | .328 | .498 | .550 | .357 | .595 | .512 | .606 | .556 | .628 | .583 .653 | .827 | .779 | .793 | .840 | .841 | .850 | .873 | .886 | | NJU2K [28] | Eξ ↑ | .447 | .572 | .590 | .466 | .706 | .594 | .655 | .619 | .677 | .624 .700 | .867 | .846 | .851 | .895 | .895 | .910 | .920 | .930 | | | M ↓ .205 | | .199 | .283 | .211 | .180 | .202 | .153 | .172 | .157 | .169 .140 | .077 | .085 | .079 | .059 | .061 | .053 | .051 | .043 | | SSB [40] | Sα ↑ | .562 | .615 | .642 | .588 | .713 | .692 | .660 | .731 | .728 | .708 .757 | .825 | .848 | .873 | .875 | .871 | .879 | .835 | .903 | | | Fβ ↑ | .378 | .489 | .519 | .405 | .638 | .478 | .501 | .590 | .527 | .611 .617 | .806 | .758 | .813 | .818 | .828 | .841 | .837 | .884 | | | Eξ ↑ | .484 | .561 | .579 | .508 | .751 | .592 | .601 | .655 | .614 | .664 .692 | .872 | .841 | .873 | .887 | .893 | .911 | .879 | .938 | | | M ↓ .172 | | .166 | .295 | .182 | .149 | .200 | .250 | .148 | .176 | .143 .141 | .075 | .086 | .068 | .064 | .060 | .051 | .066 | .039 | | | Sα ↑ | .578 | .645 | .622 | .636 | .709 | .728 | .703 | .707 | .741 | .741 .752 | .770 | .863 | .848 | .842 | .858 | .872 | .900 | .934 | | DES [8] | Fβ ↑ | .345 | .502 | .483 | .412 | .585 | .513 | .576 | .542 | .523 | .618 .604 | .713 | .756 | .735 | .765 | .790 | .824 | .873 | .919 | | | Eξ ↑ | .477 | .572 | .566 | .503 | .748 | .613 | .650 | .631 | .621 | .706 .684 | .809 | .826 | .825 | .838 | .863 | .888 | .933 | .967 | | | M ↓ .114 | | .100 | .299 | .168 | .115 | .169 | .208 | .111 | .122 | .090 .093 | .068 | .055 | .065 | .049 | .046 | .038 | .030 | .019 | | NLPR [41] | Sα ↑ | .630 | .632 | .572 | .655 | .727 | .673 | .762 | .724 | .805 | .756 .806 | .799 | .860 | .856 | .874 | .886 | .888 | .899 | .920 | | | Fβ ↑ | .427 | .421 | .430 | .451 | .609 | .429 | .636 | .542 | .649 | .624 .664 | .755 | .740 | .737 | .802 | .819 | .840 | .865 | .891 | | | Eξ ↑ | .560 | .567 | .542 | .571 | .782 | .579 | .719 | .684 | .745 | .742 .757 | .851 | .840 | .841 | .887 | .902 | .918 | .940 | .951 | | | M ↓ .108 | | .108 | .312 | .146 | .112 | .179 | .081 | .117 | .095 | .091 .079 | .058 | .056 | .059 | .044 | .041 | .036 | .031 | .025 | | LFSD [35] | Sα ↑ | .557 | .520 | .722 | .640 | .717 | .734 | .736 | .753 | .700 | .698 .791 | .738 | .796 | .787 | .794 | .801 | .828 | .847 | .864 | | | Fβ ↑ | .396 | .376 | .612 | .519 | .680 | .566 | .612 | .655 | .521 | .640 .679 | .736 | .756 | .722 | .761 | .771 | .811 | .845 | .855 | | | Eξ ↑ | .491 | .465 | .638 | .584 | .754 | .625 | .670 | .682 | .588 | .653 .725 | .796 | .810 | .775 | .818 | .821 | .863 | .893 | .901 | | | M ↓ .211 | | .218 | .248 | .183 | .167 | .188 | .208 | .155 | .190 | .167 .138 | .134 | .119 | .132 | .112 | .111 | .088 | .075 | .066 | | SIP [18] | Sα ↑ | .511 | .557 | .616 | .588 | .595 | .732 | .727 | .683 | .717 | .628 .653 | .720 | .716 | .833 | .842 | .835 | .850 | .806 | .875 | | | Fβ ↑ | .287 | .341 | .496 | .411 | .482 | .542 | .572 | .500 | .568 | .515 .465 | .702 | .608 | .771 | .814 | .803 | .821 | .811 | .867 | | | Eξ ↑ | .437 | .455 | .564 | .511 | .683 | .614 | .651 | .598 | .645 | .592 .565 | .793 | .704 | .845 | .878 | .870 | .893 | .844 | .914 | | | M ↓ .184 | | .192 | .298 | .173 | .224 | .172 | .200 | .186 | .167 | .164 .185 | .118 | .139 | .086 | .071 | .075 | .064 | .085 | .051 |"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-143-Sentence-1431 ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-143-Sentence-1431 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "| | | | | | | | Handcrafted Feature based Models | | | | | | | | Deep Models | | | | | |------------|----------|---------------------|------|------|------|------|------------------------------------|-------------------------|------|------|------------|------|------|------|---------------|---------------------------------------------|------|------|------| | | | Metric LHM CDB DESM | | | GP | | | CDCP ACSD LBE DCMC MDSF | | | SE DF | | | | | AFNet CTMF MMCI PCF TANet CPFP DMRA UC\\-Net | | | | | | | [41] | [36] | [8] | [44] | [66] | [28] | [20] | [10] | [51] | [22] [43] | [54] | [24] | [7] | [5] | [6] | [64] | [61] | Ours | | | Sα ↑ | .514 | .632 | .665 | .527 | .669 | .699 | .695 | .686 | .748 | .664 .763 | .822 | .849 | .858 | .877 | .879 | .878 | .886 | .897 | | | Fβ ↑ | .328 | .498 | .550 | .357 | .595 | .512 | .606 | .556 | .628 | .583 .653 | .827 | .779 | .793 | .840 | .841 | .850 | .873 | .886 | | NJU2K [28] | Eξ ↑ | .447 | .572 | .590 | .466 | .706 | .594 | .655 | .619 | .677 | .624 .700 | .867 | .846 | .851 | .895 | .895 | .910 | .920 | .930 | | | M ↓ .205 | | .199 | .283 | .211 | .180 | .202 | .153 | .172 | .157 | .169 .140 | .077 | .085 | .079 | .059 | .061 | .053 | .051 | .043 | | SSB [40] | Sα ↑ | .562 | .615 | .642 | .588 | .713 | .692 | .660 | .731 | .728 | .708 .757 | .825 | .848 | .873 | .875 | .871 | .879 | .835 | .903 | | | Fβ ↑ | .378 | .489 | .519 | .405 | .638 | .478 | .501 | .590 | .527 | .611 .617 | .806 | .758 | .813 | .818 | .828 | .841 | .837 | .884 | | | Eξ ↑ | .484 | .561 | .579 | .508 | .751 | .592 | .601 | .655 | .614 | .664 .692 | .872 | .841 | .873 | .887 | .893 | .911 | .879 | .938 | | | M ↓ .172 | | .166 | .295 | .182 | .149 | .200 | .250 | .148 | .176 | .143 .141 | .075 | .086 | .068 | .064 | .060 | .051 | .066 | .039 | | | Sα ↑ | .578 | .645 | .622 | .636 | .709 | .728 | .703 | .707 | .741 | .741 .752 | .770 | .863 | .848 | .842 | .858 | .872 | .900 | .934 | | DES [8] | Fβ ↑ | .345 | .502 | .483 | .412 | .585 | .513 | .576 | .542 | .523 | .618 .604 | .713 | .756 | .735 | .765 | .790 | .824 | .873 | .919 | | | Eξ ↑ | .477 | .572 | .566 | .503 | .748 | .613 | .650 | .631 | .621 | .706 .684 | .809 | .826 | .825 | .838 | .863 | .888 | .933 | .967 | | | M ↓ .114 | | .100 | .299 | .168 | .115 | .169 | .208 | .111 | .122 | .090 .093 | .068 | .055 | .065 | .049 | .046 | .038 | .030 | .019 | | NLPR [41] | Sα ↑ | .630 | .632 | .572 | .655 | .727 | .673 | .762 | .724 | .805 | .756 .806 | .799 | .860 | .856 | .874 | .886 | .888 | .899 | .920 | | | Fβ ↑ | .427 | .421 | .430 | .451 | .609 | .429 | .636 | .542 | .649 | .624 .664 | .755 | .740 | .737 | .802 | .819 | .840 | .865 | .891 | | | Eξ ↑ | .560 | .567 | .542 | .571 | .782 | .579 | .719 | .684 | .745 | .742 .757 | .851 | .840 | .841 | .887 | .902 | .918 | .940 | .951 | | | M ↓ .108 | | .108 | .312 | .146 | .112 | .179 | .081 | .117 | .095 | .091 .079 | .058 | .056 | .059 | .044 | .041 | .036 | .031 | .025 | | LFSD [35] | Sα ↑ | .557 | .520 | .722 | .640 | .717 | .734 | .736 | .753 | .700 | .698 .791 | .738 | .796 | .787 | .794 | .801 | .828 | .847 | .864 | | | Fβ ↑ | .396 | .376 | .612 | .519 | .680 | .566 | .612 | .655 | .521 | .640 .679 | .736 | .756 | .722 | .761 | .771 | .811 | .845 | .855 | | | Eξ ↑ | .491 | .465 | .638 | .584 | .754 | .625 | .670 | .682 | .588 | .653 .725 | .796 | .810 | .775 | .818 | .821 | .863 | .893 | .901 | | | M ↓ .211 | | .218 | .248 | .183 | .167 | .188 | .208 | .155 | .190 | .167 .138 | .134 | .119 | .132 | .112 | .111 | .088 | .075 | .066 | | SIP [18] | Sα ↑ | .511 | .557 | .616 | .588 | .595 | .732 | .727 | .683 | .717 | .628 .653 | .720 | .716 | .833 | .842 | .835 | .850 | .806 | .875 | | | Fβ ↑ | .287 | .341 | .496 | .411 | .482 | .542 | .572 | .500 | .568 | .515 .465 | .702 | .608 | .771 | .814 | .803 | .821 | .811 | .867 | | | Eξ ↑ | .437 | .455 | .564 | .511 | .683 | .614 | .651 | .598 | .645 | .592 .565 | .793 | .704 | .845 | .878 | .870 | .893 | .844 | .914 | | | M ↓ .184 | | .192 | .298 | .173 | .224 | .172 | .200 | .186 | .167 | .164 .185 | .118 | .139 | .086 | .071 | .075 | .064 | .085 | .051 |"@en ;
    askg-onto:inSentence "| | | | | | | | Handcrafted Feature based Models | | | | | | | | Deep Models | | | | | |------------|----------|---------------------|------|------|------|------|------------------------------------|-------------------------|------|------|------------|------|------|------|---------------|---------------------------------------------|------|------|------| | | | Metric LHM CDB DESM | | | GP | | | CDCP ACSD LBE DCMC MDSF | | | SE DF | | | | | AFNet CTMF MMCI PCF TANet CPFP DMRA UC\\-Net | | | | | | | [41] | [36] | [8] | [44] | [66] | [28] | [20] | [10] | [51] | [22] [43] | [54] | [24] | [7] | [5] | [6] | [64] | [61] | Ours | | | Sα ↑ | .514 | .632 | .665 | .527 | .669 | .699 | .695 | .686 | .748 | .664 .763 | .822 | .849 | .858 | .877 | .879 | .878 | .886 | .897 | | | Fβ ↑ | .328 | .498 | .550 | .357 | .595 | .512 | .606 | .556 | .628 | .583 .653 | .827 | .779 | .793 | .840 | .841 | .850 | .873 | .886 | | NJU2K [28] | Eξ ↑ | .447 | .572 | .590 | .466 | .706 | .594 | .655 | .619 | .677 | .624 .700 | .867 | .846 | .851 | .895 | .895 | .910 | .920 | .930 | | | M ↓ .205 | | .199 | .283 | .211 | .180 | .202 | .153 | .172 | .157 | .169 .140 | .077 | .085 | .079 | .059 | .061 | .053 | .051 | .043 | | SSB [40] | Sα ↑ | .562 | .615 | .642 | .588 | .713 | .692 | .660 | .731 | .728 | .708 .757 | .825 | .848 | .873 | .875 | .871 | .879 | .835 | .903 | | | Fβ ↑ | .378 | .489 | .519 | .405 | .638 | .478 | .501 | .590 | .527 | .611 .617 | .806 | .758 | .813 | .818 | .828 | .841 | .837 | .884 | | | Eξ ↑ | .484 | .561 | .579 | .508 | .751 | .592 | .601 | .655 | .614 | .664 .692 | .872 | .841 | .873 | .887 | .893 | .911 | .879 | .938 | | | M ↓ .172 | | .166 | .295 | .182 | .149 | .200 | .250 | .148 | .176 | .143 .141 | .075 | .086 | .068 | .064 | .060 | .051 | .066 | .039 | | | Sα ↑ | .578 | .645 | .622 | .636 | .709 | .728 | .703 | .707 | .741 | .741 .752 | .770 | .863 | .848 | .842 | .858 | .872 | .900 | .934 | | DES [8] | Fβ ↑ | .345 | .502 | .483 | .412 | .585 | .513 | .576 | .542 | .523 | .618 .604 | .713 | .756 | .735 | .765 | .790 | .824 | .873 | .919 | | | Eξ ↑ | .477 | .572 | .566 | .503 | .748 | .613 | .650 | .631 | .621 | .706 .684 | .809 | .826 | .825 | .838 | .863 | .888 | .933 | .967 | | | M ↓ .114 | | .100 | .299 | .168 | .115 | .169 | .208 | .111 | .122 | .090 .093 | .068 | .055 | .065 | .049 | .046 | .038 | .030 | .019 | | NLPR [41] | Sα ↑ | .630 | .632 | .572 | .655 | .727 | .673 | .762 | .724 | .805 | .756 .806 | .799 | .860 | .856 | .874 | .886 | .888 | .899 | .920 | | | Fβ ↑ | .427 | .421 | .430 | .451 | .609 | .429 | .636 | .542 | .649 | .624 .664 | .755 | .740 | .737 | .802 | .819 | .840 | .865 | .891 | | | Eξ ↑ | .560 | .567 | .542 | .571 | .782 | .579 | .719 | .684 | .745 | .742 .757 | .851 | .840 | .841 | .887 | .902 | .918 | .940 | .951 | | | M ↓ .108 | | .108 | .312 | .146 | .112 | .179 | .081 | .117 | .095 | .091 .079 | .058 | .056 | .059 | .044 | .041 | .036 | .031 | .025 | | LFSD [35] | Sα ↑ | .557 | .520 | .722 | .640 | .717 | .734 | .736 | .753 | .700 | .698 .791 | .738 | .796 | .787 | .794 | .801 | .828 | .847 | .864 | | | Fβ ↑ | .396 | .376 | .612 | .519 | .680 | .566 | .612 | .655 | .521 | .640 .679 | .736 | .756 | .722 | .761 | .771 | .811 | .845 | .855 | | | Eξ ↑ | .491 | .465 | .638 | .584 | .754 | .625 | .670 | .682 | .588 | .653 .725 | .796 | .810 | .775 | .818 | .821 | .863 | .893 | .901 | | | M ↓ .211 | | .218 | .248 | .183 | .167 | .188 | .208 | .155 | .190 | .167 .138 | .134 | .119 | .132 | .112 | .111 | .088 | .075 | .066 | | SIP [18] | Sα ↑ | .511 | .557 | .616 | .588 | .595 | .732 | .727 | .683 | .717 | .628 .653 | .720 | .716 | .833 | .842 | .835 | .850 | .806 | .875 | | | Fβ ↑ | .287 | .341 | .496 | .411 | .482 | .542 | .572 | .500 | .568 | .515 .465 | .702 | .608 | .771 | .814 | .803 | .821 | .811 | .867 | | | Eξ ↑ | .437 | .455 | .564 | .511 | .683 | .614 | .651 | .598 | .645 | .592 .565 | .793 | .704 | .845 | .878 | .870 | .893 | .844 | .914 | | | M ↓ .184 | | .192 | .298 | .173 | .224 | .172 | .200 | .186 | .167 | .164 .185 | .118 | .139 | .086 | .071 | .075 | .064 | .085 | .051 |"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-afnet_ctmf_mmci_pcf_tanet_cpfp_dmra_uc-net,
        askg-data:Entity-cdcp_acsd_lbe_dcmc_mdsf,
        askg-data:Entity-deep_models,
        askg-data:Entity-des,
        askg-data:Entity-gp,
        askg-data:Entity-handcrafted_feature_based_models,
        askg-data:Entity-lfsd,
        askg-data:Entity-lhm_cdb_desm,
        askg-data:Entity-metric,
        askg-data:Entity-nju2k,
        askg-data:Entity-nlpr,
        askg-data:Entity-se_df,
        askg-data:Entity-sip,
        askg-data:Entity-ssb .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 4"@en ;
    domo:Text "Table 1. Benchmarking results of ten leading handcrafted feature-based models and eight deep models on six RGBD saliency datasets. ↑ & ↓ denote larger and smaller is better, respectively. Here, we adopt mean Fβ and mean Eξ [15]."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144-Sentence-1441,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144-Sentence-1442,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144-Sentence-1443,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144-Sentence-1444 ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144-Sentence-1441 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Table 1."@en ;
    askg-onto:inSentence "Table 1."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-table_1,
        askg-data:Entity-triple_data .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144-Sentence-1442 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Benchmarking results of ten leading handcrafted feature-based models and eight deep models on six RGBD saliency datasets."@en ;
    askg-onto:inSentence "Benchmarking results of ten leading handcrafted feature-based models and eight deep models on six RGBD saliency datasets."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-eight_deep_models,
        askg-data:Entity-six_rgbd_saliency_datasets,
        askg-data:Entity-ten_leading_handcrafted_feature-based_models .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144-Sentence-1443 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "↑ & ↓ denote larger and smaller is better, respectively."@en ;
    askg-onto:inSentence "↑ & ↓ denote larger and smaller is better, respectively."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-,
        askg-data:Entity-larger_is_better,
        askg-data:Entity-smaller_is_better .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-144-Sentence-1444 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Here, we adopt mean Fβ and mean Eξ [15]."@en ;
    askg-onto:inSentence "Here, we adopt mean Fβ and mean Eξ [15]."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-mean_e%CE%BE,
        askg-data:Entity-mean_f%CE%B2 .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-145 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 5"@en ;
    domo:Text "![6_image_0.png](6_image_0.png)"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-145-Sentence-1451 ;
    askg-onto:index "5"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-145-Sentence-1451 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "![6_image_0.png](6_image_0.png)"@en ;
    askg-onto:inSentence "![6_image_0.png](6_image_0.png)"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-crispr,
        askg-data:Entity-crispr-cas9,
        askg-data:Entity-crispr_research,
        askg-data:Entity-emmanuelle_charpentier,
        askg-data:Entity-genetic_engineering,
        askg-data:Entity-genetics,
        askg-data:Entity-harvard_university,
        askg-data:Entity-jennifer_doudna,
        askg-data:Entity-national_institutes_of_health,
        askg-data:Entity-scientist,
        askg-data:Entity-technology,
        askg-data:Entity-tool,
        askg-data:Entity-university_of_california_san_francisco .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-146 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 6"@en ;
    domo:Text "Figure 7. E-measure (1st row) and F-measure (2nd row) curves on four testing datasets."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-146-Sentence-1461,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-146-Sentence-1462 ;
    askg-onto:index "6"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-146-Sentence-1461 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Figure 7."@en ;
    askg-onto:inSentence "Figure 7."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-data_visualization,
        askg-data:Entity-figure_7 .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-146-Sentence-1462 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "E-measure (1st row) and F-measure (2nd row) curves on four testing datasets."@en ;
    askg-onto:inSentence "E-measure (1st row) and F-measure (2nd row) curves on four testing datasets."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dataset,
        askg-data:Entity-e-measure,
        askg-data:Entity-f-measure,
        askg-data:Entity-four_testing_datasets,
        askg-data:Entity-metric .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 7"@en ;
    domo:Text "and copy the decoder of the SaliencyNet multiple times to achieve multiple predictions (\"M5\" in this paper). We report performance in \"M5\" as mean of the multiple predictions. \"M5\" is better than SOTA models (e.g., DMRA) while there still exists gap between M-head based method (\"M5\") and our CVAE based model (*UC-Net*). Monte-Carlo Dropout vs. **CVAE:** Monte-Carlo Dropout [30] uses dropout during the testing stage to introduce stochastic to the network. We follow [30] to remove the LatentNet, and use dropout in the encoder and decoder of the SaliencyNet in the testing stage. We repeats five times of random dropout (dropout ratio = 0.1), and report the mean performance as \"M6\". Similar to \"M5\", \"M6\" also achieves the best performance comparing with SOTA models (e.g., CPFP and DMRA), while the proposed CVAE based model achieves even better performance."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1471,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1472,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1473,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1474,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1475,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1476,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1477,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1478 ;
    askg-onto:index "7"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1471 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "and copy the decoder of the SaliencyNet multiple times to achieve multiple predictions (\"M5\" in this paper)."@en ;
    askg-onto:inSentence "and copy the decoder of the SaliencyNet multiple times to achieve multiple predictions (\"M5\" in this paper)."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-multiple_predictions,
        askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1472 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "We report performance in \"M5\" as mean of the multiple predictions."@en ;
    askg-onto:inSentence "We report performance in \"M5\" as mean of the multiple predictions."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-m5,
        askg-data:Entity-mean_of_the_multiple_predictions .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1473 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "\"M5\" is better than SOTA models (e.g., DMRA) while there still exists gap between M-head based method (\"M5\") and our CVAE based model (*UC-Net*)."@en ;
    askg-onto:inSentence "\"M5\" is better than SOTA models (e.g., DMRA) while there still exists gap between M-head based method (\"M5\") and our CVAE based model (*UC-Net*)."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae_based_model,
        askg-data:Entity-m-head_based_method,
        askg-data:Entity-m5,
        askg-data:Entity-sota_models,
        askg-data:Entity-uc-net .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1474 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Monte-Carlo Dropout vs."@en ;
    askg-onto:inSentence "Monte-Carlo Dropout vs."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-method,
        askg-data:Entity-monte-carlo_dropout .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1475 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "**CVAE:** Monte-Carlo Dropout [30] uses dropout during the testing stage to introduce stochastic to the network."@en ;
    askg-onto:inSentence "**CVAE:** Monte-Carlo Dropout [30] uses dropout during the testing stage to introduce stochastic to the network."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dropout,
        askg-data:Entity-monte-carlo_dropout,
        askg-data:Entity-stochastic .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1476 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "We follow [30] to remove the LatentNet, and use dropout in the encoder and decoder of the SaliencyNet in the testing stage."@en ;
    askg-onto:inSentence "We follow [30] to remove the LatentNet, and use dropout in the encoder and decoder of the SaliencyNet in the testing stage."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-decoder,
        askg-data:Entity-dropout,
        askg-data:Entity-encoder,
        askg-data:Entity-latentnet,
        askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1477 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "We repeats five times of random dropout (dropout ratio = 0.1), and report the mean performance as \"M6\"."@en ;
    askg-onto:inSentence "We repeats five times of random dropout (dropout ratio = 0.1), and report the mean performance as \"M6\"."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-m6,
        askg-data:Entity-method,
        askg-data:Entity-metric,
        askg-data:Entity-random_dropout .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-147-Sentence-1478 a askg-onto:Sentence ;
    rdfs:label "Sentence 8"@en ;
    domo:Text "Similar to \"M5\", \"M6\" also achieves the best performance comparing with SOTA models (e.g., CPFP and DMRA), while the proposed CVAE based model achieves even better performance."@en ;
    askg-onto:inSentence "Similar to \"M5\", \"M6\" also achieves the best performance comparing with SOTA models (e.g., CPFP and DMRA), while the proposed CVAE based model achieves even better performance."^^xsd:string ;
    askg-onto:index "8"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-best_performance,
        askg-data:Entity-better_performance,
        askg-data:Entity-cpfp_and_dmra,
        askg-data:Entity-cvae_based_model,
        askg-data:Entity-m5,
        askg-data:Entity-m6,
        askg-data:Entity-sota_models .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-148 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 8"@en ;
    domo:Text "HHA vs. **Depth:** HHA [23] is a widely used technique that encodes the depth data to three channels: horizontal disparity, height above ground, and the angle the pixels local surface normal makes with the inferred gravity direction."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-148-Sentence-1481,
        askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-148-Sentence-1482 ;
    askg-onto:index "8"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-148-Sentence-1481 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "HHA vs."@en ;
    askg-onto:inSentence "HHA vs."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hha .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-148-Sentence-1482 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "**Depth:** HHA [23] is a widely used technique that encodes the depth data to three channels: horizontal disparity, height above ground, and the angle the pixels local surface normal makes with the inferred gravity direction."@en ;
    askg-onto:inSentence "**Depth:** HHA [23] is a widely used technique that encodes the depth data to three channels: horizontal disparity, height above ground, and the angle the pixels local surface normal makes with the inferred gravity direction."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-angle,
        askg-data:Entity-depth_data,
        askg-data:Entity-height_above_ground,
        askg-data:Entity-hha,
        askg-data:Entity-horizontal_disparity,
        askg-data:Entity-inferred_gravity_direction,
        askg-data:Entity-pixels_local_surface_normal,
        askg-data:Entity-technique,
        askg-data:Entity-three_channels .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-149 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 9"@en ;
    domo:Text "![7_image_0.png](7_image_0.png)"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-149-Sentence-1491 ;
    askg-onto:index "9"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-14-Paragraph-149-Sentence-1491 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "![7_image_0.png](7_image_0.png)"@en ;
    askg-onto:inSentence "![7_image_0.png](7_image_0.png)"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-caffe,
        askg-data:Entity-convolutional_neural_network,
        askg-data:Entity-data,
        askg-data:Entity-dataset,
        askg-data:Entity-deep_learning,
        askg-data:Entity-framework,
        askg-data:Entity-image_classification,
        askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-organization,
        askg-data:Entity-pytorch,
        askg-data:Entity-research_group,
        askg-data:Entity-study,
        askg-data:Entity-task,
        askg-data:Entity-tensorflow,
        askg-data:Entity-university_of_california .

askg-data:Paper-fdb482b8f2df5642-Section-15 a askg-onto:Section ;
    rdfs:label "Section 15"@en ;
    domo:Text "5. Conclusion"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151,
        askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-152 ;
    askg-onto:index "15"^^xsd:int ;
    askg-onto:level "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "Inspired by human uncertainty in ground truth (GT) annotation, we proposed the first uncertainty network named UC-Net for RGB-D saliency detection based on a conditional variational autoencoder. Different from existing methods, which generally treat saliency detection as a point estimation problem, we propose to learn the distribution of saliency maps. Under our formulation, our model is able to generate multiple labels which have been discarded in the GT annotation generation process through saliency consensus. Quantitative and qualitative evaluations on six standard and challenging benchmark datasets demonstrated the superiority of our approach in learning the distribution of saliency maps. In the future, we would like to extend our approach to other saliency detection problems (e.g., VSOD [19], RGB SOD [13, 65], Co-SOD [17]). Furthermore, we plan to capture new datasets with multiple human annotations to further model the statistics of human uncertainty in interactive image segmentation [37], camouflaged object detection [16], etc."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1511,
        askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1512,
        askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1513,
        askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1514,
        askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1515,
        askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1516 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1511 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Inspired by human uncertainty in ground truth (GT) annotation, we proposed the first uncertainty network named UC-Net for RGB-D saliency detection based on a conditional variational autoencoder."@en ;
    askg-onto:inSentence "Inspired by human uncertainty in ground truth (GT) annotation, we proposed the first uncertainty network named UC-Net for RGB-D saliency detection based on a conditional variational autoencoder."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-conditional_variational_autoencoder,
        askg-data:Entity-model,
        askg-data:Entity-rgb-d_saliency_detection,
        askg-data:Entity-uc-net,
        askg-data:Entity-uncertainty_network .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1512 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Different from existing methods, which generally treat saliency detection as a point estimation problem, we propose to learn the distribution of saliency maps."@en ;
    askg-onto:inSentence "Different from existing methods, which generally treat saliency detection as a point estimation problem, we propose to learn the distribution of saliency maps."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-distribution_of_saliency_maps,
        askg-data:Entity-point_estimation_problem,
        askg-data:Entity-saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1513 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Under our formulation, our model is able to generate multiple labels which have been discarded in the GT annotation generation process through saliency consensus."@en ;
    askg-onto:inSentence "Under our formulation, our model is able to generate multiple labels which have been discarded in the GT annotation generation process through saliency consensus."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-model,
        askg-data:Entity-multiple_labels .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1514 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Quantitative and qualitative evaluations on six standard and challenging benchmark datasets demonstrated the superiority of our approach in learning the distribution of saliency maps."@en ;
    askg-onto:inSentence "Quantitative and qualitative evaluations on six standard and challenging benchmark datasets demonstrated the superiority of our approach in learning the distribution of saliency maps."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-our_approach,
        askg-data:Entity-six_standard_and_challenging_benchmark_datasets,
        askg-data:Entity-the_superiority_of_learning_the_distribution_of_saliency_maps .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1515 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "In the future, we would like to extend our approach to other saliency detection problems (e.g., VSOD [19], RGB SOD [13, 65], Co-SOD [17])."@en ;
    askg-onto:inSentence "In the future, we would like to extend our approach to other saliency detection problems (e.g., VSOD [19], RGB SOD [13, 65], Co-SOD [17])."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-co-sod,
        askg-data:Entity-rgb_sod,
        askg-data:Entity-saliency_detection_problems,
        askg-data:Entity-vsod .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-151-Sentence-1516 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "Furthermore, we plan to capture new datasets with multiple human annotations to further model the statistics of human uncertainty in interactive image segmentation [37], camouflaged object detection [16], etc."@en ;
    askg-onto:inSentence "Furthermore, we plan to capture new datasets with multiple human annotations to further model the statistics of human uncertainty in interactive image segmentation [37], camouflaged object detection [16], etc."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-camouflaged_object_detection,
        askg-data:Entity-datasets,
        askg-data:Entity-human_annotations,
        askg-data:Entity-human_uncertainty,
        askg-data:Entity-image_segmentation,
        askg-data:Entity-statistics .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-152 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "Acknowledgments. This research was supported in part by Natural Science Foundation of China grants (61871325, 61420106007, 61671387), the Australia Research Council Centre of Excellence for Robotics Vision (CE140100016), and the National Key Research and Development Program of China under Grant 2018AAA0102803. We thank all reviewers and Area Chairs for their constructive comments."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-152-Sentence-1521,
        askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-152-Sentence-1522,
        askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-152-Sentence-1523 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-152-Sentence-1521 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Acknowledgments."@en ;
    askg-onto:inSentence "Acknowledgments."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-acknowledgments .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-152-Sentence-1522 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "This research was supported in part by Natural Science Foundation of China grants (61871325, 61420106007, 61671387), the Australia Research Council Centre of Excellence for Robotics Vision (CE140100016), and the National Key Research and Development Program of China under Grant 2018AAA0102803."@en ;
    askg-onto:inSentence "This research was supported in part by Natural Science Foundation of China grants (61871325, 61420106007, 61671387), the Australia Research Council Centre of Excellence for Robotics Vision (CE140100016), and the National Key Research and Development Program of China under Grant 2018AAA0102803."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-australia_research_council_centre_of_excellence_for_robotics_vision,
        askg-data:Entity-national_key_research_and_development_program_of_china,
        askg-data:Entity-natural_science_foundation_of_china,
        askg-data:Entity-this_research .

askg-data:Paper-fdb482b8f2df5642-Section-15-Paragraph-152-Sentence-1523 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "We thank all reviewers and Area Chairs for their constructive comments."@en ;
    askg-onto:inSentence "We thank all reviewers and Area Chairs for their constructive comments."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-area_chairs,
        askg-data:Entity-reviewers .

askg-data:Paper-fdb482b8f2df5642-Section-16 a askg-onto:Section ;
    rdfs:label "Section 16"@en ;
    domo:Text "References"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1611,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1612,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1613,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1614,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1615,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1616,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1617,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1618,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1619,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-162,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1620,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1621,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1622,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1623,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1624,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1625,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1626,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1627,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1628,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1629,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1630,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1631,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1632,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1633,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1634,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1635,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1636,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1638,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1639,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-164,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1640,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1641,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1642,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1643,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1644,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1645,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1646,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1647,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1648,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1649,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-165,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1650,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1651,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1652,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1654,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1655,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1656,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1657,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1658,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1659,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-166,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1660,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1661,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1662,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1663,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1664,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1665,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1666,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1667,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1669,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-167,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1670,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1671,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1672,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1673,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1674,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1675,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1676,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-168,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-169 ;
    askg-onto:index "16"^^xsd:int ;
    askg-onto:level "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "[1] Abubakar Abid and James Y. Zou. Contrastive Variational Autoencoder Enhances Salient Features. *CoRR*, abs/1902.04601, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161-Sentence-1611,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161-Sentence-1612,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161-Sentence-1613,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161-Sentence-1614 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161-Sentence-1611 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[1] Abubakar Abid and James Y."@en ;
    askg-onto:inSentence "[1] Abubakar Abid and James Y."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-abubakar_abid,
        askg-data:Entity-james_y,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161-Sentence-1612 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Zou."@en ;
    askg-onto:inSentence "Zou."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-person,
        askg-data:Entity-zou .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161-Sentence-1613 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Contrastive Variational Autoencoder Enhances Salient Features."@en ;
    askg-onto:inSentence "Contrastive Variational Autoencoder Enhances Salient Features."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-contrastive_variational_autoencoder,
        askg-data:Entity-salient_features .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-161-Sentence-1614 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "*CoRR*, abs/1902.04601, 2019."@en ;
    askg-onto:inSentence "*CoRR*, abs/1902.04601, 2019."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2019,
        askg-data:Entity-abs190204601,
        askg-data:Entity-article,
        askg-data:Entity-corr,
        askg-data:Entity-publication,
        askg-data:Entity-year .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 10"@en ;
    domo:Text "[9] Gabriel J. Brostow Clment Godard, Oisin Mac Aodha. Unsupervised Monocular Depth Estimation with Left-Right Consistency. In *IEEE CVPR*, pages 6602–6611, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610-Sentence-16101,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610-Sentence-16102,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610-Sentence-16103,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610-Sentence-16104 ;
    askg-onto:index "10"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610-Sentence-16101 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[9] Gabriel J."@en ;
    askg-onto:inSentence "[9] Gabriel J."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gabriel_j,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610-Sentence-16102 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Brostow Clment Godard, Oisin Mac Aodha."@en ;
    askg-onto:inSentence "Brostow Clment Godard, Oisin Mac Aodha."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-brostow_clment_godard,
        askg-data:Entity-oisin_mac_aodha,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610-Sentence-16103 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Unsupervised Monocular Depth Estimation with Left-Right Consistency."@en ;
    askg-onto:inSentence "Unsupervised Monocular Depth Estimation with Left-Right Consistency."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-left-right_consistency,
        askg-data:Entity-method,
        askg-data:Entity-technique,
        askg-data:Entity-unsupervised_monocular_depth_estimation .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1610-Sentence-16104 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "In *IEEE CVPR*, pages 6602–6611, 2017."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 6602–6611, 2017."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1611 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 11"@en ;
    domo:Text "[10] Runmin Cong, Jianjun Lei, Changqing Zhang, Qingming Huang, Xiaochun Cao, and Chunping Hou. Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion. *IEEE SPL*, 23(6):819–823, 2016."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1611-Sentence-16111,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1611-Sentence-16112,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1611-Sentence-16113 ;
    askg-onto:index "11"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1611-Sentence-16111 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[10] Runmin Cong, Jianjun Lei, Changqing Zhang, Qingming Huang, Xiaochun Cao, and Chunping Hou."@en ;
    askg-onto:inSentence "[10] Runmin Cong, Jianjun Lei, Changqing Zhang, Qingming Huang, Xiaochun Cao, and Chunping Hou."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-changqing_zhang,
        askg-data:Entity-chunping_hou,
        askg-data:Entity-jianjun_lei,
        askg-data:Entity-qingming_huang,
        askg-data:Entity-runmin_cong,
        askg-data:Entity-scientist,
        askg-data:Entity-xiaochun_cao .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1611-Sentence-16112 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion."@en ;
    askg-onto:inSentence "Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth_confidence_analysis,
        askg-data:Entity-multiple_cues_fusion,
        askg-data:Entity-saliency_detection,
        askg-data:Entity-stereoscopic_images .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1611-Sentence-16113 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "*IEEE SPL*, 23(6):819–823, 2016."@en ;
    askg-onto:inSentence "*IEEE SPL*, 23(6):819–823, 2016."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2016,
        askg-data:Entity-236819823,
        askg-data:Entity-ieee_spl,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1612 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 12"@en ;
    domo:Text "[11] Dapeng Du, Limin Wang, Huiling Wang, Kai Zhao, and Gangshan Wu. Translate-to-Recognize Networks for RGB- D Scene Recognition. In *IEEE CVPR*, pages 11836–11845, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1612-Sentence-16121,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1612-Sentence-16122,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1612-Sentence-16123 ;
    askg-onto:index "12"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1612-Sentence-16121 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[11] Dapeng Du, Limin Wang, Huiling Wang, Kai Zhao, and Gangshan Wu."@en ;
    askg-onto:inSentence "[11] Dapeng Du, Limin Wang, Huiling Wang, Kai Zhao, and Gangshan Wu."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dapeng_du,
        askg-data:Entity-gangshan_wu,
        askg-data:Entity-huiling_wang,
        askg-data:Entity-kai_zhao,
        askg-data:Entity-limin_wang,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1612-Sentence-16122 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Translate-to-Recognize Networks for RGB- D Scene Recognition."@en ;
    askg-onto:inSentence "Translate-to-Recognize Networks for RGB- D Scene Recognition."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-model,
        askg-data:Entity-rgb-d_scene_recognition,
        askg-data:Entity-translate-to-recognize_networks .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1612-Sentence-16123 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, pages 11836–11845, 2019."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 11836–11845, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1613 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 13"@en ;
    domo:Text "[12] Patrick Esser, Ekaterina Sutter, and Bjrn Ommer. A Variational U-Net for Conditional Appearance and Shape Generation. In *IEEE CVPR*, pages 8857–8865, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1613-Sentence-16131,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1613-Sentence-16132,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1613-Sentence-16133 ;
    askg-onto:index "13"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1613-Sentence-16131 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[12] Patrick Esser, Ekaterina Sutter, and Bjrn Ommer."@en ;
    askg-onto:inSentence "[12] Patrick Esser, Ekaterina Sutter, and Bjrn Ommer."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-bjrn_ommer,
        askg-data:Entity-ekaterina_sutter,
        askg-data:Entity-patrick_esser,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1613-Sentence-16132 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "A Variational U-Net for Conditional Appearance and Shape Generation."@en ;
    askg-onto:inSentence "A Variational U-Net for Conditional Appearance and Shape Generation."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-model,
        askg-data:Entity-variational_u-net .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1613-Sentence-16133 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, pages 8857–8865, 2018."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 8857–8865, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1614 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 14"@en ;
    domo:Text "[13] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang- Hua Gao, Qibin Hou, and Ali Borji. Salient objects in clutter: Bringing salient object detection to the foreground. In ECCV, pages 186–202, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1614-Sentence-16141,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1614-Sentence-16142,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1614-Sentence-16143 ;
    askg-onto:index "14"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1614-Sentence-16141 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[13] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang- Hua Gao, Qibin Hou, and Ali Borji."@en ;
    askg-onto:inSentence "[13] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang- Hua Gao, Qibin Hou, and Ali Borji."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ali_borji,
        askg-data:Entity-deng-ping_fan,
        askg-data:Entity-jiang-jiang_liu,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-qibin_hou,
        askg-data:Entity-shang-hua_gao .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1614-Sentence-16142 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Salient objects in clutter: Bringing salient object detection to the foreground."@en ;
    askg-onto:inSentence "Salient objects in clutter: Bringing salient object detection to the foreground."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-foreground,
        askg-data:Entity-salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1614-Sentence-16143 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In ECCV, pages 186–202, 2018."@en ;
    askg-onto:inSentence "In ECCV, pages 186–202, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-186202,
        askg-data:Entity-eccv,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1615 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 15"@en ;
    domo:Text "[14] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji. Structure-measure: A new way to evaluate foreground maps. In *IEEE ICCV*, pages 4548–4557, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1615-Sentence-16151,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1615-Sentence-16152,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1615-Sentence-16153 ;
    askg-onto:index "15"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1615-Sentence-16151 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[14] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji."@en ;
    askg-onto:inSentence "[14] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ali_borji,
        askg-data:Entity-deng-ping_fan,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-tao_li,
        askg-data:Entity-yun_liu .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1615-Sentence-16152 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Structure-measure: A new way to evaluate foreground maps."@en ;
    askg-onto:inSentence "Structure-measure: A new way to evaluate foreground maps."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-method,
        askg-data:Entity-structure-measure .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1615-Sentence-16153 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE ICCV*, pages 4548–4557, 2017."@en ;
    askg-onto:inSentence "In *IEEE ICCV*, pages 4548–4557, 2017."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_iccv .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1616 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 16"@en ;
    domo:Text "[15] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming- Ming Cheng, and Ali Borji. Enhanced-alignment Measure for Binary Foreground Map Evaluation. In *IJCAI*, pages 698–704, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1616-Sentence-16161,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1616-Sentence-16162,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1616-Sentence-16163 ;
    askg-onto:index "16"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1616-Sentence-16161 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[15] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming- Ming Cheng, and Ali Borji."@en ;
    askg-onto:inSentence "[15] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming- Ming Cheng, and Ali Borji."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ali_borji,
        askg-data:Entity-bo_ren,
        askg-data:Entity-cheng_gong,
        askg-data:Entity-deng-ping_fan,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-yang_cao .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1616-Sentence-16162 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Enhanced-alignment Measure for Binary Foreground Map Evaluation."@en ;
    askg-onto:inSentence "Enhanced-alignment Measure for Binary Foreground Map Evaluation."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-binary_foreground_map_evaluation,
        askg-data:Entity-concept,
        askg-data:Entity-enhanced-alignment_measure,
        askg-data:Entity-measure .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1616-Sentence-16163 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IJCAI*, pages 698–704, 2018."@en ;
    askg-onto:inSentence "In *IJCAI*, pages 698–704, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ijcai,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1617 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 17"@en ;
    domo:Text "[16] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged Object Detection. In *IEEE CVPR*, 2020."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1617-Sentence-16171,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1617-Sentence-16172,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1617-Sentence-16173 ;
    askg-onto:index "17"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1617-Sentence-16171 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[16] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao."@en ;
    askg-onto:inSentence "[16] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deng-ping_fan,
        askg-data:Entity-ge-peng_ji,
        askg-data:Entity-guolei_sun,
        askg-data:Entity-jianbing_shen,
        askg-data:Entity-ling_shao,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1617-Sentence-16172 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Camouflaged Object Detection."@en ;
    askg-onto:inSentence "Camouflaged Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-camouflaged_object_detection,
        askg-data:Entity-method .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1617-Sentence-16173 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2020."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2020."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1618 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 18"@en ;
    domo:Text "[17] Deng-Ping Fan, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Huazhu Fu, and Ming-Ming Cheng. Taking a Deeper Look at the Co-salient Object Detection. In *IEEE CVPR*, 2020."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1618-Sentence-16181,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1618-Sentence-16182,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1618-Sentence-16183 ;
    askg-onto:index "18"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1618-Sentence-16181 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[17] Deng-Ping Fan, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Huazhu Fu, and Ming-Ming Cheng."@en ;
    askg-onto:inSentence "[17] Deng-Ping Fan, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Huazhu Fu, and Ming-Ming Cheng."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deng-ping_fan,
        askg-data:Entity-dingwen_zhang,
        askg-data:Entity-ge-peng_ji,
        askg-data:Entity-huazhu_fu,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-zheng_lin .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1618-Sentence-16182 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Taking a Deeper Look at the Co-salient Object Detection."@en ;
    askg-onto:inSentence "Taking a Deeper Look at the Co-salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-co-salient_object_detection,
        askg-data:Entity-method .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1618-Sentence-16183 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2020."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2020."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2020,
        askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1619 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 19"@en ;
    domo:Text "[18] Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks. IEEE TNNLS, 2020."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1619-Sentence-16191,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1619-Sentence-16192,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1619-Sentence-16193 ;
    askg-onto:index "19"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1619-Sentence-16191 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[18] Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng."@en ;
    askg-onto:inSentence "[18] Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deng-ping_fan,
        askg-data:Entity-menglong_zhu,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-zhao_zhang,
        askg-data:Entity-zheng_lin .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1619-Sentence-16192 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks."@en ;
    askg-onto:inSentence "Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-datasets,
        askg-data:Entity-large-scale_benchmarks,
        askg-data:Entity-method,
        askg-data:Entity-models,
        askg-data:Entity-rgb-d_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1619-Sentence-16193 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "IEEE TNNLS, 2020."@en ;
    askg-onto:inSentence "IEEE TNNLS, 2020."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_tnnls,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-162 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "[2] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada, and Sabine Susstrunk. Frequency-tuned salient region detection. In *IEEE CVPR*, pages 1597–1604, 2009."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-162-Sentence-1621,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-162-Sentence-1622,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-162-Sentence-1623 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-162-Sentence-1621 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[2] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada, and Sabine Susstrunk."@en ;
    askg-onto:inSentence "[2] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada, and Sabine Susstrunk."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-francisco_estrada,
        askg-data:Entity-person,
        askg-data:Entity-radhakrishna_achanta,
        askg-data:Entity-sabine_susstrunk,
        askg-data:Entity-sheila_hemami .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-162-Sentence-1622 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Frequency-tuned salient region detection."@en ;
    askg-onto:inSentence "Frequency-tuned salient region detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-frequency-tuned_salient_region_detection,
        askg-data:Entity-method .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-162-Sentence-1623 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, pages 1597–1604, 2009."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 1597–1604, 2009."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1620 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 20"@en ;
    domo:Text "[19] Deng-Ping Fan, Wenguan Wang, Ming-Ming Cheng, and Jianbing Shen. Shifting more attention to video salient object detection. In *IEEE CVPR*, pages 8554–8564, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1620-Sentence-16201,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1620-Sentence-16202,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1620-Sentence-16203 ;
    askg-onto:index "20"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1620-Sentence-16201 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[19] Deng-Ping Fan, Wenguan Wang, Ming-Ming Cheng, and Jianbing Shen."@en ;
    askg-onto:inSentence "[19] Deng-Ping Fan, Wenguan Wang, Ming-Ming Cheng, and Jianbing Shen."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deng-ping_fan,
        askg-data:Entity-jianbing_shen,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-wenguan_wang .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1620-Sentence-16202 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Shifting more attention to video salient object detection."@en ;
    askg-onto:inSentence "Shifting more attention to video salient object detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-attention,
        askg-data:Entity-video_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1620-Sentence-16203 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, pages 8554–8564, 2019."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 8554–8564, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1621 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 21"@en ;
    domo:Text "[20] David Feng, Nick Barnes, Shaodi You, and Chris McCarthy."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1621-Sentence-16211 ;
    askg-onto:index "21"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1621-Sentence-16211 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[20] David Feng, Nick Barnes, Shaodi You, and Chris McCarthy."@en ;
    askg-onto:inSentence "[20] David Feng, Nick Barnes, Shaodi You, and Chris McCarthy."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-chris_mccarthy,
        askg-data:Entity-david_feng,
        askg-data:Entity-nick_barnes,
        askg-data:Entity-person,
        askg-data:Entity-shaodi_you .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1622 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 22"@en ;
    domo:Text "Local background enclosure for RGB-D salient object detection. In *IEEE CVPR*, pages 2343–2350, 2016."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1622-Sentence-16221,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1622-Sentence-16222 ;
    askg-onto:index "22"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1622-Sentence-16221 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Local background enclosure for RGB-D salient object detection."@en ;
    askg-onto:inSentence "Local background enclosure for RGB-D salient object detection."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-local_background_enclosure,
        askg-data:Entity-rgb-d_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1622-Sentence-16222 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "In *IEEE CVPR*, pages 2343–2350, 2016."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 2343–2350, 2016."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1623 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 23"@en ;
    domo:Text "[21] Keren Fu Fu, Deng-Ping Fan, Ge-Peng Ji, and Qijun Zhao."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1623-Sentence-16231 ;
    askg-onto:index "23"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1623-Sentence-16231 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[21] Keren Fu Fu, Deng-Ping Fan, Ge-Peng Ji, and Qijun Zhao."@en ;
    askg-onto:inSentence "[21] Keren Fu Fu, Deng-Ping Fan, Ge-Peng Ji, and Qijun Zhao."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deng-ping_fan,
        askg-data:Entity-ge-peng_ji,
        askg-data:Entity-keren_fu_fu,
        askg-data:Entity-qijun_zhao,
        askg-data:Entity-scientist .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1624 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 24"@en ;
    domo:Text "JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection. In IEEE CVPR, 2020."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1624-Sentence-16241,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1624-Sentence-16242 ;
    askg-onto:index "24"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1624-Sentence-16241 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection."@en ;
    askg-onto:inSentence "JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-jl-dcf,
        askg-data:Entity-joint_learning_and_densely-cooperative_fusion_framework_for_rgb-d_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1624-Sentence-16242 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "In IEEE CVPR, 2020."@en ;
    askg-onto:inSentence "In IEEE CVPR, 2020."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1625 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 25"@en ;
    domo:Text "[22] Jingfan Guo, Tongwei Ren, and Jia Bei. Salient object detection for rgb-d image via saliency evolution. In *ICME*, pages 1–6, 2016."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1625-Sentence-16251,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1625-Sentence-16252,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1625-Sentence-16253 ;
    askg-onto:index "25"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1625-Sentence-16251 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[22] Jingfan Guo, Tongwei Ren, and Jia Bei."@en ;
    askg-onto:inSentence "[22] Jingfan Guo, Tongwei Ren, and Jia Bei."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-jia_bei,
        askg-data:Entity-jingfan_guo,
        askg-data:Entity-person,
        askg-data:Entity-tongwei_ren .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1625-Sentence-16252 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Salient object detection for rgb-d image via saliency evolution."@en ;
    askg-onto:inSentence "Salient object detection for rgb-d image via saliency evolution."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-dataset,
        askg-data:Entity-method,
        askg-data:Entity-rgb-d_image,
        askg-data:Entity-saliency_evolution,
        askg-data:Entity-salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1625-Sentence-16253 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ICME*, pages 1–6, 2016."@en ;
    askg-onto:inSentence "In *ICME*, pages 1–6, 2016."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-icme .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1626 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 26"@en ;
    domo:Text "[23] Saurabh Gupta, Ross Girshick, Pablo Arbelaez, and Jitendra ´ Malik. Learning rich features from RGB-D images for object detection and segmentation. In *ECCV*, pages 345–360, 2014."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1626-Sentence-16261,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1626-Sentence-16262,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1626-Sentence-16263 ;
    askg-onto:index "26"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1626-Sentence-16261 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[23] Saurabh Gupta, Ross Girshick, Pablo Arbelaez, and Jitendra ´ Malik."@en ;
    askg-onto:inSentence "[23] Saurabh Gupta, Ross Girshick, Pablo Arbelaez, and Jitendra ´ Malik."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-jitendra_malik,
        askg-data:Entity-pablo_arbelaez,
        askg-data:Entity-person,
        askg-data:Entity-ross_girshick,
        askg-data:Entity-saurabh_gupta .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1626-Sentence-16262 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Learning rich features from RGB-D images for object detection and segmentation."@en ;
    askg-onto:inSentence "Learning rich features from RGB-D images for object detection and segmentation."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-object_detection_and_segmentation,
        askg-data:Entity-rgb-d_images .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1626-Sentence-16263 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ECCV*, pages 345–360, 2014."@en ;
    askg-onto:inSentence "In *ECCV*, pages 345–360, 2014."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-eccv .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1627 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 27"@en ;
    domo:Text "[24] Junwei Han, Hao Chen, Nian Liu, Chenggang Yan, and Xuelong Li. CNNs-based RGB-D saliency detection via crossview transfer and multiview fusion. *IEEE TCYB*, pages 3171–3183, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1627-Sentence-16271,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1627-Sentence-16272,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1627-Sentence-16273 ;
    askg-onto:index "27"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1627-Sentence-16271 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[24] Junwei Han, Hao Chen, Nian Liu, Chenggang Yan, and Xuelong Li."@en ;
    askg-onto:inSentence "[24] Junwei Han, Hao Chen, Nian Liu, Chenggang Yan, and Xuelong Li."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-chenggang_yan,
        askg-data:Entity-hao_chen,
        askg-data:Entity-junwei_han,
        askg-data:Entity-nian_liu,
        askg-data:Entity-scientist,
        askg-data:Entity-xuelong_li .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1627-Sentence-16272 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "CNNs-based RGB-D saliency detection via crossview transfer and multiview fusion."@en ;
    askg-onto:inSentence "CNNs-based RGB-D saliency detection via crossview transfer and multiview fusion."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cnns,
        askg-data:Entity-crossview_transfer,
        askg-data:Entity-multiview_fusion,
        askg-data:Entity-rgb-d_saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1627-Sentence-16273 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "*IEEE TCYB*, pages 3171–3183, 2018."@en ;
    askg-onto:inSentence "*IEEE TCYB*, pages 3171–3183, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_tcyb,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1628 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 28"@en ;
    domo:Text "[25] Faruk Ahmed Adrien Ali Taga Francesco Visin David Vzquez Aaron C. Courville Ishaan Gulrajani, Kundan Kumar. PixelVAE: A Latent Variable Model for Natural Images."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1628-Sentence-16281,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1628-Sentence-16282,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1628-Sentence-16283 ;
    askg-onto:index "28"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1628-Sentence-16281 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[25] Faruk Ahmed Adrien Ali Taga Francesco Visin David Vzquez Aaron C."@en ;
    askg-onto:inSentence "[25] Faruk Ahmed Adrien Ali Taga Francesco Visin David Vzquez Aaron C."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-aaron_c,
        askg-data:Entity-adrien_ali_taga,
        askg-data:Entity-david_vzquez,
        askg-data:Entity-faruk_ahmed,
        askg-data:Entity-francesco_visin,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1628-Sentence-16282 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Courville Ishaan Gulrajani, Kundan Kumar."@en ;
    askg-onto:inSentence "Courville Ishaan Gulrajani, Kundan Kumar."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-courville_ishaan_gulrajani,
        askg-data:Entity-kundan_kumar,
        askg-data:Entity-scientist .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1628-Sentence-16283 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "PixelVAE: A Latent Variable Model for Natural Images."@en ;
    askg-onto:inSentence "PixelVAE: A Latent Variable Model for Natural Images."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-latent_variable_model,
        askg-data:Entity-natural_images,
        askg-data:Entity-pixelvae .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1629 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 29"@en ;
    domo:Text "In *ICLR*, 2016."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1629-Sentence-16291 ;
    askg-onto:index "29"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1629-Sentence-16291 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "In *ICLR*, 2016."@en ;
    askg-onto:inSentence "In *ICLR*, 2016."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-iclr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 3"@en ;
    domo:Text "[3] Christian F. Baumgartner, Kerem Can Tezcan, Krishna Chaitanya, Andreas M. Hotker, Urs J. Muehlematter, Khoschy ¨ Schawkat, Anton S. Becker, Olivio Donati, and Ender Konukoglu. PHiSeg: Capturing Uncertainty in Medical Image Segmentation. In *MICCAI*, pages 119–127, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1631,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1632,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1633,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1634,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1635,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1636,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1637 ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1631 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[3] Christian F."@en ;
    askg-onto:inSentence "[3] Christian F."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-christian_f,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1632 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Baumgartner, Kerem Can Tezcan, Krishna Chaitanya, Andreas M."@en ;
    askg-onto:inSentence "Baumgartner, Kerem Can Tezcan, Krishna Chaitanya, Andreas M."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-andreas_m,
        askg-data:Entity-baumgartner,
        askg-data:Entity-kerem_can_tezcan,
        askg-data:Entity-krishna_chaitanya,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1633 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Hotker, Urs J."@en ;
    askg-onto:inSentence "Hotker, Urs J."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hotker_urs_j,
        askg-data:Entity-scientist .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1634 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Muehlematter, Khoschy ¨ Schawkat, Anton S."@en ;
    askg-onto:inSentence "Muehlematter, Khoschy ¨ Schawkat, Anton S."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-anton_s,
        askg-data:Entity-khoschy,
        askg-data:Entity-muehlematter,
        askg-data:Entity-person,
        askg-data:Entity-schawkat .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1635 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "Becker, Olivio Donati, and Ender Konukoglu."@en ;
    askg-onto:inSentence "Becker, Olivio Donati, and Ender Konukoglu."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-becker,
        askg-data:Entity-ender_konukoglu,
        askg-data:Entity-olivio_donati,
        askg-data:Entity-scientist .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1636 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "PHiSeg: Capturing Uncertainty in Medical Image Segmentation."@en ;
    askg-onto:inSentence "PHiSeg: Capturing Uncertainty in Medical Image Segmentation."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-framework,
        askg-data:Entity-phiseg,
        askg-data:Entity-uncertainty_in_medical_image_segmentation .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-163-Sentence-1637 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "In *MICCAI*, pages 119–127, 2019."@en ;
    askg-onto:inSentence "In *MICCAI*, pages 119–127, 2019."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2019 .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1630 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 30"@en ;
    domo:Text "[26] Laurent Itti and Christof Koch. A saliency-based search mechanism for overt and covert shifts of visual attention. VR, 40(10):1489 - 1506, 2000."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1630-Sentence-16301,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1630-Sentence-16302,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1630-Sentence-16303 ;
    askg-onto:index "30"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1630-Sentence-16301 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[26] Laurent Itti and Christof Koch."@en ;
    askg-onto:inSentence "[26] Laurent Itti and Christof Koch."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-christof_koch,
        askg-data:Entity-laurent_itti .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1630-Sentence-16302 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "A saliency-based search mechanism for overt and covert shifts of visual attention."@en ;
    askg-onto:inSentence "A saliency-based search mechanism for overt and covert shifts of visual attention."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-method,
        askg-data:Entity-overt_and_covert_shifts_of_visual_attention,
        askg-data:Entity-saliency-based_search_mechanism .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1630-Sentence-16303 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "VR, 40(10):1489 - 1506, 2000."@en ;
    askg-onto:inSentence "VR, 40(10):1489 - 1506, 2000."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-40101489_-_1506_2000,
        askg-data:Entity-vr .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1631 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 31"@en ;
    domo:Text "[27] Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE TPAMI, 20(11):1254–1259, 1998."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1631-Sentence-16311,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1631-Sentence-16312,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1631-Sentence-16313 ;
    askg-onto:index "31"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1631-Sentence-16311 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[27] Laurent Itti, Christof Koch, and Ernst Niebur."@en ;
    askg-onto:inSentence "[27] Laurent Itti, Christof Koch, and Ernst Niebur."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-christof_koch,
        askg-data:Entity-ernst_niebur,
        askg-data:Entity-laurent_itti,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1631-Sentence-16312 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "A model of saliency-based visual attention for rapid scene analysis."@en ;
    askg-onto:inSentence "A model of saliency-based visual attention for rapid scene analysis."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-saliency-based_visual_attention,
        askg-data:Entity-scene_analysis .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1631-Sentence-16313 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "IEEE TPAMI, 20(11):1254–1259, 1998."@en ;
    askg-onto:inSentence "IEEE TPAMI, 20(11):1254–1259, 1998."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-201112541259_1998,
        askg-data:Entity-ieee_tpami .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1632 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 32"@en ;
    domo:Text "[28] Ran Ju, Ling Ge, Wenjing Geng, Tongwei Ren, and Gangshan Wu. Depth saliency based on anisotropic centersurround difference. In *ICIP*, pages 1115–1119, 2014."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1632-Sentence-16321,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1632-Sentence-16322,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1632-Sentence-16323 ;
    askg-onto:index "32"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1632-Sentence-16321 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[28] Ran Ju, Ling Ge, Wenjing Geng, Tongwei Ren, and Gangshan Wu."@en ;
    askg-onto:inSentence "[28] Ran Ju, Ling Ge, Wenjing Geng, Tongwei Ren, and Gangshan Wu."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gangshan_wu,
        askg-data:Entity-ling_ge,
        askg-data:Entity-person,
        askg-data:Entity-ran_ju,
        askg-data:Entity-tongwei_ren,
        askg-data:Entity-wenjing_geng .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1632-Sentence-16322 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Depth saliency based on anisotropic centersurround difference."@en ;
    askg-onto:inSentence "Depth saliency based on anisotropic centersurround difference."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-anisotropic_center-surround_difference,
        askg-data:Entity-depth_saliency .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1632-Sentence-16323 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ICIP*, pages 1115–1119, 2014."@en ;
    askg-onto:inSentence "In *ICIP*, pages 1115–1119, 2014."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-icip .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1633 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 33"@en ;
    domo:Text "[29] Shuhui Wang Jun Wei and Qingming Huang. F3Net: Fusion, Feedback and Focus for Salient Object Detection. In AAAI, 2020."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1633-Sentence-16331,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1633-Sentence-16332,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1633-Sentence-16333 ;
    askg-onto:index "33"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1633-Sentence-16331 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[29] Shuhui Wang Jun Wei and Qingming Huang."@en ;
    askg-onto:inSentence "[29] Shuhui Wang Jun Wei and Qingming Huang."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-jun_wei,
        askg-data:Entity-person,
        askg-data:Entity-qingming_huang,
        askg-data:Entity-shuhui_wang .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1633-Sentence-16332 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "F3Net: Fusion, Feedback and Focus for Salient Object Detection."@en ;
    askg-onto:inSentence "F3Net: Fusion, Feedback and Focus for Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-f3net,
        askg-data:Entity-model .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1633-Sentence-16333 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In AAAI, 2020."@en ;
    askg-onto:inSentence "In AAAI, 2020."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2020,
        askg-data:Entity-aaai,
        askg-data:Entity-publication,
        askg-data:Entity-year .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1634 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 34"@en ;
    domo:Text "[30] Alex Kendall, Vijay Badrinarayanan, , and Roberto Cipolla."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1634-Sentence-16341 ;
    askg-onto:index "34"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1634-Sentence-16341 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[30] Alex Kendall, Vijay Badrinarayanan, , and Roberto Cipolla."@en ;
    askg-onto:inSentence "[30] Alex Kendall, Vijay Badrinarayanan, , and Roberto Cipolla."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-alex_kendall,
        askg-data:Entity-person,
        askg-data:Entity-roberto_cipolla,
        askg-data:Entity-vijay_badrinarayanan .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1635 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 35"@en ;
    domo:Text "Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding. In BMVC, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1635-Sentence-16351,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1635-Sentence-16352 ;
    askg-onto:index "35"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1635-Sentence-16351 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding."@en ;
    askg-onto:inSentence "Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-architecture,
        askg-data:Entity-bayesian_segnet,
        askg-data:Entity-deep_convolutional_encoder-decoder_architectures,
        askg-data:Entity-model,
        askg-data:Entity-scene_understanding .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1635-Sentence-16352 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "In BMVC, 2017."@en ;
    askg-onto:inSentence "In BMVC, 2017."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2017,
        askg-data:Entity-bmvc,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1636 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 36"@en ;
    domo:Text "[31] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In *ICLR*, 2013."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1636-Sentence-16361,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1636-Sentence-16362,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1636-Sentence-16363 ;
    askg-onto:index "36"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1636-Sentence-16361 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[31] Diederik P Kingma and Max Welling."@en ;
    askg-onto:inSentence "[31] Diederik P Kingma and Max Welling."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-diederik_p_kingma,
        askg-data:Entity-max_welling,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1636-Sentence-16362 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Auto-Encoding Variational Bayes."@en ;
    askg-onto:inSentence "Auto-Encoding Variational Bayes."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-auto-encoding_variational_bayes,
        askg-data:Entity-model .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1636-Sentence-16363 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ICLR*, 2013."@en ;
    askg-onto:inSentence "In *ICLR*, 2013."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 37"@en ;
    domo:Text "[32] Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R. Ledsam, Klaus Maier-Hein, S. M. Ali Eslami, Danilo Jimenez Rezende, and Olaf Ronneberger. A Probabilistic U-Net for Segmentation of Ambiguous Images. In *NeurIPS*, pages 6965–6975, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16371,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16372,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16373,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16374,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16375,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16376 ;
    askg-onto:index "37"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16371 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[32] Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R."@en ;
    askg-onto:inSentence "[32] Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-bernardino_romera-paredes,
        askg-data:Entity-clemens_meyer,
        askg-data:Entity-jeffrey_de_fauw,
        askg-data:Entity-joseph_r,
        askg-data:Entity-person,
        askg-data:Entity-simon_kohl .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16372 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Ledsam, Klaus Maier-Hein, S."@en ;
    askg-onto:inSentence "Ledsam, Klaus Maier-Hein, S."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-klaus_maier-hein,
        askg-data:Entity-ledsam,
        askg-data:Entity-person,
        askg-data:Entity-s .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16373 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "M."@en ;
    askg-onto:inSentence "M."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-m,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16374 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Ali Eslami, Danilo Jimenez Rezende, and Olaf Ronneberger."@en ;
    askg-onto:inSentence "Ali Eslami, Danilo Jimenez Rezende, and Olaf Ronneberger."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ali_eslami,
        askg-data:Entity-danilo_jimenez_rezende,
        askg-data:Entity-olaf_ronneberger,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16375 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "A Probabilistic U-Net for Segmentation of Ambiguous Images."@en ;
    askg-onto:inSentence "A Probabilistic U-Net for Segmentation of Ambiguous Images."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-model,
        askg-data:Entity-probabilistic_u-net .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1637-Sentence-16376 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "In *NeurIPS*, pages 6965–6975, 2018."@en ;
    askg-onto:inSentence "In *NeurIPS*, pages 6965–6975, 2018."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2018 .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1638 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 38"@en ;
    domo:Text "[33] Olivier Le Meur and Thierry Baccino. Methods for comparing scanpaths and saliency maps: strengths and weaknesses. Behavior Research Methods, 45(1):251–266, 2013."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1638-Sentence-16381,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1638-Sentence-16382,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1638-Sentence-16383 ;
    askg-onto:index "38"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1638-Sentence-16381 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[33] Olivier Le Meur and Thierry Baccino."@en ;
    askg-onto:inSentence "[33] Olivier Le Meur and Thierry Baccino."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-olivier_le_meur,
        askg-data:Entity-thierry_baccino .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1638-Sentence-16382 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Methods for comparing scanpaths and saliency maps: strengths and weaknesses."@en ;
    askg-onto:inSentence "Methods for comparing scanpaths and saliency maps: strengths and weaknesses."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-saliency_maps,
        askg-data:Entity-scanpaths,
        askg-data:Entity-strengths_and_weaknesses .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1638-Sentence-16383 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Behavior Research Methods, 45(1):251–266, 2013."@en ;
    askg-onto:inSentence "Behavior Research Methods, 45(1):251–266, 2013."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-behavior_research_methods,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1639 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 39"@en ;
    domo:Text "[34] Bo Li, Zhengxing Sun, and Yuqi Guo. SuperVAE: Superpixelwise Variational Autoencoder for Salient Object Detection. In *AAAI*, pages 8569–8576, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1639-Sentence-16391,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1639-Sentence-16392,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1639-Sentence-16393 ;
    askg-onto:index "39"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1639-Sentence-16391 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[34] Bo Li, Zhengxing Sun, and Yuqi Guo."@en ;
    askg-onto:inSentence "[34] Bo Li, Zhengxing Sun, and Yuqi Guo."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-bo_li,
        askg-data:Entity-person,
        askg-data:Entity-yuqi_guo,
        askg-data:Entity-zhengxing_sun .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1639-Sentence-16392 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "SuperVAE: Superpixelwise Variational Autoencoder for Salient Object Detection."@en ;
    askg-onto:inSentence "SuperVAE: Superpixelwise Variational Autoencoder for Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-salient_object_detection,
        askg-data:Entity-superpixelwise_variational_autoencoder,
        askg-data:Entity-supervae .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1639-Sentence-16393 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *AAAI*, pages 8569–8576, 2019."@en ;
    askg-onto:inSentence "In *AAAI*, pages 8569–8576, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-aaai,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-164 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 4"@en ;
    domo:Text "[4] Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-164-Sentence-1641 ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-164-Sentence-1641 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[4] Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li."@en ;
    askg-onto:inSentence "[4] Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ali_borji,
        askg-data:Entity-huaizu_jiang,
        askg-data:Entity-jia_li,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1640 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 40"@en ;
    domo:Text "[35] Nianyi Li, Jinwei Ye, Yu Ji, Haibin Ling, and Jingyi Yu."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1640-Sentence-16401 ;
    askg-onto:index "40"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1640-Sentence-16401 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[35] Nianyi Li, Jinwei Ye, Yu Ji, Haibin Ling, and Jingyi Yu."@en ;
    askg-onto:inSentence "[35] Nianyi Li, Jinwei Ye, Yu Ji, Haibin Ling, and Jingyi Yu."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-haibin_ling,
        askg-data:Entity-jingyi_yu,
        askg-data:Entity-jinwei_ye,
        askg-data:Entity-nianyi_li,
        askg-data:Entity-scientist,
        askg-data:Entity-yu_ji .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1641 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 41"@en ;
    domo:Text "Saliency detection on light field. In *IEEE CVPR*, pages 2806–2813, 2014."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1641-Sentence-16411,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1641-Sentence-16412 ;
    askg-onto:index "41"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1641-Sentence-16411 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Saliency detection on light field."@en ;
    askg-onto:inSentence "Saliency detection on light field."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-light_field,
        askg-data:Entity-method,
        askg-data:Entity-saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1641-Sentence-16412 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "In *IEEE CVPR*, pages 2806–2813, 2014."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 2806–2813, 2014."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-28062813,
        askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1642 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 42"@en ;
    domo:Text "[36] Fangfang Liang, Lijuan Duan, Wei Ma, Yuanhua Qiao, Zhi Cai, and Laiyun Qing. Stereoscopic saliency model using contrast and depth-guided-background prior. Neurocomputing, 275:2227–2238, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1642-Sentence-16421,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1642-Sentence-16422,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1642-Sentence-16423 ;
    askg-onto:index "42"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1642-Sentence-16421 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[36] Fangfang Liang, Lijuan Duan, Wei Ma, Yuanhua Qiao, Zhi Cai, and Laiyun Qing."@en ;
    askg-onto:inSentence "[36] Fangfang Liang, Lijuan Duan, Wei Ma, Yuanhua Qiao, Zhi Cai, and Laiyun Qing."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-fangfang_liang,
        askg-data:Entity-laiyun_qing,
        askg-data:Entity-lijuan_duan,
        askg-data:Entity-scientist,
        askg-data:Entity-wei_ma,
        askg-data:Entity-yuanhua_qiao,
        askg-data:Entity-zhi_cai .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1642-Sentence-16422 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Stereoscopic saliency model using contrast and depth-guided-background prior."@en ;
    askg-onto:inSentence "Stereoscopic saliency model using contrast and depth-guided-background prior."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-contrast_and_depth-guided-background_prior,
        askg-data:Entity-stereoscopic_saliency_model .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1642-Sentence-16423 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Neurocomputing, 275:2227–2238, 2018."@en ;
    askg-onto:inSentence "Neurocomputing, 275:2227–2238, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2018,
        askg-data:Entity-22272238,
        askg-data:Entity-275,
        askg-data:Entity-neurocomputing,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1643 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 43"@en ;
    domo:Text "[37] Zheng Lin, Zhao Zhang, Lin-Zhuo Chen, Ming-Ming Cheng, and Shao-Ping Lu. Interactive Image Segmentation with First Click Attention. In *IEEE CVPR*, 2020."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1643-Sentence-16431,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1643-Sentence-16432,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1643-Sentence-16433 ;
    askg-onto:index "43"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1643-Sentence-16431 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[37] Zheng Lin, Zhao Zhang, Lin-Zhuo Chen, Ming-Ming Cheng, and Shao-Ping Lu."@en ;
    askg-onto:inSentence "[37] Zheng Lin, Zhao Zhang, Lin-Zhuo Chen, Ming-Ming Cheng, and Shao-Ping Lu."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-lin-zhuo_chen,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-shao-ping_lu,
        askg-data:Entity-zhao_zhang,
        askg-data:Entity-zheng_lin .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1643-Sentence-16432 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Interactive Image Segmentation with First Click Attention."@en ;
    askg-onto:inSentence "Interactive Image Segmentation with First Click Attention."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-first_click_attention,
        askg-data:Entity-interactive_image_segmentation,
        askg-data:Entity-method,
        askg-data:Entity-technique .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1643-Sentence-16433 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2020."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2020."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1644 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 44"@en ;
    domo:Text "[38] Yi Liu, Qiang Zhang, Dingwen Zhang, and Jungong Han."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1644-Sentence-16441 ;
    askg-onto:index "44"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1644-Sentence-16441 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[38] Yi Liu, Qiang Zhang, Dingwen Zhang, and Jungong Han."@en ;
    askg-onto:inSentence "[38] Yi Liu, Qiang Zhang, Dingwen Zhang, and Jungong Han."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dingwen_zhang,
        askg-data:Entity-jungong_han,
        askg-data:Entity-person,
        askg-data:Entity-qiang_zhang,
        askg-data:Entity-yi_liu .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1645 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 45"@en ;
    domo:Text "Employing Deep Part-Object Relationships for Salient Object Detection. In *IEEE ICCV*, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1645-Sentence-16451,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1645-Sentence-16452 ;
    askg-onto:index "45"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1645-Sentence-16451 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Employing Deep Part-Object Relationships for Salient Object Detection."@en ;
    askg-onto:inSentence "Employing Deep Part-Object Relationships for Salient Object Detection."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deep_part-object_relationships,
        askg-data:Entity-salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1645-Sentence-16452 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "In *IEEE ICCV*, 2019."@en ;
    askg-onto:inSentence "In *IEEE ICCV*, 2019."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_iccv,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1646 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 46"@en ;
    domo:Text "[39] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-Local Deep Features for Salient Object Detection. In *IEEE CVPR*, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1646-Sentence-16461,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1646-Sentence-16462,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1646-Sentence-16463 ;
    askg-onto:index "46"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1646-Sentence-16461 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[39] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin."@en ;
    askg-onto:inSentence "[39] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-akshaya_mishra,
        askg-data:Entity-andrew_achkar,
        askg-data:Entity-justin_eichel,
        askg-data:Entity-person,
        askg-data:Entity-pierre-marc_jodoin,
        askg-data:Entity-shaozi_li,
        askg-data:Entity-zhiming_luo .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1646-Sentence-16462 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Non-Local Deep Features for Salient Object Detection."@en ;
    askg-onto:inSentence "Non-Local Deep Features for Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-non-local_deep_features,
        askg-data:Entity-salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1646-Sentence-16463 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2017."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2017."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2017,
        askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1647 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 47"@en ;
    domo:Text "[40] Yuzhen Niu, Yujie Geng, Xueqing Li, and Feng Liu. Leveraging stereopsis for saliency analysis. In *IEEE CVPR*, pages 454–461, 2012."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1647-Sentence-16471,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1647-Sentence-16472,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1647-Sentence-16473 ;
    askg-onto:index "47"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1647-Sentence-16471 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[40] Yuzhen Niu, Yujie Geng, Xueqing Li, and Feng Liu."@en ;
    askg-onto:inSentence "[40] Yuzhen Niu, Yujie Geng, Xueqing Li, and Feng Liu."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-feng_liu,
        askg-data:Entity-person,
        askg-data:Entity-xueqing_li,
        askg-data:Entity-yujie_geng,
        askg-data:Entity-yuzhen_niu .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1647-Sentence-16472 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Leveraging stereopsis for saliency analysis."@en ;
    askg-onto:inSentence "Leveraging stereopsis for saliency analysis."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-saliency_analysis,
        askg-data:Entity-stereopsis .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1647-Sentence-16473 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, pages 454–461, 2012."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 454–461, 2012."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1648 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 48"@en ;
    domo:Text "[41] Houwen Peng, Bing Li, Weihua Xiong, Weiming Hu, and Rongrong Ji. Rgbd salient object detection: a benchmark and algorithms. In *ECCV*, pages 92–109, 2014."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1648-Sentence-16481,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1648-Sentence-16482,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1648-Sentence-16483 ;
    askg-onto:index "48"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1648-Sentence-16481 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[41] Houwen Peng, Bing Li, Weihua Xiong, Weiming Hu, and Rongrong Ji."@en ;
    askg-onto:inSentence "[41] Houwen Peng, Bing Li, Weihua Xiong, Weiming Hu, and Rongrong Ji."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-bing_li,
        askg-data:Entity-houwen_peng,
        askg-data:Entity-person,
        askg-data:Entity-rongrong_ji,
        askg-data:Entity-weihua_xiong,
        askg-data:Entity-weiming_hu .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1648-Sentence-16482 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Rgbd salient object detection: a benchmark and algorithms."@en ;
    askg-onto:inSentence "Rgbd salient object detection: a benchmark and algorithms."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-algorithms,
        askg-data:Entity-benchmark,
        askg-data:Entity-rgbd_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1648-Sentence-16483 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ECCV*, pages 92–109, 2014."@en ;
    askg-onto:inSentence "In *ECCV*, pages 92–109, 2014."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2014,
        askg-data:Entity-92-109,
        askg-data:Entity-eccv,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1649 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 49"@en ;
    domo:Text "[42] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. BASNet: Boundary-Aware Salient Object Detection. In *IEEE CVPR*, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1649-Sentence-16491,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1649-Sentence-16492,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1649-Sentence-16493 ;
    askg-onto:index "49"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1649-Sentence-16491 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[42] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand."@en ;
    askg-onto:inSentence "[42] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-chao_gao,
        askg-data:Entity-chenyang_huang,
        askg-data:Entity-martin_jagersand,
        askg-data:Entity-masood_dehghan,
        askg-data:Entity-person,
        askg-data:Entity-xuebin_qin,
        askg-data:Entity-zichen_zhang .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1649-Sentence-16492 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "BASNet: Boundary-Aware Salient Object Detection."@en ;
    askg-onto:inSentence "BASNet: Boundary-Aware Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-basnet,
        askg-data:Entity-boundary-aware_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1649-Sentence-16493 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2019."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-165 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 5"@en ;
    domo:Text "Salient Object Detection: A Benchmark. *IEEE TIP*, 24(12):5706–5722, 2015."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-165-Sentence-1651,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-165-Sentence-1652 ;
    askg-onto:index "5"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-165-Sentence-1651 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Salient Object Detection: A Benchmark."@en ;
    askg-onto:inSentence "Salient Object Detection: A Benchmark."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-benchmark,
        askg-data:Entity-salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-165-Sentence-1652 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "*IEEE TIP*, 24(12):5706–5722, 2015."@en ;
    askg-onto:inSentence "*IEEE TIP*, 24(12):5706–5722, 2015."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-12,
        askg-data:Entity-24,
        askg-data:Entity-57065722,
        askg-data:Entity-ieee_tip,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1650 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 50"@en ;
    domo:Text "[43] Liangqiong Qu, Shengfeng He, Jiawei Zhang, Jiandong Tian, Yandong Tang, and Qingxiong Yang. RGBD salient object detection via deep fusion. *IEEE TIP*, 26(5):2274– 2285, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1650-Sentence-16501,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1650-Sentence-16502,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1650-Sentence-16503 ;
    askg-onto:index "50"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1650-Sentence-16501 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[43] Liangqiong Qu, Shengfeng He, Jiawei Zhang, Jiandong Tian, Yandong Tang, and Qingxiong Yang."@en ;
    askg-onto:inSentence "[43] Liangqiong Qu, Shengfeng He, Jiawei Zhang, Jiandong Tian, Yandong Tang, and Qingxiong Yang."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-jiandong_tian,
        askg-data:Entity-jiawei_zhang,
        askg-data:Entity-liangqiong_qu,
        askg-data:Entity-person,
        askg-data:Entity-qingxiong_yang,
        askg-data:Entity-shengfeng_he,
        askg-data:Entity-yandong_tang .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1650-Sentence-16502 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "RGBD salient object detection via deep fusion."@en ;
    askg-onto:inSentence "RGBD salient object detection via deep fusion."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deep_fusion,
        askg-data:Entity-method,
        askg-data:Entity-rgbd_salient_object_detection,
        askg-data:Entity-technique .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1650-Sentence-16503 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "*IEEE TIP*, 26(5):2274– 2285, 2017."@en ;
    askg-onto:inSentence "*IEEE TIP*, 26(5):2274– 2285, 2017."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2017,
        askg-data:Entity-22742285,
        askg-data:Entity-26,
        askg-data:Entity-5,
        askg-data:Entity-ieee_tip,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1651 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 51"@en ;
    domo:Text "[44] Jianqiang Ren, Xiaojin Gong, Lu Yu, Wenhui Zhou, and Michael Ying Yang. Exploiting Global Priors for RGB-D Saliency Detection. In *IEEE CVPRW*, pages 25–32, 2015."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1651-Sentence-16511,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1651-Sentence-16512,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1651-Sentence-16513 ;
    askg-onto:index "51"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1651-Sentence-16511 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[44] Jianqiang Ren, Xiaojin Gong, Lu Yu, Wenhui Zhou, and Michael Ying Yang."@en ;
    askg-onto:inSentence "[44] Jianqiang Ren, Xiaojin Gong, Lu Yu, Wenhui Zhou, and Michael Ying Yang."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-jianqiang_ren,
        askg-data:Entity-lu_yu,
        askg-data:Entity-michael_ying_yang,
        askg-data:Entity-person,
        askg-data:Entity-wenhui_zhou,
        askg-data:Entity-xiaojin_gong .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1651-Sentence-16512 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Exploiting Global Priors for RGB-D Saliency Detection."@en ;
    askg-onto:inSentence "Exploiting Global Priors for RGB-D Saliency Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-exploiting_global_priors,
        askg-data:Entity-method,
        askg-data:Entity-rgb-d_saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1651-Sentence-16513 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPRW*, pages 25–32, 2015."@en ;
    askg-onto:inSentence "In *IEEE CVPRW*, pages 25–32, 2015."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvprw .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1652 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 52"@en ;
    domo:Text "[45] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In *ICML*, pages 1278–1286, 2014."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1652-Sentence-16521,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1652-Sentence-16522,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1652-Sentence-16523 ;
    askg-onto:index "52"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1652-Sentence-16521 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[45] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra."@en ;
    askg-onto:inSentence "[45] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-daan_wierstra,
        askg-data:Entity-danilo_jimenez_rezende,
        askg-data:Entity-person,
        askg-data:Entity-shakir_mohamed .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1652-Sentence-16522 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Stochastic Backpropagation and Approximate Inference in Deep Generative Models."@en ;
    askg-onto:inSentence "Stochastic Backpropagation and Approximate Inference in Deep Generative Models."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-approximate_inference,
        askg-data:Entity-deep_generative_models,
        askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-stochastic_backpropagation .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1652-Sentence-16523 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ICML*, pages 1278–1286, 2014."@en ;
    askg-onto:inSentence "In *ICML*, pages 1278–1286, 2014."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-icml .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 53"@en ;
    domo:Text "[46] Christian Rupprecht, Iro Laina, Maximilian Baust, Federico Tombari, Gregory D. Hager, and Nassir Navab. Learning in an Uncertain World: Representing Ambiguity Through Multiple Hypotheses. In *IEEE ICCV*, pages 3611–3620, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653-Sentence-16531,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653-Sentence-16532,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653-Sentence-16533,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653-Sentence-16534 ;
    askg-onto:index "53"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653-Sentence-16531 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[46] Christian Rupprecht, Iro Laina, Maximilian Baust, Federico Tombari, Gregory D."@en ;
    askg-onto:inSentence "[46] Christian Rupprecht, Iro Laina, Maximilian Baust, Federico Tombari, Gregory D."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-christian_rupprecht,
        askg-data:Entity-federico_tombari,
        askg-data:Entity-gregory_d,
        askg-data:Entity-iro_laina,
        askg-data:Entity-maximilian_baust,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653-Sentence-16532 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Hager, and Nassir Navab."@en ;
    askg-onto:inSentence "Hager, and Nassir Navab."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hager,
        askg-data:Entity-nassir_navab,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653-Sentence-16533 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Learning in an Uncertain World: Representing Ambiguity Through Multiple Hypotheses."@en ;
    askg-onto:inSentence "Learning in an Uncertain World: Representing Ambiguity Through Multiple Hypotheses."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ambiguity,
        askg-data:Entity-learning_in_an_uncertain_world,
        askg-data:Entity-multiple_hypotheses,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1653-Sentence-16534 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "In *IEEE ICCV*, pages 3611–3620, 2017."@en ;
    askg-onto:inSentence "In *IEEE ICCV*, pages 3611–3620, 2017."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_iccv .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1654 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 54"@en ;
    domo:Text "[47] Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, Stephen Gould, and Amirhossein Habibian. Learning Variations in Human Motion via Mix-and-Match Perturbation. *arXiv e-prints*, page arXiv:1908.00733, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1654-Sentence-16541,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1654-Sentence-16542,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1654-Sentence-16543 ;
    askg-onto:index "54"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1654-Sentence-16541 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[47] Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, Stephen Gould, and Amirhossein Habibian."@en ;
    askg-onto:inSentence "[47] Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, Stephen Gould, and Amirhossein Habibian."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-amirhossein_habibian,
        askg-data:Entity-fatemeh_sadat_saleh,
        askg-data:Entity-lars_petersson,
        askg-data:Entity-mathieu_salzmann,
        askg-data:Entity-mohammad_sadegh_aliakbarian,
        askg-data:Entity-person,
        askg-data:Entity-stephen_gould .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1654-Sentence-16542 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Learning Variations in Human Motion via Mix-and-Match Perturbation."@en ;
    askg-onto:inSentence "Learning Variations in Human Motion via Mix-and-Match Perturbation."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-learning_variations_in_human_motion,
        askg-data:Entity-mix-and-match_perturbation .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1654-Sentence-16543 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "*arXiv e-prints*, page arXiv:1908.00733, 2019."@en ;
    askg-onto:inSentence "*arXiv e-prints*, page arXiv:1908.00733, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2019,
        askg-data:Entity-article,
        askg-data:Entity-arxiv190800733,
        askg-data:Entity-arxiv_e-prints,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1655 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 55"@en ;
    domo:Text "[48] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In ICLR, 2014."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1655-Sentence-16551,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1655-Sentence-16552,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1655-Sentence-16553 ;
    askg-onto:index "55"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1655-Sentence-16551 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[48] Karen Simonyan and Andrew Zisserman."@en ;
    askg-onto:inSentence "[48] Karen Simonyan and Andrew Zisserman."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-andrew_zisserman,
        askg-data:Entity-karen_simonyan .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1655-Sentence-16552 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Very Deep Convolutional Networks for Large-Scale Image Recognition."@en ;
    askg-onto:inSentence "Very Deep Convolutional Networks for Large-Scale Image Recognition."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-large-scale_image_recognition,
        askg-data:Entity-model,
        askg-data:Entity-very_deep_convolutional_networks .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1655-Sentence-16553 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In ICLR, 2014."@en ;
    askg-onto:inSentence "In ICLR, 2014."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2014,
        askg-data:Entity-iclr .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1656 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 56"@en ;
    domo:Text "[49] Krishna Kumar Singh and Yong Jae Lee. Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization. In *IEEE ICCV*, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1656-Sentence-16561,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1656-Sentence-16562,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1656-Sentence-16563 ;
    askg-onto:index "56"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1656-Sentence-16561 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[49] Krishna Kumar Singh and Yong Jae Lee."@en ;
    askg-onto:inSentence "[49] Krishna Kumar Singh and Yong Jae Lee."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-krishna_kumar_singh,
        askg-data:Entity-yong_jae_lee .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1656-Sentence-16562 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization."@en ;
    askg-onto:inSentence "Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-hide-and-seek,
        askg-data:Entity-method,
        askg-data:Entity-weakly-supervised_object_and_action_localization .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1656-Sentence-16563 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE ICCV*, 2017."@en ;
    askg-onto:inSentence "In *IEEE ICCV*, 2017."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2017,
        askg-data:Entity-ieee_iccv,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1657 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 57"@en ;
    domo:Text "[50] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning Structured Output Representation using Deep Conditional Generative Models. In *NeurIPS*, pages 3483–3491, 2015."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1657-Sentence-16571,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1657-Sentence-16572,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1657-Sentence-16573 ;
    askg-onto:index "57"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1657-Sentence-16571 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[50] Kihyuk Sohn, Honglak Lee, and Xinchen Yan."@en ;
    askg-onto:inSentence "[50] Kihyuk Sohn, Honglak Lee, and Xinchen Yan."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-honglak_lee,
        askg-data:Entity-kihyuk_sohn,
        askg-data:Entity-person,
        askg-data:Entity-xinchen_yan .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1657-Sentence-16572 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Learning Structured Output Representation using Deep Conditional Generative Models."@en ;
    askg-onto:inSentence "Learning Structured Output Representation using Deep Conditional Generative Models."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deep_conditional_generative_models,
        askg-data:Entity-learning_structured_output_representation .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1657-Sentence-16573 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *NeurIPS*, pages 3483–3491, 2015."@en ;
    askg-onto:inSentence "In *NeurIPS*, pages 3483–3491, 2015."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-neurips .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1658 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 58"@en ;
    domo:Text "[51] Hangke Song, Zhi Liu, Huan Du, Guangling Sun, Olivier Le Meur, and Tongwei Ren. Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning. *IEEE TIP*, 26(9):4204–4216, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1658-Sentence-16581,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1658-Sentence-16582,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1658-Sentence-16583 ;
    askg-onto:index "58"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1658-Sentence-16581 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[51] Hangke Song, Zhi Liu, Huan Du, Guangling Sun, Olivier Le Meur, and Tongwei Ren."@en ;
    askg-onto:inSentence "[51] Hangke Song, Zhi Liu, Huan Du, Guangling Sun, Olivier Le Meur, and Tongwei Ren."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-guangling_sun,
        askg-data:Entity-hangke_song,
        askg-data:Entity-huan_du,
        askg-data:Entity-olivier_le_meur,
        askg-data:Entity-person,
        askg-data:Entity-tongwei_ren,
        askg-data:Entity-zhi_liu .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1658-Sentence-16582 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning."@en ;
    askg-onto:inSentence "Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-bootstrap_learning,
        askg-data:Entity-depth-aware_salient_object_detection_and_segmentation,
        askg-data:Entity-method,
        askg-data:Entity-multiscale_discriminative_saliency_fusion,
        askg-data:Entity-technique .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1658-Sentence-16583 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "*IEEE TIP*, 26(9):4204–4216, 2017."@en ;
    askg-onto:inSentence "*IEEE TIP*, 26(9):4204–4216, 2017."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-26,
        askg-data:Entity-42044216,
        askg-data:Entity-9,
        askg-data:Entity-ieee_tip,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1659 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 59"@en ;
    domo:Text "[52] Qingyang Tan, Lin Gao, Yu-Kun Lai, and Shihong Xia. Variational Autoencoders for Deforming 3D Mesh Models. In IEEE CVPR, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1659-Sentence-16591,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1659-Sentence-16592,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1659-Sentence-16593 ;
    askg-onto:index "59"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1659-Sentence-16591 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[52] Qingyang Tan, Lin Gao, Yu-Kun Lai, and Shihong Xia."@en ;
    askg-onto:inSentence "[52] Qingyang Tan, Lin Gao, Yu-Kun Lai, and Shihong Xia."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-lin_gao,
        askg-data:Entity-qingyang_tan,
        askg-data:Entity-scientist,
        askg-data:Entity-shihong_xia,
        askg-data:Entity-yu-kun_lai .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1659-Sentence-16592 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Variational Autoencoders for Deforming 3D Mesh Models."@en ;
    askg-onto:inSentence "Variational Autoencoders for Deforming 3D Mesh Models."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deforming_3d_mesh_models,
        askg-data:Entity-variational_autoencoders .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1659-Sentence-16593 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In IEEE CVPR, 2018."@en ;
    askg-onto:inSentence "In IEEE CVPR, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2018,
        askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-166 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 6"@en ;
    domo:Text "[5] Hao Chen and Youfu Li. Progressively complementarityaware fusion network for RGB-D Salient Object Detection. In *IEEE CVPR*, pages 3051–3060, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-166-Sentence-1661,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-166-Sentence-1662,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-166-Sentence-1663 ;
    askg-onto:index "6"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-166-Sentence-1661 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[5] Hao Chen and Youfu Li."@en ;
    askg-onto:inSentence "[5] Hao Chen and Youfu Li."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hao_chen,
        askg-data:Entity-person,
        askg-data:Entity-youfu_li .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-166-Sentence-1662 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Progressively complementarityaware fusion network for RGB-D Salient Object Detection."@en ;
    askg-onto:inSentence "Progressively complementarityaware fusion network for RGB-D Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-progressively_complementarityaware_fusion_network,
        askg-data:Entity-rgb-d_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-166-Sentence-1663 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, pages 3051–3060, 2018."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 3051–3060, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1660 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 60"@en ;
    domo:Text "[53] Jacob Walker, Carl Doersch, Harikrishna Mulam, and Martial Hebert. An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders. In *ECCVW*, pages 835–851, 2016."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1660-Sentence-16601,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1660-Sentence-16602,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1660-Sentence-16603 ;
    askg-onto:index "60"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1660-Sentence-16601 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[53] Jacob Walker, Carl Doersch, Harikrishna Mulam, and Martial Hebert."@en ;
    askg-onto:inSentence "[53] Jacob Walker, Carl Doersch, Harikrishna Mulam, and Martial Hebert."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-carl_doersch,
        askg-data:Entity-harikrishna_mulam,
        askg-data:Entity-jacob_walker,
        askg-data:Entity-martial_hebert,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1660-Sentence-16602 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders."@en ;
    askg-onto:inSentence "An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-an_uncertain_future,
        askg-data:Entity-forecasting,
        askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-publication,
        askg-data:Entity-static_images,
        askg-data:Entity-variational_autoencoders .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1660-Sentence-16603 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ECCVW*, pages 835–851, 2016."@en ;
    askg-onto:inSentence "In *ECCVW*, pages 835–851, 2016."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-eccvw .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1661 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 61"@en ;
    domo:Text "[54] Ningning Wang and Xiaojin Gong. Adaptive Fusion for RGB-D Salient Object Detection. *IEEE Access*, 7:55277– 55284, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1661-Sentence-16611,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1661-Sentence-16612,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1661-Sentence-16613 ;
    askg-onto:index "61"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1661-Sentence-16611 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[54] Ningning Wang and Xiaojin Gong."@en ;
    askg-onto:inSentence "[54] Ningning Wang and Xiaojin Gong."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ningning_wang,
        askg-data:Entity-person,
        askg-data:Entity-xiaojin_gong .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1661-Sentence-16612 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Adaptive Fusion for RGB-D Salient Object Detection."@en ;
    askg-onto:inSentence "Adaptive Fusion for RGB-D Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-adaptive_fusion,
        askg-data:Entity-rgb-d_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1661-Sentence-16613 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "*IEEE Access*, 7:55277– 55284, 2019."@en ;
    askg-onto:inSentence "*IEEE Access*, 7:55277– 55284, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2019,
        askg-data:Entity-5527755284,
        askg-data:Entity-7,
        askg-data:Entity-ieee_access,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1662 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 62"@en ;
    domo:Text "[55] Wenguan Wang, Jianbing Shen, Ming-Ming Cheng, and Ling Shao. An Iterative and Cooperative Top-Down and Bottom-Up Inference Network for Salient Object Detection. In *IEEE CVPR*, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1662-Sentence-16621,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1662-Sentence-16622,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1662-Sentence-16623 ;
    askg-onto:index "62"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1662-Sentence-16621 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[55] Wenguan Wang, Jianbing Shen, Ming-Ming Cheng, and Ling Shao."@en ;
    askg-onto:inSentence "[55] Wenguan Wang, Jianbing Shen, Ming-Ming Cheng, and Ling Shao."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-jianbing_shen,
        askg-data:Entity-ling_shao,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-wenguan_wang .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1662-Sentence-16622 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "An Iterative and Cooperative Top-Down and Bottom-Up Inference Network for Salient Object Detection."@en ;
    askg-onto:inSentence "An Iterative and Cooperative Top-Down and Bottom-Up Inference Network for Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-iterative_and_cooperative_top-down_and_bottom-up_inference_network,
        askg-data:Entity-salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1662-Sentence-16623 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2019."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1663 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 63"@en ;
    domo:Text "[56] Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, and Wei Xu. Occlusion Aware Unsupervised Learning of Optical Flow. In *IEEE CVPR*, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1663-Sentence-16631,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1663-Sentence-16632,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1663-Sentence-16633 ;
    askg-onto:index "63"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1663-Sentence-16631 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[56] Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, and Wei Xu."@en ;
    askg-onto:inSentence "[56] Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, and Wei Xu."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-liang_zhao,
        askg-data:Entity-peng_wang,
        askg-data:Entity-person,
        askg-data:Entity-wei_xu,
        askg-data:Entity-yang_wang,
        askg-data:Entity-yi_yang,
        askg-data:Entity-zhenheng_yang .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1663-Sentence-16632 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Occlusion Aware Unsupervised Learning of Optical Flow."@en ;
    askg-onto:inSentence "Occlusion Aware Unsupervised Learning of Optical Flow."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-method,
        askg-data:Entity-occlusion_aware_unsupervised_learning_of_optical_flow .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1663-Sentence-16633 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2018."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1664 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 64"@en ;
    domo:Text "[57] Rastogi Akash Villegas Ruben Sunkavalli Kalyan Shechtman Eli Hadap Sunil Yumer Ersin Lee Honglak Yan, Xinchen. MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics. In *ECCV*, pages 276–293, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1664-Sentence-16641,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1664-Sentence-16642,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1664-Sentence-16643 ;
    askg-onto:index "64"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1664-Sentence-16641 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[57] Rastogi Akash Villegas Ruben Sunkavalli Kalyan Shechtman Eli Hadap Sunil Yumer Ersin Lee Honglak Yan, Xinchen."@en ;
    askg-onto:inSentence "[57] Rastogi Akash Villegas Ruben Sunkavalli Kalyan Shechtman Eli Hadap Sunil Yumer Ersin Lee Honglak Yan, Xinchen."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hadap_sunil,
        askg-data:Entity-lee_honglak,
        askg-data:Entity-person,
        askg-data:Entity-rastogi_akash,
        askg-data:Entity-shechtman_eli,
        askg-data:Entity-sunkavalli_kalyan,
        askg-data:Entity-villegas_ruben,
        askg-data:Entity-yan_xinchen,
        askg-data:Entity-yumer_ersin .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1664-Sentence-16642 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics."@en ;
    askg-onto:inSentence "MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-model,
        askg-data:Entity-mt-vae .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1664-Sentence-16643 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ECCV*, pages 276–293, 2018."@en ;
    askg-onto:inSentence "In *ECCV*, pages 276–293, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-eccv .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1665 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 65"@en ;
    domo:Text "[58] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang. DenseASPP for Semantic Segmentation in Street Scenes. In *IEEE CVPR*, pages 3684–3692, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1665-Sentence-16651,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1665-Sentence-16652,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1665-Sentence-16653 ;
    askg-onto:index "65"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1665-Sentence-16651 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[58] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang."@en ;
    askg-onto:inSentence "[58] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-chi_zhang,
        askg-data:Entity-kuiyuan_yang,
        askg-data:Entity-kun_yu,
        askg-data:Entity-maoke_yang,
        askg-data:Entity-person,
        askg-data:Entity-zhiwei_li .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1665-Sentence-16652 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "DenseASPP for Semantic Segmentation in Street Scenes."@en ;
    askg-onto:inSentence "DenseASPP for Semantic Segmentation in Street Scenes."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-denseaspp,
        askg-data:Entity-framework,
        askg-data:Entity-semantic_segmentation,
        askg-data:Entity-street_scenes .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1665-Sentence-16653 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, pages 3684–3692, 2018."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, pages 3684–3692, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1666 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 66"@en ;
    domo:Text "[59] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1666-Sentence-16661 ;
    askg-onto:index "66"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1666-Sentence-16661 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[59] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J."@en ;
    askg-onto:inSentence "[59] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-he_wang,
        askg-data:Entity-leonidas_j,
        askg-data:Entity-li_yi,
        askg-data:Entity-minhyuk_sung,
        askg-data:Entity-person,
        askg-data:Entity-wang_zhao .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1667 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 67"@en ;
    domo:Text "Guibas. GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud. In *IEEE CVPR*, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1667-Sentence-16671,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1667-Sentence-16672,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1667-Sentence-16673 ;
    askg-onto:index "67"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1667-Sentence-16671 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Guibas."@en ;
    askg-onto:inSentence "Guibas."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-guibas,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1667-Sentence-16672 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud."@en ;
    askg-onto:inSentence "GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-3d_instance_segmentation,
        askg-data:Entity-generative_shape_proposal_network,
        askg-data:Entity-gspn,
        askg-data:Entity-point_cloud .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1667-Sentence-16673 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2019."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 68"@en ;
    domo:Text "[60] Shivanthan A. C. Yohanandan, Adrian G. Dyer, Dacheng Tao, and Andy Song. Saliency Preservation in Low- Resolution Grayscale Images. In *ECCV*, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16681,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16682,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16683,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16684,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16685,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16686 ;
    askg-onto:index "68"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16681 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[60] Shivanthan A."@en ;
    askg-onto:inSentence "[60] Shivanthan A."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-person,
        askg-data:Entity-shivanthan_a .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16682 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "C."@en ;
    askg-onto:inSentence "C."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-c .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16683 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Yohanandan, Adrian G."@en ;
    askg-onto:inSentence "Yohanandan, Adrian G."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-person,
        askg-data:Entity-yohanandan_adrian_g .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16684 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Dyer, Dacheng Tao, and Andy Song."@en ;
    askg-onto:inSentence "Dyer, Dacheng Tao, and Andy Song."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-andy_song,
        askg-data:Entity-dacheng_tao,
        askg-data:Entity-dyer,
        askg-data:Entity-person .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16685 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "Saliency Preservation in Low- Resolution Grayscale Images."@en ;
    askg-onto:inSentence "Saliency Preservation in Low- Resolution Grayscale Images."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-low-resolution_grayscale_images,
        askg-data:Entity-method,
        askg-data:Entity-saliency_preservation .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1668-Sentence-16686 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "In *ECCV*, 2018."@en ;
    askg-onto:inSentence "In *ECCV*, 2018."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-eccv .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1669 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 69"@en ;
    domo:Text "[61] Jingjing Li Miao Zhang Huchuan Lu Yongri Piao, Wei Ji."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1669-Sentence-16691 ;
    askg-onto:index "69"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1669-Sentence-16691 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[61] Jingjing Li Miao Zhang Huchuan Lu Yongri Piao, Wei Ji."@en ;
    askg-onto:inSentence "[61] Jingjing Li Miao Zhang Huchuan Lu Yongri Piao, Wei Ji."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-huchuan_lu,
        askg-data:Entity-jingjing_li,
        askg-data:Entity-miao_zhang,
        askg-data:Entity-person,
        askg-data:Entity-wei_ji,
        askg-data:Entity-yongri_piao .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-167 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 7"@en ;
    domo:Text "[6] Hao Chen and Youfu Li. Three-stream Attention-aware Network for RGB-D Salient Object Detection. *IEEE TIP*, pages 2825–2835, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-167-Sentence-1671,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-167-Sentence-1672,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-167-Sentence-1673 ;
    askg-onto:index "7"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-167-Sentence-1671 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[6] Hao Chen and Youfu Li."@en ;
    askg-onto:inSentence "[6] Hao Chen and Youfu Li."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hao_chen,
        askg-data:Entity-person,
        askg-data:Entity-youfu_li .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-167-Sentence-1672 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Three-stream Attention-aware Network for RGB-D Salient Object Detection."@en ;
    askg-onto:inSentence "Three-stream Attention-aware Network for RGB-D Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-model,
        askg-data:Entity-rgb-d_salient_object_detection,
        askg-data:Entity-three-stream_attention-aware_network .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-167-Sentence-1673 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "*IEEE TIP*, pages 2825–2835, 2019."@en ;
    askg-onto:inSentence "*IEEE TIP*, pages 2825–2835, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_tip,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1670 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 70"@en ;
    domo:Text "Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection. In *IEEE ICCV*, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1670-Sentence-16701,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1670-Sentence-16702 ;
    askg-onto:index "70"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1670-Sentence-16701 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection."@en ;
    askg-onto:inSentence "Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-concept,
        askg-data:Entity-depth-induced_multi-scale_recurrent_attention_network,
        askg-data:Entity-model,
        askg-data:Entity-saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1670-Sentence-16702 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "In *IEEE ICCV*, 2019."@en ;
    askg-onto:inSentence "In *IEEE ICCV*, 2019."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_iccv .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1671 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 71"@en ;
    domo:Text "[62] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai. Weakly-Supervised Salient Object Detection via Scribble Annotations. In *IEEE CVPR*, 2020."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1671-Sentence-16711,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1671-Sentence-16712,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1671-Sentence-16713 ;
    askg-onto:index "71"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1671-Sentence-16711 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[62] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai."@en ;
    askg-onto:inSentence "[62] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-aixuan_li,
        askg-data:Entity-bowen_liu,
        askg-data:Entity-jing_zhang,
        askg-data:Entity-peipei_song,
        askg-data:Entity-person,
        askg-data:Entity-xin_yu,
        askg-data:Entity-yuchao_dai .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1671-Sentence-16712 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Weakly-Supervised Salient Object Detection via Scribble Annotations."@en ;
    askg-onto:inSentence "Weakly-Supervised Salient Object Detection via Scribble Annotations."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-method,
        askg-data:Entity-scribble_annotations,
        askg-data:Entity-technique,
        askg-data:Entity-weakly-supervised_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1671-Sentence-16713 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE CVPR*, 2020."@en ;
    askg-onto:inSentence "In *IEEE CVPR*, 2020."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2020,
        askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1672 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 72"@en ;
    domo:Text "[63] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi, and Richard Hartley. Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective. In IEEE CVPR, pages 9029–9038, 2018."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1672-Sentence-16721,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1672-Sentence-16722,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1672-Sentence-16723 ;
    askg-onto:index "72"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1672-Sentence-16721 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[63] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi, and Richard Hartley."@en ;
    askg-onto:inSentence "[63] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi, and Richard Hartley."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-jing_zhang,
        askg-data:Entity-mehrtash_harandi,
        askg-data:Entity-person,
        askg-data:Entity-richard_hartley,
        askg-data:Entity-tong_zhang,
        askg-data:Entity-yuchao_dai .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1672-Sentence-16722 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective."@en ;
    askg-onto:inSentence "Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-a_multiple_noisy_labeling_perspective,
        askg-data:Entity-deep_unsupervised_saliency_detection,
        askg-data:Entity-method .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1672-Sentence-16723 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In IEEE CVPR, pages 9029–9038, 2018."@en ;
    askg-onto:inSentence "In IEEE CVPR, pages 9029–9038, 2018."^^xsd:string ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1673 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 73"@en ;
    domo:Text "[64] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming Cheng, Xuan-Yi Li, and Le Zhang. Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection. In IEEE CVPR, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1673-Sentence-16731,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1673-Sentence-16732,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1673-Sentence-16733 ;
    askg-onto:index "73"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1673-Sentence-16731 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[64] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming Cheng, Xuan-Yi Li, and Le Zhang."@en ;
    askg-onto:inSentence "[64] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming Cheng, Xuan-Yi Li, and Le Zhang."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deng-ping_fan,
        askg-data:Entity-jia-xing_zhao,
        askg-data:Entity-le_zhang,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-xuan-yi_li,
        askg-data:Entity-yang_cao .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1673-Sentence-16732 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection."@en ;
    askg-onto:inSentence "Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-prior_and_fluid_pyramid_integration,
        askg-data:Entity-rgbd_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1673-Sentence-16733 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In IEEE CVPR, 2019."@en ;
    askg-onto:inSentence "In IEEE CVPR, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-2019,
        askg-data:Entity-ieee_cvpr,
        askg-data:Entity-publication,
        askg-data:Entity-year .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1674 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 74"@en ;
    domo:Text "[65] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. EGNet: Edge guidance network for salient object detection. In *IEEE ICCV*, pages 8779–8788, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1674-Sentence-16741,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1674-Sentence-16742,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1674-Sentence-16743 ;
    askg-onto:index "74"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1674-Sentence-16741 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[65] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng."@en ;
    askg-onto:inSentence "[65] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deng-ping_fan,
        askg-data:Entity-jia-xing_zhao,
        askg-data:Entity-jiang-jiang_liu,
        askg-data:Entity-jufeng_yang,
        askg-data:Entity-ming-ming_cheng,
        askg-data:Entity-person,
        askg-data:Entity-yang_cao .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1674-Sentence-16742 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "EGNet: Edge guidance network for salient object detection."@en ;
    askg-onto:inSentence "EGNet: Edge guidance network for salient object detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-edge_guidance_network_for_salient_object_detection,
        askg-data:Entity-egnet .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1674-Sentence-16743 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *IEEE ICCV*, pages 8779–8788, 2019."@en ;
    askg-onto:inSentence "In *IEEE ICCV*, pages 8779–8788, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_iccv .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1675 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 75"@en ;
    domo:Text "[66] Chunbiao Zhu, Ge Li, Wenmin Wang, and Ronggang Wang."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1675-Sentence-16751 ;
    askg-onto:index "75"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1675-Sentence-16751 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[66] Chunbiao Zhu, Ge Li, Wenmin Wang, and Ronggang Wang."@en ;
    askg-onto:inSentence "[66] Chunbiao Zhu, Ge Li, Wenmin Wang, and Ronggang Wang."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-chunbiao_zhu,
        askg-data:Entity-ge_li,
        askg-data:Entity-person,
        askg-data:Entity-ronggang_wang,
        askg-data:Entity-wenmin_wang .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1676 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 76"@en ;
    domo:Text "An innovative salient object detection using center-dark channel prior. In *IEEE ICCVW*, 2017."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1676-Sentence-16761,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1676-Sentence-16762 ;
    askg-onto:index "76"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1676-Sentence-16761 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "An innovative salient object detection using center-dark channel prior."@en ;
    askg-onto:inSentence "An innovative salient object detection using center-dark channel prior."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-center-dark_channel_prior,
        askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-1676-Sentence-16762 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "In *IEEE ICCVW*, 2017."@en ;
    askg-onto:inSentence "In *IEEE ICCVW*, 2017."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ieee_iccvw,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-168 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 8"@en ;
    domo:Text "[7] Hao Chen, Youfu Li, and Dan Su. Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection. PR, 86:376–385, 2019."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-168-Sentence-1681,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-168-Sentence-1682,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-168-Sentence-1683 ;
    askg-onto:index "8"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-168-Sentence-1681 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[7] Hao Chen, Youfu Li, and Dan Su."@en ;
    askg-onto:inSentence "[7] Hao Chen, Youfu Li, and Dan Su."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dan_su,
        askg-data:Entity-hao_chen,
        askg-data:Entity-person,
        askg-data:Entity-youfu_li .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-168-Sentence-1682 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection."@en ;
    askg-onto:inSentence "Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-multi-modal_fusion_network,
        askg-data:Entity-rgb-d_salient_object_detection .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-168-Sentence-1683 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "PR, 86:376–385, 2019."@en ;
    askg-onto:inSentence "PR, 86:376–385, 2019."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-86376385_2019,
        askg-data:Entity-pr .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-169 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 9"@en ;
    domo:Text "[8] Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao. Depth enhanced saliency detection method. In *ACM ICIMCS*, pages 23–27, 2014."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-169-Sentence-1691,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-169-Sentence-1692,
        askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-169-Sentence-1693 ;
    askg-onto:index "9"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-169-Sentence-1691 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "[8] Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao."@en ;
    askg-onto:inSentence "[8] Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-huazhu_fu,
        askg-data:Entity-jiangjian_xiao,
        askg-data:Entity-person,
        askg-data:Entity-xiaochun_cao,
        askg-data:Entity-xingxing_wei,
        askg-data:Entity-yupeng_cheng .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-169-Sentence-1692 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Depth enhanced saliency detection method."@en ;
    askg-onto:inSentence "Depth enhanced saliency detection method."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth_enhanced_saliency_detection,
        askg-data:Entity-method .

askg-data:Paper-fdb482b8f2df5642-Section-16-Paragraph-169-Sentence-1693 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In *ACM ICIMCS*, pages 23–27, 2014."@en ;
    askg-onto:inSentence "In *ACM ICIMCS*, pages 23–27, 2014."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-acm_icimcs,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-2 a askg-onto:Section ;
    rdfs:label "Section 2"@en ;
    domo:Text "Abstract"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21 ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:level "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "In this paper, we propose the first framework (UC- Net) to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. Existing RGB- D saliency detection methods treat the saliency detection task as a point estimation problem, and produce a single saliency map following a deterministic learning pipeline. Inspired by the saliency data labeling process, we propose probabilistic RGB-D saliency detection network via conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space. With the proposed saliency consensus process, we are able to generate an accurate saliency map based on these multiple predictions. Quantitative and qualitative evaluations on six challenging benchmark datasets against 18 competing algorithms demonstrate the effectiveness of our approach in learning the distribution of saliency maps, leading to a new state-of-the-art in RGB-D saliency detection1."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-211,
        askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-212,
        askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-213,
        askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-214,
        askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-215 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-211 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "In this paper, we propose the first framework (UC- Net) to employ uncertainty for RGB-D saliency detection by learning from the data labeling process."@en ;
    askg-onto:inSentence "In this paper, we propose the first framework (UC- Net) to employ uncertainty for RGB-D saliency detection by learning from the data labeling process."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-data_labeling_process,
        askg-data:Entity-framework,
        askg-data:Entity-method,
        askg-data:Entity-process,
        askg-data:Entity-rgb-d_saliency_detection,
        askg-data:Entity-uc-_net .

askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-212 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Existing RGB- D saliency detection methods treat the saliency detection task as a point estimation problem, and produce a single saliency map following a deterministic learning pipeline."@en ;
    askg-onto:inSentence "Existing RGB- D saliency detection methods treat the saliency detection task as a point estimation problem, and produce a single saliency map following a deterministic learning pipeline."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deterministic_learning_pipeline,
        askg-data:Entity-rgb-d_saliency_detection_methods,
        askg-data:Entity-saliency_detection_task,
        askg-data:Entity-saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-213 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Inspired by the saliency data labeling process, we propose probabilistic RGB-D saliency detection network via conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space."@en ;
    askg-onto:inSentence "Inspired by the saliency data labeling process, we propose probabilistic RGB-D saliency detection network via conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-conditional_variational_autoencoders,
        askg-data:Entity-dataset,
        askg-data:Entity-human_annotation_uncertainty,
        askg-data:Entity-method,
        askg-data:Entity-probabilistic_model,
        askg-data:Entity-rgb-d_saliency_detection_network,
        askg-data:Entity-saliency_maps .

askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-214 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "With the proposed saliency consensus process, we are able to generate an accurate saliency map based on these multiple predictions."@en ;
    askg-onto:inSentence "With the proposed saliency consensus process, we are able to generate an accurate saliency map based on these multiple predictions."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-multiple_predictions,
        askg-data:Entity-saliency_consensus_process,
        askg-data:Entity-saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-2-Paragraph-21-Sentence-215 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "Quantitative and qualitative evaluations on six challenging benchmark datasets against 18 competing algorithms demonstrate the effectiveness of our approach in learning the distribution of saliency maps, leading to a new state-of-the-art in RGB-D saliency detection1."@en ;
    askg-onto:inSentence "Quantitative and qualitative evaluations on six challenging benchmark datasets against 18 competing algorithms demonstrate the effectiveness of our approach in learning the distribution of saliency maps, leading to a new state-of-the-art in RGB-D saliency detection1."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-18_competing_algorithms,
        askg-data:Entity-a_new_state-of-the-art,
        askg-data:Entity-effectiveness_of_our_approach,
        askg-data:Entity-our_approach,
        askg-data:Entity-rgb-d_saliency_detection,
        askg-data:Entity-six_challenging_benchmark_datasets .

askg-data:Paper-fdb482b8f2df5642-Section-3 a askg-onto:Section ;
    rdfs:label "Section 3"@en ;
    domo:Text "1. Introduction"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-31,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-310,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-311,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-32,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-33,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-35,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-37,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-38,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-39 ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:level "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-31 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "Object-level visual saliency detection involves separating the most conspicuous objects that attract humans from the background [27, 2, 55, 63, 38, 29, 62]. Recently, visual saliency detection from RGB-D images have attracted lots of interest due to the importance of depth information in human vision system and the popularity of depth sensing technologies [61, 64]. Given a pair of RGB-D images, the task of RGB-D saliency detection aims to predict a saliency map by exploring the complementary information between color image and depth data."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-31-Sentence-311,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-31-Sentence-312,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-31-Sentence-313 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-31-Sentence-311 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Object-level visual saliency detection involves separating the most conspicuous objects that attract humans from the background [27, 2, 55, 63, 38, 29, 62]."@en ;
    askg-onto:inSentence "Object-level visual saliency detection involves separating the most conspicuous objects that attract humans from the background [27, 2, 55, 63, 38, 29, 62]."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-object-level_visual_saliency_detection,
        askg-data:Entity-separating_conspicuous_objects_from_the_background .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-31-Sentence-312 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Recently, visual saliency detection from RGB-D images have attracted lots of interest due to the importance of depth information in human vision system and the popularity of depth sensing technologies [61, 64]."@en ;
    askg-onto:inSentence "Recently, visual saliency detection from RGB-D images have attracted lots of interest due to the importance of depth information in human vision system and the popularity of depth sensing technologies [61, 64]."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth_information,
        askg-data:Entity-depth_sensing_technologies,
        askg-data:Entity-human_vision_system,
        askg-data:Entity-rgb-d_images,
        askg-data:Entity-technology,
        askg-data:Entity-visual_saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-31-Sentence-313 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Given a pair of RGB-D images, the task of RGB-D saliency detection aims to predict a saliency map by exploring the complementary information between color image and depth data."@en ;
    askg-onto:inSentence "Given a pair of RGB-D images, the task of RGB-D saliency detection aims to predict a saliency map by exploring the complementary information between color image and depth data."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-color_image,
        askg-data:Entity-depth_data,
        askg-data:Entity-rgb-d_images,
        askg-data:Entity-rgb-d_saliency_detection,
        askg-data:Entity-saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-310 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 10"@en ;
    domo:Text "Moreover, depth data in the RGB-D saliency dataset can be noisy, and a direct fusion of RGB and depth information may overwhelm the network to fit noise. To deal with the noisy depth problem, a depth correction network is proposed as an auxiliary component to produce depth images with rich semantic and geometric information. We also introduce a saliency consensus module to mimic the majority voting mechanism for saliency GT generation."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-310-Sentence-3101,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-310-Sentence-3102,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-310-Sentence-3103 ;
    askg-onto:index "10"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-310-Sentence-3101 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Moreover, depth data in the RGB-D saliency dataset can be noisy, and a direct fusion of RGB and depth information may overwhelm the network to fit noise."@en ;
    askg-onto:inSentence "Moreover, depth data in the RGB-D saliency dataset can be noisy, and a direct fusion of RGB and depth information may overwhelm the network to fit noise."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth_data,
        askg-data:Entity-noise,
        askg-data:Entity-rgb-d_saliency_dataset,
        askg-data:Entity-rgb_and_depth_information,
        askg-data:Entity-the_network .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-310-Sentence-3102 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "To deal with the noisy depth problem, a depth correction network is proposed as an auxiliary component to produce depth images with rich semantic and geometric information."@en ;
    askg-onto:inSentence "To deal with the noisy depth problem, a depth correction network is proposed as an auxiliary component to produce depth images with rich semantic and geometric information."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-auxiliary_component,
        askg-data:Entity-depth_correction_network,
        askg-data:Entity-rich_semantic_and_geometric_information .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-310-Sentence-3103 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "We also introduce a saliency consensus module to mimic the majority voting mechanism for saliency GT generation."@en ;
    askg-onto:inSentence "We also introduce a saliency consensus module to mimic the majority voting mechanism for saliency GT generation."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-majority_voting_mechanism,
        askg-data:Entity-saliency_consensus_module .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-311 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 11"@en ;
    domo:Text "Our main contributions are summarized as: 1) We propose a conditional probabilistic RGB-D saliency prediction model that can produce diverse saliency predictions instead of a single saliency map; 2) We provide a mechanism via saliency consensus to better model how saliency detection works; 3) We present a depth correction network to decrease noise that is inherent in depth data; 4) Extensive experimental results on six RGB-D saliency detection benchmark datasets demonstrate the effectiveness of our *UC-Net*."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-311-Sentence-3111 ;
    askg-onto:index "11"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-311-Sentence-3111 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Our main contributions are summarized as: 1) We propose a conditional probabilistic RGB-D saliency prediction model that can produce diverse saliency predictions instead of a single saliency map; 2) We provide a mechanism via saliency consensus to better model how saliency detection works; 3) We present a depth correction network to decrease noise that is inherent in depth data; 4) Extensive experimental results on six RGB-D saliency detection benchmark datasets demonstrate the effectiveness of our *UC-Net*."@en ;
    askg-onto:inSentence "Our main contributions are summarized as: 1) We propose a conditional probabilistic RGB-D saliency prediction model that can produce diverse saliency predictions instead of a single saliency map; 2) We provide a mechanism via saliency consensus to better model how saliency detection works; 3) We present a depth correction network to decrease noise that is inherent in depth data; 4) Extensive experimental results on six RGB-D saliency detection benchmark datasets demonstrate the effectiveness of our *UC-Net*."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-conditional_probabilistic_rgb-d_saliency_prediction_model,
        askg-data:Entity-depth_correction_network,
        askg-data:Entity-depth_data_noise_reduction,
        askg-data:Entity-diverse_saliency_predictions,
        askg-data:Entity-effective_in_rgb-d_saliency_detection,
        askg-data:Entity-effectiveness_of_uc-net,
        askg-data:Entity-mechanism,
        askg-data:Entity-rgb-d_saliency_detection_benchmark_datasets,
        askg-data:Entity-saliency_consensus,
        askg-data:Entity-uc-net .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-32 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "The de-facto standard for RGB-D saliency detection is to train a deep neural network using ground truth (GT)"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-32-Sentence-321 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-32-Sentence-321 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "The de-facto standard for RGB-D saliency detection is to train a deep neural network using ground truth (GT)"@en ;
    askg-onto:inSentence "The de-facto standard for RGB-D saliency detection is to train a deep neural network using ground truth (GT)"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-dataset,
        askg-data:Entity-deep_neural_network,
        askg-data:Entity-ground_truth,
        askg-data:Entity-method,
        askg-data:Entity-rgb-d_saliency_detection,
        askg-data:Entity-standard .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-33 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 3"@en ;
    domo:Text "![0_image_0.png](0_image_0.png)"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-33-Sentence-331 ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-33-Sentence-331 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "![0_image_0.png](0_image_0.png)"@en ;
    askg-onto:inSentence "![0_image_0.png](0_image_0.png)"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-experiment,
        askg-data:Entity-finding,
        askg-data:Entity-method,
        askg-data:Entity-publication,
        askg-data:Entity-research_group,
        askg-data:Entity-scientist,
        askg-data:Entity-technology,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 4"@en ;
    domo:Text "Figure 1. Provided GT compared with UC-Net (ours) predicted saliency maps. For images with a single salient object (1 st row), we can produce consistent prediction. When multiple salient objects exist (2nd row), we can produce diverse predictions."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34-Sentence-341,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34-Sentence-342,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34-Sentence-343,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34-Sentence-344 ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34-Sentence-341 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Figure 1."@en ;
    askg-onto:inSentence "Figure 1."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-figure_1,
        askg-data:Entity-research_concepts .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34-Sentence-342 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Provided GT compared with UC-Net (ours) predicted saliency maps."@en ;
    askg-onto:inSentence "Provided GT compared with UC-Net (ours) predicted saliency maps."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt,
        askg-data:Entity-uc-net .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34-Sentence-343 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "For images with a single salient object (1 st row), we can produce consistent prediction."@en ;
    askg-onto:inSentence "For images with a single salient object (1 st row), we can produce consistent prediction."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-consistent_result,
        askg-data:Entity-images,
        askg-data:Entity-prediction,
        askg-data:Entity-single_salient_object .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-34-Sentence-344 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "When multiple salient objects exist (2nd row), we can produce diverse predictions."@en ;
    askg-onto:inSentence "When multiple salient objects exist (2nd row), we can produce diverse predictions."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-multiple_salient_objects,
        askg-data:Entity-predictions .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-35 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 5"@en ;
    domo:Text "saliency maps provided by the corresponding benchmark datasets, where the GT saliency maps are obtained through human consensus or by the dataset creators [18]. Building upon large scale RGB-D datasets, deep convolutional neural network based models [21, 61, 6, 24] have made profound progress in learning the mapping from an RGB-D image pair to the corresponding GT saliency map. Considering the progress for RGB-D saliency detection under this pipeline, in this paper, we would like to argue that this pipeline fails to capture the *uncertainty* in labeling the GT saliency maps."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-35-Sentence-351,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-35-Sentence-352,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-35-Sentence-353 ;
    askg-onto:index "5"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-35-Sentence-351 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "saliency maps provided by the corresponding benchmark datasets, where the GT saliency maps are obtained through human consensus or by the dataset creators [18]."@en ;
    askg-onto:inSentence "saliency maps provided by the corresponding benchmark datasets, where the GT saliency maps are obtained through human consensus or by the dataset creators [18]."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-benchmark_datasets,
        askg-data:Entity-dataset_creators,
        askg-data:Entity-gt_saliency_maps,
        askg-data:Entity-human_consensus,
        askg-data:Entity-saliency_maps .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-35-Sentence-352 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Building upon large scale RGB-D datasets, deep convolutional neural network based models [21, 61, 6, 24] have made profound progress in learning the mapping from an RGB-D image pair to the corresponding GT saliency map."@en ;
    askg-onto:inSentence "Building upon large scale RGB-D datasets, deep convolutional neural network based models [21, 61, 6, 24] have made profound progress in learning the mapping from an RGB-D image pair to the corresponding GT saliency map."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deep_convolutional_neural_network_based_models,
        askg-data:Entity-gt_saliency_map,
        askg-data:Entity-rgb-d_datasets,
        askg-data:Entity-rgb-d_image_pair .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-35-Sentence-353 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Considering the progress for RGB-D saliency detection under this pipeline, in this paper, we would like to argue that this pipeline fails to capture the *uncertainty* in labeling the GT saliency maps."@en ;
    askg-onto:inSentence "Considering the progress for RGB-D saliency detection under this pipeline, in this paper, we would like to argue that this pipeline fails to capture the *uncertainty* in labeling the GT saliency maps."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt_saliency_maps,
        askg-data:Entity-labeling,
        askg-data:Entity-pipeline,
        askg-data:Entity-uncertainty .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 6"@en ;
    domo:Text "According to research in human visual perception [33], visual saliency detection is subjective to some extent. Each person could have specific preferences in labeling the saliency map (which has been previous discussed in userspecific saliency detection [26]). Existing approaches to RGB-D saliency detection treat saliency detection as a point estimation problem, and produce a single saliency map for each input image pair following a *deterministic* learning pipeline, which fails to capture the stochastic characteristic of saliency, and may lead to a partisan saliency model as shown in second row of Fig. 1. Instead of obtaining only a single saliency prediction (point estimation), we are interested in how the network produces multiple predictions (distribution estimation), which are then processed further to generate a single prediction in a similar way to how the GT saliency maps are created."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-361,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-362,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-363,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-364,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-365 ;
    askg-onto:index "6"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-361 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "According to research in human visual perception [33], visual saliency detection is subjective to some extent."@en ;
    askg-onto:inSentence "According to research in human visual perception [33], visual saliency detection is subjective to some extent."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-human_visual_perception,
        askg-data:Entity-visual_saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-362 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Each person could have specific preferences in labeling the saliency map (which has been previous discussed in userspecific saliency detection [26])."@en ;
    askg-onto:inSentence "Each person could have specific preferences in labeling the saliency map (which has been previous discussed in userspecific saliency detection [26])."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-person,
        askg-data:Entity-saliency_map,
        askg-data:Entity-userspecific_saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-363 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Existing approaches to RGB-D saliency detection treat saliency detection as a point estimation problem, and produce a single saliency map for each input image pair following a *deterministic* learning pipeline, which fails to capture the stochastic characteristic of saliency, and may lead to a partisan saliency model as shown in second row of Fig."@en ;
    askg-onto:inSentence "Existing approaches to RGB-D saliency detection treat saliency detection as a point estimation problem, and produce a single saliency map for each input image pair following a *deterministic* learning pipeline, which fails to capture the stochastic characteristic of saliency, and may lead to a partisan saliency model as shown in second row of Fig."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-existing_approaches,
        askg-data:Entity-point_estimation_problem,
        askg-data:Entity-rgb-d_saliency_detection,
        askg-data:Entity-saliency_detection,
        askg-data:Entity-saliency_map,
        askg-data:Entity-saliency_model,
        askg-data:Entity-second_row_of_fig,
        askg-data:Entity-stochastic_characteristic .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-364 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "1."@en ;
    askg-onto:inSentence "1."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-article,
        askg-data:Entity-city,
        askg-data:Entity-database,
        askg-data:Entity-dataset,
        askg-data:Entity-model,
        askg-data:Entity-research_group,
        askg-data:Entity-scientist,
        askg-data:Entity-study,
        askg-data:Entity-technology,
        askg-data:Entity-theory,
        askg-data:Entity-tool,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-36-Sentence-365 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "Instead of obtaining only a single saliency prediction (point estimation), we are interested in how the network produces multiple predictions (distribution estimation), which are then processed further to generate a single prediction in a similar way to how the GT saliency maps are created."@en ;
    askg-onto:inSentence "Instead of obtaining only a single saliency prediction (point estimation), we are interested in how the network produces multiple predictions (distribution estimation), which are then processed further to generate a single prediction in a similar way to how the GT saliency maps are created."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt_saliency_maps,
        askg-data:Entity-multiple_predictions,
        askg-data:Entity-network,
        askg-data:Entity-point_estimation,
        askg-data:Entity-saliency_prediction,
        askg-data:Entity-single_prediction .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-37 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 7"@en ;
    domo:Text "In this paper, inspired by human perceptual uncertainty,"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-37-Sentence-371 ;
    askg-onto:index "7"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-37-Sentence-371 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "In this paper, inspired by human perceptual uncertainty,"@en ;
    askg-onto:inSentence "In this paper, inspired by human perceptual uncertainty,"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-human_perceptual_uncertainty,
        askg-data:Entity-paper .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-38 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 8"@en ;
    domo:Text "∗Corresponding author: Deng-Ping Fan *(dengpfan@gmail.com)* 1Our code is publicly available at: https://github.com/ JingZhang617/UCNet. we propose a conditional variational autoencoders [50] (CVAE) based RGB-D saliency detection model *UC-Net* to produce multiple saliency predictions by modeling the distribution of output space as a generative model conditioned on the input RGB-D images to account for the human uncertainty in annotation."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-38-Sentence-381,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-38-Sentence-382 ;
    askg-onto:index "8"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-38-Sentence-381 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "∗Corresponding author: Deng-Ping Fan *(dengpfan@gmail.com)* 1Our code is publicly available at: https://github.com/ JingZhang617/UCNet."@en ;
    askg-onto:inSentence "∗Corresponding author: Deng-Ping Fan *(dengpfan@gmail.com)* 1Our code is publicly available at: https://github.com/ JingZhang617/UCNet."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deng-ping_fan,
        askg-data:Entity-httpsgithubcomjingzhang617ucnet,
        askg-data:Entity-jingzhang617,
        askg-data:Entity-ucnet .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-38-Sentence-382 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "we propose a conditional variational autoencoders [50] (CVAE) based RGB-D saliency detection model *UC-Net* to produce multiple saliency predictions by modeling the distribution of output space as a generative model conditioned on the input RGB-D images to account for the human uncertainty in annotation."@en ;
    askg-onto:inSentence "we propose a conditional variational autoencoders [50] (CVAE) based RGB-D saliency detection model *UC-Net* to produce multiple saliency predictions by modeling the distribution of output space as a generative model conditioned on the input RGB-D images to account for the human uncertainty in annotation."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-conditional_variational_autoencoders,
        askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-rgb-d_images,
        askg-data:Entity-rgb-d_saliency_detection,
        askg-data:Entity-technique,
        askg-data:Entity-uc-net .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-39 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 9"@en ;
    domo:Text "However, there still exists one obstacle before we could apply the probabilistic framework, that is existing RGB- D benchmark datasets generally only provide a single GT saliency map for each RGB-D image pair. To produce diverse and accurate predictions2, we resort to the \"hide and seek\" [49] principle following the orientation shifting theory [26] by iteratively hiding the salient foreground from the RGB image for testing, which forces the deep network to learn the saliency map with diversity. Through this iterative hiding strategy, we obtain multiple saliency maps for each input RGB-D image pair, which reflects the diversity/uncertainty from human labeling."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-39-Sentence-391,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-39-Sentence-392,
        askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-39-Sentence-393 ;
    askg-onto:index "9"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-39-Sentence-391 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "However, there still exists one obstacle before we could apply the probabilistic framework, that is existing RGB- D benchmark datasets generally only provide a single GT saliency map for each RGB-D image pair."@en ;
    askg-onto:inSentence "However, there still exists one obstacle before we could apply the probabilistic framework, that is existing RGB- D benchmark datasets generally only provide a single GT saliency map for each RGB-D image pair."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt_saliency_map,
        askg-data:Entity-rgb-d_benchmark_datasets,
        askg-data:Entity-rgb-d_image_pairs,
        askg-data:Entity-single_gt_saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-39-Sentence-392 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "To produce diverse and accurate predictions2, we resort to the \"hide and seek\" [49] principle following the orientation shifting theory [26] by iteratively hiding the salient foreground from the RGB image for testing, which forces the deep network to learn the saliency map with diversity."@en ;
    askg-onto:inSentence "To produce diverse and accurate predictions2, we resort to the \"hide and seek\" [49] principle following the orientation shifting theory [26] by iteratively hiding the salient foreground from the RGB image for testing, which forces the deep network to learn the saliency map with diversity."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-hide_and_seek_principle,
        askg-data:Entity-orientation_shifting_theory .

askg-data:Paper-fdb482b8f2df5642-Section-3-Paragraph-39-Sentence-393 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Through this iterative hiding strategy, we obtain multiple saliency maps for each input RGB-D image pair, which reflects the diversity/uncertainty from human labeling."@en ;
    askg-onto:inSentence "Through this iterative hiding strategy, we obtain multiple saliency maps for each input RGB-D image pair, which reflects the diversity/uncertainty from human labeling."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-diversityuncertainty,
        askg-data:Entity-human_labeling,
        askg-data:Entity-multiple_saliency_maps,
        askg-data:Entity-rgb-d_image_pair .

askg-data:Paper-fdb482b8f2df5642-Section-4 a askg-onto:Section ;
    rdfs:label "Section 4"@en ;
    domo:Text "2. Related Work"@en ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:level "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-5 a askg-onto:Section ;
    rdfs:label "Section 5"@en ;
    domo:Text "2.1. Rgb-D Saliency Detection"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51 ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:level "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "Depend on how the complementary information between RGB images and depth images is fused, existing RGB-D saliency detection models can be roughly classified into three categories: early-fusion models [43], late-fusion models [54, 24] and cross-level fusion models [61, 5, 7, 6, 64]. Qu *et al*. [43] proposed an early-fusion model to generate feature for each superpixel of the RGB-D pair, which was then fed to a CNN to produce saliency of each superpixel. Recently, Wang *et al*. [54] introduced a late-fusion network (i.e. AFNet) to fuse predictions from the RGB and depth branch adaptively. In a similar pipeline, Han *et al*. [24] fused the RGB and depth information through fully connected layers. Chen *et al*. [7] used a multi-scale multi-path network for different modality information fusion. Chen et al. [5] proposed a complementary-aware RGB-D saliency detection model by fusing features from the same stage of each modality with a complementary-aware fusion block. Chen *et al*. [6] presented attention-aware cross-level combination blocks for multi-modality fusion. Zhao *et al*. [64] integrated a contrast prior to enhance depth cues, and employed a fluid pyramid integration framework to achieve multi-scale cross-modal feature fusion. To effectively incorporate geometric and semantic information within a recurrent learning framework, Li *et al*. [61] introduced a depthinduced multi-scale RGB-D saliency detection network."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-511,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5110,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5111,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5112,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5113,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5114,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5115,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5116,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5117,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5118,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-512,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-513,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-514,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-515,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-516,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-517,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-518,
        askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-519 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-511 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Depend on how the complementary information between RGB images and depth images is fused, existing RGB-D saliency detection models can be roughly classified into three categories: early-fusion models [43], late-fusion models [54, 24] and cross-level fusion models [61, 5, 7, 6, 64]."@en ;
    askg-onto:inSentence "Depend on how the complementary information between RGB images and depth images is fused, existing RGB-D saliency detection models can be roughly classified into three categories: early-fusion models [43], late-fusion models [54, 24] and cross-level fusion models [61, 5, 7, 6, 64]."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cross-level_fusion_models,
        askg-data:Entity-depth_images,
        askg-data:Entity-early-fusion_models,
        askg-data:Entity-late-fusion_models,
        askg-data:Entity-rgb-d_saliency_detection_models,
        askg-data:Entity-rgb_images .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5110 a askg-onto:Sentence ;
    rdfs:label "Sentence 10"@en ;
    domo:Text "[7] used a multi-scale multi-path network for different modality information fusion."@en ;
    askg-onto:inSentence "[7] used a multi-scale multi-path network for different modality information fusion."^^xsd:string ;
    askg-onto:index "10"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-different_modality_information_fusion,
        askg-data:Entity-multi-scale_multi-path_network .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5111 a askg-onto:Sentence ;
    rdfs:label "Sentence 11"@en ;
    domo:Text "Chen et al."@en ;
    askg-onto:inSentence "Chen et al."^^xsd:string ;
    askg-onto:index "11"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-chen_et_al,
        askg-data:Entity-publication .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5112 a askg-onto:Sentence ;
    rdfs:label "Sentence 12"@en ;
    domo:Text "[5] proposed a complementary-aware RGB-D saliency detection model by fusing features from the same stage of each modality with a complementary-aware fusion block."@en ;
    askg-onto:inSentence "[5] proposed a complementary-aware RGB-D saliency detection model by fusing features from the same stage of each modality with a complementary-aware fusion block."^^xsd:string ;
    askg-onto:index "12"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-complementary-aware_fusion_block,
        askg-data:Entity-rgb-d_saliency_detection_model .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5113 a askg-onto:Sentence ;
    rdfs:label "Sentence 13"@en ;
    domo:Text "Chen *et al*."@en ;
    askg-onto:inSentence "Chen *et al*."^^xsd:string ;
    askg-onto:index "13"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-chen .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5114 a askg-onto:Sentence ;
    rdfs:label "Sentence 14"@en ;
    domo:Text "[6] presented attention-aware cross-level combination blocks for multi-modality fusion."@en ;
    askg-onto:inSentence "[6] presented attention-aware cross-level combination blocks for multi-modality fusion."^^xsd:string ;
    askg-onto:index "14"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-attention-aware_cross-level_combination_blocks,
        askg-data:Entity-multi-modality_fusion .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5115 a askg-onto:Sentence ;
    rdfs:label "Sentence 15"@en ;
    domo:Text "Zhao *et al*."@en ;
    askg-onto:inSentence "Zhao *et al*."^^xsd:string ;
    askg-onto:index "15"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-zhao .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5116 a askg-onto:Sentence ;
    rdfs:label "Sentence 16"@en ;
    domo:Text "[64] integrated a contrast prior to enhance depth cues, and employed a fluid pyramid integration framework to achieve multi-scale cross-modal feature fusion."@en ;
    askg-onto:inSentence "[64] integrated a contrast prior to enhance depth cues, and employed a fluid pyramid integration framework to achieve multi-scale cross-modal feature fusion."^^xsd:string ;
    askg-onto:index "16"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-contrast_prior,
        askg-data:Entity-depth_cues,
        askg-data:Entity-fluid_pyramid_integration_framework,
        askg-data:Entity-multi-scale_cross-modal_feature_fusion .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5117 a askg-onto:Sentence ;
    rdfs:label "Sentence 17"@en ;
    domo:Text "To effectively incorporate geometric and semantic information within a recurrent learning framework, Li *et al*."@en ;
    askg-onto:inSentence "To effectively incorporate geometric and semantic information within a recurrent learning framework, Li *et al*."^^xsd:string ;
    askg-onto:index "17"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-geometric_and_semantic_information,
        askg-data:Entity-li_et_al,
        askg-data:Entity-recurrent_learning_framework .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-5118 a askg-onto:Sentence ;
    rdfs:label "Sentence 18"@en ;
    domo:Text "[61] introduced a depthinduced multi-scale RGB-D saliency detection network."@en ;
    askg-onto:inSentence "[61] introduced a depthinduced multi-scale RGB-D saliency detection network."^^xsd:string ;
    askg-onto:index "18"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depthinduced_multi-scale_rgb-d_saliency_detection_network,
        askg-data:Entity-saliency_detection_network .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-512 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Qu *et al*."@en ;
    askg-onto:inSentence "Qu *et al*."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-publication,
        askg-data:Entity-qu_et_al .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-513 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "[43] proposed an early-fusion model to generate feature for each superpixel of the RGB-D pair, which was then fed to a CNN to produce saliency of each superpixel."@en ;
    askg-onto:inSentence "[43] proposed an early-fusion model to generate feature for each superpixel of the RGB-D pair, which was then fed to a CNN to produce saliency of each superpixel."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cnn,
        askg-data:Entity-early-fusion_model,
        askg-data:Entity-feature_generation,
        askg-data:Entity-saliency_of_superpixel .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-514 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Recently, Wang *et al*."@en ;
    askg-onto:inSentence "Recently, Wang *et al*."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-publication,
        askg-data:Entity-wang_et_al .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-515 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "[54] introduced a late-fusion network (i.e."@en ;
    askg-onto:inSentence "[54] introduced a late-fusion network (i.e."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-54,
        askg-data:Entity-late-fusion_network .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-516 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "AFNet) to fuse predictions from the RGB and depth branch adaptively."@en ;
    askg-onto:inSentence "AFNet) to fuse predictions from the RGB and depth branch adaptively."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-afnet,
        askg-data:Entity-predictions_from_the_rgb_and_depth_branch_adaptively .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-517 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "In a similar pipeline, Han *et al*."@en ;
    askg-onto:inSentence "In a similar pipeline, Han *et al*."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-han_et_al,
        askg-data:Entity-pipeline .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-518 a askg-onto:Sentence ;
    rdfs:label "Sentence 8"@en ;
    domo:Text "[24] fused the RGB and depth information through fully connected layers."@en ;
    askg-onto:inSentence "[24] fused the RGB and depth information through fully connected layers."^^xsd:string ;
    askg-onto:index "8"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-fully_connected_layers,
        askg-data:Entity-rgb_and_depth_information .

askg-data:Paper-fdb482b8f2df5642-Section-5-Paragraph-51-Sentence-519 a askg-onto:Sentence ;
    rdfs:label "Sentence 9"@en ;
    domo:Text "Chen *et al*."@en ;
    askg-onto:inSentence "Chen *et al*."^^xsd:string ;
    askg-onto:index "9"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-chen .

askg-data:Paper-fdb482b8f2df5642-Section-6 a askg-onto:Section ;
    rdfs:label "Section 6"@en ;
    domo:Text "2.2. Vae Or Cvae Based Deep Probabilistic Models"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62 ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:level "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "Ever since the seminal work by Kingma *et al*. [31] and Rezende *et al*. [45], variational autoencoder (VAE) and its conditional counterpart CVAE [50] have been widely applied in various computer vision problems. To train a VAE, a reconstruction loss and a regularizer are needed to penalize the disagreement of the prior and posterior distribution of the latent representation. Instead of defining the prior distribution of the latent representation as a standard Gaussian distribution, CVAE utilizes the input observation to modulate the prior on Gaussian latent variables to generate the output. In low-level vision, VAE and CVAE have been applied to the tasks such as image background modeling [34], latent representations with sharp samples [25], difference of motion modes [57], medical image segmentation model [3], and modeling inherent ambiguities of an image [32]. Meanwhile, VAE and CVAE have been explored in more complex vision tasks such as uncertain future forecast [1, 53], human motion prediction [47], and shape-guided image generation [12]. Recently, VAE algorithms have been extened to 3D domain targeting applications such as 3D meshes deformation [52], and point cloud instance segmentation [59]."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-611,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-612,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-613,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-614,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-615,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-616,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-617,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-618 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-611 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Ever since the seminal work by Kingma *et al*."@en ;
    askg-onto:inSentence "Ever since the seminal work by Kingma *et al*."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-kingma,
        askg-data:Entity-seminal_work .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-612 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "[31] and Rezende *et al*."@en ;
    askg-onto:inSentence "[31] and Rezende *et al*."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-rezende .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-613 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "[45], variational autoencoder (VAE) and its conditional counterpart CVAE [50] have been widely applied in various computer vision problems."@en ;
    askg-onto:inSentence "[45], variational autoencoder (VAE) and its conditional counterpart CVAE [50] have been widely applied in various computer vision problems."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-computer_vision_problems,
        askg-data:Entity-conditional_counterpart_cvae,
        askg-data:Entity-model,
        askg-data:Entity-vae,
        askg-data:Entity-variational_autoencoder_vae .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-614 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "To train a VAE, a reconstruction loss and a regularizer are needed to penalize the disagreement of the prior and posterior distribution of the latent representation."@en ;
    askg-onto:inSentence "To train a VAE, a reconstruction loss and a regularizer are needed to penalize the disagreement of the prior and posterior distribution of the latent representation."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-disagreement,
        askg-data:Entity-latent_representation,
        askg-data:Entity-posterior_distribution,
        askg-data:Entity-prior_distribution,
        askg-data:Entity-reconstruction_loss,
        askg-data:Entity-regularizer,
        askg-data:Entity-vae .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-615 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "Instead of defining the prior distribution of the latent representation as a standard Gaussian distribution, CVAE utilizes the input observation to modulate the prior on Gaussian latent variables to generate the output."@en ;
    askg-onto:inSentence "Instead of defining the prior distribution of the latent representation as a standard Gaussian distribution, CVAE utilizes the input observation to modulate the prior on Gaussian latent variables to generate the output."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-input_observation,
        askg-data:Entity-output,
        askg-data:Entity-prior_on_gaussian_latent_variables .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-616 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "In low-level vision, VAE and CVAE have been applied to the tasks such as image background modeling [34], latent representations with sharp samples [25], difference of motion modes [57], medical image segmentation model [3], and modeling inherent ambiguities of an image [32]."@en ;
    askg-onto:inSentence "In low-level vision, VAE and CVAE have been applied to the tasks such as image background modeling [34], latent representations with sharp samples [25], difference of motion modes [57], medical image segmentation model [3], and modeling inherent ambiguities of an image [32]."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-difference_of_motion_modes,
        askg-data:Entity-image_background_modeling,
        askg-data:Entity-latent_representations_with_sharp_samples,
        askg-data:Entity-medical_image_segmentation_model,
        askg-data:Entity-modeling_inherent_ambiguities_of_an_image,
        askg-data:Entity-vae .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-617 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "Meanwhile, VAE and CVAE have been explored in more complex vision tasks such as uncertain future forecast [1, 53], human motion prediction [47], and shape-guided image generation [12]."@en ;
    askg-onto:inSentence "Meanwhile, VAE and CVAE have been explored in more complex vision tasks such as uncertain future forecast [1, 53], human motion prediction [47], and shape-guided image generation [12]."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-complex_vision_tasks,
        askg-data:Entity-cvae,
        askg-data:Entity-human_motion_prediction,
        askg-data:Entity-shape-guided_image_generation,
        askg-data:Entity-uncertain_future_forecast,
        askg-data:Entity-vae .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-61-Sentence-618 a askg-onto:Sentence ;
    rdfs:label "Sentence 8"@en ;
    domo:Text "Recently, VAE algorithms have been extened to 3D domain targeting applications such as 3D meshes deformation [52], and point cloud instance segmentation [59]."@en ;
    askg-onto:inSentence "Recently, VAE algorithms have been extened to 3D domain targeting applications such as 3D meshes deformation [52], and point cloud instance segmentation [59]."^^xsd:string ;
    askg-onto:index "8"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-3d_domain,
        askg-data:Entity-3d_meshes_deformation,
        askg-data:Entity-point_cloud_instance_segmentation,
        askg-data:Entity-vae_algorithms .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "To the best of our knowledge, CVAE has not been exploited in saliency detection. Although Li *et al*. [34] adopted VAE in their saliency prediction framework, they used VAE to model the image background, and separated salient objects from the background through the reconstruction residuals. In contrast, we use CVAE to model labeling variants, indicating human uncertainty of labeling. We are the first to employ CVAE in saliency prediction network by considering the human uncertainty in annotation."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-621,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-622,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-623,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-624,
        askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-625 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-621 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "To the best of our knowledge, CVAE has not been exploited in saliency detection."@en ;
    askg-onto:inSentence "To the best of our knowledge, CVAE has not been exploited in saliency detection."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-622 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Although Li *et al*."@en ;
    askg-onto:inSentence "Although Li *et al*."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-li .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-623 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "[34] adopted VAE in their saliency prediction framework, they used VAE to model the image background, and separated salient objects from the background through the reconstruction residuals."@en ;
    askg-onto:inSentence "[34] adopted VAE in their saliency prediction framework, they used VAE to model the image background, and separated salient objects from the background through the reconstruction residuals."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-background,
        askg-data:Entity-image_background,
        askg-data:Entity-reconstruction_residuals,
        askg-data:Entity-saliency_prediction_framework,
        askg-data:Entity-salient_objects,
        askg-data:Entity-separation_of_salient_objects,
        askg-data:Entity-vae .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-624 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "In contrast, we use CVAE to model labeling variants, indicating human uncertainty of labeling."@en ;
    askg-onto:inSentence "In contrast, we use CVAE to model labeling variants, indicating human uncertainty of labeling."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-labeling_variants .

askg-data:Paper-fdb482b8f2df5642-Section-6-Paragraph-62-Sentence-625 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "We are the first to employ CVAE in saliency prediction network by considering the human uncertainty in annotation."@en ;
    askg-onto:inSentence "We are the first to employ CVAE in saliency prediction network by considering the human uncertainty in annotation."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-annotation,
        askg-data:Entity-cvae,
        askg-data:Entity-human_uncertainty,
        askg-data:Entity-saliency_prediction_network .

askg-data:Paper-fdb482b8f2df5642-Section-7 a askg-onto:Section ;
    rdfs:label "Section 7"@en ;
    domo:Text "3. Our Model"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-71,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-73,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-75,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-78 ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:level "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-71 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "In this section, we present our probabilistic RGB-D saliency detection model based on a conditional variational autoencoder, which learns the distribution of saliency maps rather than a single prediction. Let ξ = {Xi, Yi} N i=1 be the training dataset, where Xi = {Ii, Di} denotes the RGB-D"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-71-Sentence-711,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-71-Sentence-712 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-71-Sentence-711 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "In this section, we present our probabilistic RGB-D saliency detection model based on a conditional variational autoencoder, which learns the distribution of saliency maps rather than a single prediction."@en ;
    askg-onto:inSentence "In this section, we present our probabilistic RGB-D saliency detection model based on a conditional variational autoencoder, which learns the distribution of saliency maps rather than a single prediction."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-conditional_variational_autoencoder,
        askg-data:Entity-probabilistic_rgb-d_saliency_detection_model .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-71-Sentence-712 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Let ξ = {Xi, Yi} N i=1 be the training dataset, where Xi = {Ii, Di} denotes the RGB-D"@en ;
    askg-onto:inSentence "Let ξ = {Xi, Yi} N i=1 be the training dataset, where Xi = {Ii, Di} denotes the RGB-D"^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%CE%BE,
        askg-data:Entity-ii_di,
        askg-data:Entity-rgb-d,
        askg-data:Entity-training_dataset,
        askg-data:Entity-xi,
        askg-data:Entity-yi .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "2Diversity of prediction is related to the content of image. Image with clear content may lead to consistent prediction (1st row in Fig. 1), while complex image may produce diverse predictions (2nd row of Fig. 1)."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72-Sentence-721,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72-Sentence-722,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72-Sentence-723,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72-Sentence-724 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72-Sentence-721 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "2Diversity of prediction is related to the content of image."@en ;
    askg-onto:inSentence "2Diversity of prediction is related to the content of image."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-diversity_of_prediction,
        askg-data:Entity-the_content_of_image .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72-Sentence-722 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Image with clear content may lead to consistent prediction (1st row in Fig."@en ;
    askg-onto:inSentence "Image with clear content may lead to consistent prediction (1st row in Fig."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-consistent_prediction,
        askg-data:Entity-image .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72-Sentence-723 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "1), while complex image may produce diverse predictions (2nd row of Fig."@en ;
    askg-onto:inSentence "1), while complex image may produce diverse predictions (2nd row of Fig."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-complex_image,
        askg-data:Entity-predictions .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-72-Sentence-724 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "1)."@en ;
    askg-onto:inSentence "1)."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-field,
        askg-data:Entity-organization,
        askg-data:Entity-research_area,
        askg-data:Entity-research_group,
        askg-data:Entity-study,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-73 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 3"@en ;
    domo:Text "![2_image_0.png](2_image_0.png)"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-73-Sentence-731 ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-73-Sentence-731 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "![2_image_0.png](2_image_0.png)"@en ;
    askg-onto:inSentence "![2_image_0.png](2_image_0.png)"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-advances_in_biomaterials,
        askg-data:Entity-cambridge_centre_for_medical_materials,
        askg-data:Entity-regenerative_medicine,
        askg-data:Entity-research_group,
        askg-data:Entity-tissue_engineering,
        askg-data:Entity-university_of_cambridge .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 4"@en ;
    domo:Text "Figure 2. Network training pipeline. Four main modules are included, namely a LatentNet (PriorNet (µprior, σprior) and PosteriorNet (µpost, σpost)), a SaliencyNet, a DepthCorrectionNet and a PredictionNet. The LatentNet maps the RGB-D image pair X (or together with GT Y for the PosteriorNet) to low dimensional Gaussian latent variable z. The DepthCorrectionNet refines the raw depth with a semantic guided loss. The SaliencyNet takes the RGB image and the refined depth as input to generate a saliency feature map. The PredictionNet takes both stochastic features and deterministic features to produce a final saliency map. We perform saliency consensus in the testing stage, as shown in Fig. 3 to generate the final saliency map according to the mechanism of GT saliency map generation."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-741,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-742,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-743,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-744,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-745,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-746,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-747,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-748,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-749 ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-741 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Figure 2."@en ;
    askg-onto:inSentence "Figure 2."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-data_visualization .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-742 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Network training pipeline."@en ;
    askg-onto:inSentence "Network training pipeline."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-framework,
        askg-data:Entity-network_training_pipeline .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-743 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Four main modules are included, namely a LatentNet (PriorNet (µprior, σprior) and PosteriorNet (µpost, σpost)), a SaliencyNet, a DepthCorrectionNet and a PredictionNet."@en ;
    askg-onto:inSentence "Four main modules are included, namely a LatentNet (PriorNet (µprior, σprior) and PosteriorNet (µpost, σpost)), a SaliencyNet, a DepthCorrectionNet and a PredictionNet."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depthcorrectionnet,
        askg-data:Entity-latentnet,
        askg-data:Entity-posteriornet,
        askg-data:Entity-predictionnet,
        askg-data:Entity-priornet,
        askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-744 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "The LatentNet maps the RGB-D image pair X (or together with GT Y for the PosteriorNet) to low dimensional Gaussian latent variable z."@en ;
    askg-onto:inSentence "The LatentNet maps the RGB-D image pair X (or together with GT Y for the PosteriorNet) to low dimensional Gaussian latent variable z."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt_y,
        askg-data:Entity-latentnet,
        askg-data:Entity-low_dimensional_gaussian_latent_variable_z,
        askg-data:Entity-posteriornet,
        askg-data:Entity-rgb-d_image_pair_x .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-745 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "The DepthCorrectionNet refines the raw depth with a semantic guided loss."@en ;
    askg-onto:inSentence "The DepthCorrectionNet refines the raw depth with a semantic guided loss."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depthcorrectionnet,
        askg-data:Entity-raw_depth .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-746 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "The SaliencyNet takes the RGB image and the refined depth as input to generate a saliency feature map."@en ;
    askg-onto:inSentence "The SaliencyNet takes the RGB image and the refined depth as input to generate a saliency feature map."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-refined_depth,
        askg-data:Entity-rgb_image,
        askg-data:Entity-saliency_feature_map,
        askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-747 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "The PredictionNet takes both stochastic features and deterministic features to produce a final saliency map."@en ;
    askg-onto:inSentence "The PredictionNet takes both stochastic features and deterministic features to produce a final saliency map."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-predictionnet,
        askg-data:Entity-saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-748 a askg-onto:Sentence ;
    rdfs:label "Sentence 8"@en ;
    domo:Text "We perform saliency consensus in the testing stage, as shown in Fig."@en ;
    askg-onto:inSentence "We perform saliency consensus in the testing stage, as shown in Fig."^^xsd:string ;
    askg-onto:index "8"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-saliency_consensus,
        askg-data:Entity-testing_stage .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-74-Sentence-749 a askg-onto:Sentence ;
    rdfs:label "Sentence 9"@en ;
    domo:Text "3 to generate the final saliency map according to the mechanism of GT saliency map generation."@en ;
    askg-onto:inSentence "3 to generate the final saliency map according to the mechanism of GT saliency map generation."^^xsd:string ;
    askg-onto:index "9"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt_saliency_map_generation,
        askg-data:Entity-saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-75 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 5"@en ;
    domo:Text "![2_image_1.png](2_image_1.png)"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-75-Sentence-751 ;
    askg-onto:index "5"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-75-Sentence-751 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "![2_image_1.png](2_image_1.png)"@en ;
    askg-onto:inSentence "![2_image_1.png](2_image_1.png)"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-artificial_intelligence,
        askg-data:Entity-clinical_trials,
        askg-data:Entity-covid-19,
        askg-data:Entity-deep_learning,
        askg-data:Entity-disease,
        askg-data:Entity-framework,
        askg-data:Entity-machine_learning,
        askg-data:Entity-pytorch,
        askg-data:Entity-tensorflow,
        askg-data:Entity-ucla,
        askg-data:Entity-ucsf,
        askg-data:Entity-university_of_california_san_francisco .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 6"@en ;
    domo:Text "Figure 3. Overview of the proposed framework during testing. We sample the PriorNet multiple times to generate diverse and accurate predictions. The saliency consensus module is then used to obtain the majority voting of the final predictions."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76-Sentence-761,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76-Sentence-762,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76-Sentence-763,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76-Sentence-764 ;
    askg-onto:index "6"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76-Sentence-761 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Figure 3."@en ;
    askg-onto:inSentence "Figure 3."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-data_visualization .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76-Sentence-762 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Overview of the proposed framework during testing."@en ;
    askg-onto:inSentence "Overview of the proposed framework during testing."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-framework,
        askg-data:Entity-testing .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76-Sentence-763 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "We sample the PriorNet multiple times to generate diverse and accurate predictions."@en ;
    askg-onto:inSentence "We sample the PriorNet multiple times to generate diverse and accurate predictions."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-predictions,
        askg-data:Entity-priornet .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-76-Sentence-764 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "The saliency consensus module is then used to obtain the majority voting of the final predictions."@en ;
    askg-onto:inSentence "The saliency consensus module is then used to obtain the majority voting of the final predictions."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-majority_voting,
        askg-data:Entity-saliency_consensus_module .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 7"@en ;
    domo:Text "input (consisting of the RGB image Ii and the depth image Di), Yi denotes the ground truth saliency map. The whole pipeline of our model during training and testing are illustrated in Fig. 2 and Fig. 3, respectively."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77-Sentence-771,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77-Sentence-772,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77-Sentence-773,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77-Sentence-774 ;
    askg-onto:index "7"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77-Sentence-771 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "input (consisting of the RGB image Ii and the depth image Di), Yi denotes the ground truth saliency map."@en ;
    askg-onto:inSentence "input (consisting of the RGB image Ii and the depth image Di), Yi denotes the ground truth saliency map."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-ground_truth_saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77-Sentence-772 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "The whole pipeline of our model during training and testing are illustrated in Fig."@en ;
    askg-onto:inSentence "The whole pipeline of our model during training and testing are illustrated in Fig."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-fig,
        askg-data:Entity-model .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77-Sentence-773 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "2 and Fig."@en ;
    askg-onto:inSentence "2 and Fig."^^xsd:string ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-77-Sentence-774 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "3, respectively."@en ;
    askg-onto:inSentence "3, respectively."^^xsd:string ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-78 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 8"@en ;
    domo:Text "Our network is composed of five main modules: 1) LatentNet (PriorNet and PosteriorNet) that maps the RGB-D input Xi (for PriorNet) or Xi and Yi (for PosteriorNet) to the low dimensional latent variables zi ∈ R K (K is dimension of the latent space); 2) DepthCorrectionNet that takes Ii and Di as input to generate a refined depth image D0i; 3) SaliencyNet that maps the RGB image Ii and the refined depth image D0ito saliency feature maps S d i; 4) Prediction- Net that employs stochastic features S s i from LatentNet and deterministic features S d i from SaliencyNet to produce our saliency map prediction Pi; 5) A saliency consensus module in the testing stage that mimics the mechanism of saliency GT generation to evaluate the performance with the provided single GT saliency map Yi. We will introduce each module as follows."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-78-Sentence-781,
        askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-78-Sentence-782 ;
    askg-onto:index "8"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-78-Sentence-781 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Our network is composed of five main modules: 1) LatentNet (PriorNet and PosteriorNet) that maps the RGB-D input Xi (for PriorNet) or Xi and Yi (for PosteriorNet) to the low dimensional latent variables zi ∈ R K (K is dimension of the latent space); 2) DepthCorrectionNet that takes Ii and Di as input to generate a refined depth image D0i; 3) SaliencyNet that maps the RGB image Ii and the refined depth image D0ito saliency feature maps S d i; 4) Prediction- Net that employs stochastic features S s i from LatentNet and deterministic features S d i from SaliencyNet to produce our saliency map prediction Pi; 5) A saliency consensus module in the testing stage that mimics the mechanism of saliency GT generation to evaluate the performance with the provided single GT saliency map Yi."@en ;
    askg-onto:inSentence "Our network is composed of five main modules: 1) LatentNet (PriorNet and PosteriorNet) that maps the RGB-D input Xi (for PriorNet) or Xi and Yi (for PosteriorNet) to the low dimensional latent variables zi ∈ R K (K is dimension of the latent space); 2) DepthCorrectionNet that takes Ii and Di as input to generate a refined depth image D0i; 3) SaliencyNet that maps the RGB image Ii and the refined depth image D0ito saliency feature maps S d i; 4) Prediction- Net that employs stochastic features S s i from LatentNet and deterministic features S d i from SaliencyNet to produce our saliency map prediction Pi; 5) A saliency consensus module in the testing stage that mimics the mechanism of saliency GT generation to evaluate the performance with the provided single GT saliency map Yi."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-d0i,
        askg-data:Entity-depthcorrectionnet,
        askg-data:Entity-deterministic_features_s_d_i,
        askg-data:Entity-di,
        askg-data:Entity-ii,
        askg-data:Entity-latentnet,
        askg-data:Entity-model,
        askg-data:Entity-module,
        askg-data:Entity-posteriornet,
        askg-data:Entity-prediction-net,
        askg-data:Entity-priornet,
        askg-data:Entity-refined_depth_image_d0i,
        askg-data:Entity-rgb-d_input_xi,
        askg-data:Entity-rgb_image_ii,
        askg-data:Entity-saliency_consensus_module,
        askg-data:Entity-saliency_feature_maps_sd_i,
        askg-data:Entity-saliency_map_prediction_pi,
        askg-data:Entity-saliencynet,
        askg-data:Entity-single_gt_saliency_map_yi,
        askg-data:Entity-stochastic_features_s_s_i,
        askg-data:Entity-xi_and_yi .

askg-data:Paper-fdb482b8f2df5642-Section-7-Paragraph-78-Sentence-782 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "We will introduce each module as follows."@en ;
    askg-onto:inSentence "We will introduce each module as follows."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-each_module,
        askg-data:Entity-modules .

askg-data:Paper-fdb482b8f2df5642-Section-8 a askg-onto:Section ;
    rdfs:label "Section 8"@en ;
    domo:Text "3.1. Probabilistic Rgb-D Saliency Model Via Cvae"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-811,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-812,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-814,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-815,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-817,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-818,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-82,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-83,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-84,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-85,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-86,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-88,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89 ;
    askg-onto:index "8"^^xsd:int ;
    askg-onto:level "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "The Conditional Variational Autoencoder (CVAE) modulates the prior as a Gaussian distribution with parameters conditioned on the input data X. There are three types of variables in the conditional generative model: conditioning variable X (RGB-D image pair in our setting), latent variable z, and output variable Y . For the latent variable z drawn from the Gaussian distribution Pθ(z|X), the output variable Y is generated from Pω(Y |*X, z*), then the posterior of z is formulated as Qφ(z|*X, Y* ). The loss of CVAE is defined as:"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81-Sentence-811,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81-Sentence-812,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81-Sentence-813,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81-Sentence-814 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81-Sentence-811 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "The Conditional Variational Autoencoder (CVAE) modulates the prior as a Gaussian distribution with parameters conditioned on the input data X."@en ;
    askg-onto:inSentence "The Conditional Variational Autoencoder (CVAE) modulates the prior as a Gaussian distribution with parameters conditioned on the input data X."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-conditional_variational_autoencoder_cvae,
        askg-data:Entity-gaussian_distribution,
        askg-data:Entity-input_data_x .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81-Sentence-812 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "There are three types of variables in the conditional generative model: conditioning variable X (RGB-D image pair in our setting), latent variable z, and output variable Y ."@en ;
    askg-onto:inSentence "There are three types of variables in the conditional generative model: conditioning variable X (RGB-D image pair in our setting), latent variable z, and output variable Y ."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-conditional_generative_model,
        askg-data:Entity-conditioning_variable_x,
        askg-data:Entity-latent_variable_z,
        askg-data:Entity-output_variable_y .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81-Sentence-813 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "For the latent variable z drawn from the Gaussian distribution Pθ(z|X), the output variable Y is generated from Pω(Y |*X, z*), then the posterior of z is formulated as Qφ(z|*X, Y* )."@en ;
    askg-onto:inSentence "For the latent variable z drawn from the Gaussian distribution Pθ(z|X), the output variable Y is generated from Pω(Y |*X, z*), then the posterior of z is formulated as Qφ(z|*X, Y* )."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gaussian_distribution_p%CE%B8zx,
        askg-data:Entity-latent_variable_z,
        askg-data:Entity-output_variable_y,
        askg-data:Entity-p%CF%89y_x_z,
        askg-data:Entity-posterior_of_z,
        askg-data:Entity-q%CF%86zx_y .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-81-Sentence-814 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "The loss of CVAE is defined as:"@en ;
    askg-onto:inSentence "The loss of CVAE is defined as:"^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-loss .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 10"@en ;
    domo:Text "SaliencyNet: We design SaliencyNet to produce a deterministic saliency feature map S dfrom the input RGB-D data, where the refined depth data comes from the Depth- CorrectionNet. We use VGG16 [48] as our encoder, and remove layers after the fifth pooling layer. To enlarge the receptive field, we follow DenseASPP [58] to obtain feature Figure 5. New label generation. The 1st row: we iteratively hide the predicted salient region, where no region is hidden in the first image. The 2nd row: the corresponding GT of the hidden image."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8101,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8102,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8103,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8104,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8105,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8106 ;
    askg-onto:index "10"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8101 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "SaliencyNet: We design SaliencyNet to produce a deterministic saliency feature map S dfrom the input RGB-D data, where the refined depth data comes from the Depth- CorrectionNet."@en ;
    askg-onto:inSentence "SaliencyNet: We design SaliencyNet to produce a deterministic saliency feature map S dfrom the input RGB-D data, where the refined depth data comes from the Depth- CorrectionNet."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth-correctionnet,
        askg-data:Entity-refined_depth_data,
        askg-data:Entity-rgb-d_data,
        askg-data:Entity-saliency_feature_map_s,
        askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8102 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "We use VGG16 [48] as our encoder, and remove layers after the fifth pooling layer."@en ;
    askg-onto:inSentence "We use VGG16 [48] as our encoder, and remove layers after the fifth pooling layer."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-encoder,
        askg-data:Entity-vgg16 .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8103 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "To enlarge the receptive field, we follow DenseASPP [58] to obtain feature Figure 5."@en ;
    askg-onto:inSentence "To enlarge the receptive field, we follow DenseASPP [58] to obtain feature Figure 5."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-denseaspp,
        askg-data:Entity-feature_figure_5 .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8104 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "New label generation."@en ;
    askg-onto:inSentence "New label generation."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-label_generation,
        askg-data:Entity-new_label_generation .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8105 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "The 1st row: we iteratively hide the predicted salient region, where no region is hidden in the first image."@en ;
    askg-onto:inSentence "The 1st row: we iteratively hide the predicted salient region, where no region is hidden in the first image."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-salient_region .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-810-Sentence-8106 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "The 2nd row: the corresponding GT of the hidden image."@en ;
    askg-onto:inSentence "The 2nd row: the corresponding GT of the hidden image."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt,
        askg-data:Entity-hidden_image .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-811 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 11"@en ;
    domo:Text "![3_image_1.png](3_image_1.png)"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-811-Sentence-8111 ;
    askg-onto:index "11"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-811-Sentence-8111 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "![3_image_1.png](3_image_1.png)"@en ;
    askg-onto:inSentence "![3_image_1.png](3_image_1.png)"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-analytical_technique,
        askg-data:Entity-biochemistry,
        askg-data:Entity-functional_mri,
        askg-data:Entity-imaging_technology,
        askg-data:Entity-mass_spectrometry,
        askg-data:Entity-neuroscience,
        askg-data:Entity-university_of_california_san_francisco .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-812 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 12"@en ;
    domo:Text "Figure 6. SaliencyNet, where \"S1\" represents the first stage of the VGG16 network, \"daspp\" is the DenseASPP module [58]."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-812-Sentence-8121,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-812-Sentence-8122 ;
    askg-onto:index "12"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-812-Sentence-8121 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Figure 6."@en ;
    askg-onto:inSentence "Figure 6."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-data,
        askg-data:Entity-figure_6 .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-812-Sentence-8122 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "SaliencyNet, where \"S1\" represents the first stage of the VGG16 network, \"daspp\" is the DenseASPP module [58]."@en ;
    askg-onto:inSentence "SaliencyNet, where \"S1\" represents the first stage of the VGG16 network, \"daspp\" is the DenseASPP module [58]."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-denseaspp_module,
        askg-data:Entity-first_stage_of_the_vgg16_network,
        askg-data:Entity-neural_network_architecture,
        askg-data:Entity-s1,
        askg-data:Entity-saliencynet,
        askg-data:Entity-vgg16_network .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 13"@en ;
    domo:Text "map with the receptive field of the whole image on each stage of the VGG16 network. We then concatenate those feature maps and feed it to another convolutional layer to obtain S d. The detail of the SaliencyNet is illustrated in Fig. 6, where \"c1 M\" represents convolutional layer of kernel size 1 × 1, and M is channel size of S d."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813-Sentence-8131,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813-Sentence-8132,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813-Sentence-8133,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813-Sentence-8134 ;
    askg-onto:index "13"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813-Sentence-8131 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "map with the receptive field of the whole image on each stage of the VGG16 network."@en ;
    askg-onto:inSentence "map with the receptive field of the whole image on each stage of the VGG16 network."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-receptive_field,
        askg-data:Entity-vgg16_network .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813-Sentence-8132 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "We then concatenate those feature maps and feed it to another convolutional layer to obtain S d."@en ;
    askg-onto:inSentence "We then concatenate those feature maps and feed it to another convolutional layer to obtain S d."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-another_convolutional_layer,
        askg-data:Entity-convolutional_layer,
        askg-data:Entity-feature_maps,
        askg-data:Entity-s_d .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813-Sentence-8133 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "The detail of the SaliencyNet is illustrated in Fig."@en ;
    askg-onto:inSentence "The detail of the SaliencyNet is illustrated in Fig."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-fig,
        askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-813-Sentence-8134 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "6, where \"c1 M\" represents convolutional layer of kernel size 1 × 1, and M is channel size of S d."@en ;
    askg-onto:inSentence "6, where \"c1 M\" represents convolutional layer of kernel size 1 × 1, and M is channel size of S d."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-c1_m,
        askg-data:Entity-convolutional_layer_of_kernel_size_1__1 .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-814 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 14"@en ;
    domo:Text "Feature Expanding: Statistics (z ∼ N (µ, diag(σ 2)) in particular) from the LatentNet (PriorNet during testing as shown in Fig. 3 \"Sampling\", or PosteriorNet during training in Fig. 2) form the input to the Feature Expanding module."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-814-Sentence-8141,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-814-Sentence-8142,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-814-Sentence-8143 ;
    askg-onto:index "14"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-814-Sentence-8141 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Feature Expanding: Statistics (z ∼ N (µ, diag(σ 2)) in particular) from the LatentNet (PriorNet during testing as shown in Fig."@en ;
    askg-onto:inSentence "Feature Expanding: Statistics (z ∼ N (µ, diag(σ 2)) in particular) from the LatentNet (PriorNet during testing as shown in Fig."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-latentnet,
        askg-data:Entity-model,
        askg-data:Entity-normal_distribution,
        askg-data:Entity-priornet,
        askg-data:Entity-statistics .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-814-Sentence-8142 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "3 \"Sampling\", or PosteriorNet during training in Fig."@en ;
    askg-onto:inSentence "3 \"Sampling\", or PosteriorNet during training in Fig."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-posteriornet,
        askg-data:Entity-sampling .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-814-Sentence-8143 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "2) form the input to the Feature Expanding module."@en ;
    askg-onto:inSentence "2) form the input to the Feature Expanding module."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-feature_expanding_module,
        askg-data:Entity-input .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-815 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 15"@en ;
    domo:Text "Given a pair of (µ k, σk) in each position of the K dimensional vector, we obtain latent vector z k = σ k + µ k, where ∈ N (0, I). To fuse with deterministic feature S d, we expand z kto feature map of the same spatial size as S d by defining as two-dimensional Gaussian noise map. With k = 1*, ..., K*, we can obtain a K (size of the latent space) channel stochastic feature S srepresenting labeling variants."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-815-Sentence-8151,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-815-Sentence-8152,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-815-Sentence-8153 ;
    askg-onto:index "15"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-815-Sentence-8151 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Given a pair of (µ k, σk) in each position of the K dimensional vector, we obtain latent vector z k = σ k + µ k, where ∈ N (0, I)."@en ;
    askg-onto:inSentence "Given a pair of (µ k, σk) in each position of the K dimensional vector, we obtain latent vector z k = σ k + µ k, where ∈ N (0, I)."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%C2%B5_k_%CF%83k,
        askg-data:Entity-%CF%83_k__%C2%B5_k,
        askg-data:Entity-k_dimensional_vector,
        askg-data:Entity-z_k .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-815-Sentence-8152 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "To fuse with deterministic feature S d, we expand z kto feature map of the same spatial size as S d by defining as two-dimensional Gaussian noise map."@en ;
    askg-onto:inSentence "To fuse with deterministic feature S d, we expand z kto feature map of the same spatial size as S d by defining as two-dimensional Gaussian noise map."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deterministic_feature_s_d,
        askg-data:Entity-feature_map,
        askg-data:Entity-two-dimensional_gaussian_noise_map .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-815-Sentence-8153 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "With k = 1*, ..., K*, we can obtain a K (size of the latent space) channel stochastic feature S srepresenting labeling variants."@en ;
    askg-onto:inSentence "With k = 1*, ..., K*, we can obtain a K (size of the latent space) channel stochastic feature S srepresenting labeling variants."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-k,
        askg-data:Entity-stochastic_feature_s .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 16"@en ;
    domo:Text "PredictionNet: The LatentNet produces stochastic features S srepresenting labeling variants, while the SaliencyNet outputs deterministic saliency features S d of input X. We propose the PredictionNet, as shown in Fig. 2 to fuse features from mentioned branches. A naive concatenation of S sand S d may lead the network to learn only from the deterministic features, thus fail to model labeling variants. Inspired by [47], we mix S sand S dchannel-wise; thus, the network cannot distinguish between features of the deterministic branch and the probabilistic branch. We concatenate S dand S sto form a K + M channel feature map S sd."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8161,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8162,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8163,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8164,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8165,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8166 ;
    askg-onto:index "16"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8161 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "PredictionNet: The LatentNet produces stochastic features S srepresenting labeling variants, while the SaliencyNet outputs deterministic saliency features S d of input X."@en ;
    askg-onto:inSentence "PredictionNet: The LatentNet produces stochastic features S srepresenting labeling variants, while the SaliencyNet outputs deterministic saliency features S d of input X."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deterministic_saliency_features_s_d,
        askg-data:Entity-input_x,
        askg-data:Entity-predictionnet,
        askg-data:Entity-saliencynet,
        askg-data:Entity-stochastic_features_s .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8162 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "We propose the PredictionNet, as shown in Fig."@en ;
    askg-onto:inSentence "We propose the PredictionNet, as shown in Fig."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-model,
        askg-data:Entity-predictionnet .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8163 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "2 to fuse features from mentioned branches."@en ;
    askg-onto:inSentence "2 to fuse features from mentioned branches."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-branches,
        askg-data:Entity-features .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8164 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "A naive concatenation of S sand S d may lead the network to learn only from the deterministic features, thus fail to model labeling variants."@en ;
    askg-onto:inSentence "A naive concatenation of S sand S d may lead the network to learn only from the deterministic features, thus fail to model labeling variants."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-labeling_variants,
        askg-data:Entity-s_sand,
        askg-data:Entity-the_network .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8165 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "Inspired by [47], we mix S sand S dchannel-wise; thus, the network cannot distinguish between features of the deterministic branch and the probabilistic branch."@en ;
    askg-onto:inSentence "Inspired by [47], we mix S sand S dchannel-wise; thus, the network cannot distinguish between features of the deterministic branch and the probabilistic branch."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-s,
        askg-data:Entity-s_dchannel .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-816-Sentence-8166 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "We concatenate S dand S sto form a K + M channel feature map S sd."@en ;
    askg-onto:inSentence "We concatenate S dand S sto form a K + M channel feature map S sd."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-k__m_channel_feature_map_s_sd,
        askg-data:Entity-s_dand_s_s .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-817 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 17"@en ;
    domo:Text "We define K + M dimensional variable r (a learnable parameter) representing possible ranking of 1, 2*, ..., K* + M, and then S sd is mixed channel-wisely according to r to obtain the mixed feature S msd. Three 1×1 convolutional layers with output channel sizes of *K, K/*2, 1, are included in"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-817-Sentence-8171,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-817-Sentence-8172 ;
    askg-onto:index "17"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-817-Sentence-8171 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "We define K + M dimensional variable r (a learnable parameter) representing possible ranking of 1, 2*, ..., K* + M, and then S sd is mixed channel-wisely according to r to obtain the mixed feature S msd."@en ;
    askg-onto:inSentence "We define K + M dimensional variable r (a learnable parameter) representing possible ranking of 1, 2*, ..., K* + M, and then S sd is mixed channel-wisely according to r to obtain the mixed feature S msd."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-k__m_dimensional_variable_r,
        askg-data:Entity-mixed_feature_s_msd,
        askg-data:Entity-possible_ranking_of_1_2__k__m,
        askg-data:Entity-s_sd .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-817-Sentence-8172 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Three 1×1 convolutional layers with output channel sizes of *K, K/*2, 1, are included in"@en ;
    askg-onto:inSentence "Three 1×1 convolutional layers with output channel sizes of *K, K/*2, 1, are included in"^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-output_channel_sizes_of_k_k2_1,
        askg-data:Entity-three_11_convolutional_layers .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-818 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 18"@en ;
    domo:Text "3We found that usually after three times of hiding, there exists no salient objects in the hidden image. the PredictionNet to map S msd to a single channel saliency map P. During testing, with multiple stochastic features S s, we can obtain multiple predictions by sampling S sfrom the LatentNet N (µprior*, diag*(σ 2prior)) multiple times."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-818-Sentence-8181,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-818-Sentence-8182,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-818-Sentence-8183 ;
    askg-onto:index "18"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-818-Sentence-8181 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "3We found that usually after three times of hiding, there exists no salient objects in the hidden image."@en ;
    askg-onto:inSentence "3We found that usually after three times of hiding, there exists no salient objects in the hidden image."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-salient_objects .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-818-Sentence-8182 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "the PredictionNet to map S msd to a single channel saliency map P."@en ;
    askg-onto:inSentence "the PredictionNet to map S msd to a single channel saliency map P."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-predictionnet,
        askg-data:Entity-single_channel_saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-818-Sentence-8183 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "During testing, with multiple stochastic features S s, we can obtain multiple predictions by sampling S sfrom the LatentNet N (µprior*, diag*(σ 2prior)) multiple times."@en ;
    askg-onto:inSentence "During testing, with multiple stochastic features S s, we can obtain multiple predictions by sampling S sfrom the LatentNet N (µprior*, diag*(σ 2prior)) multiple times."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-latentnet_n_%C2%B5prior_diag%CF%83_2prior,
        askg-data:Entity-s_s .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-82 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "$$\\begin{array}{c}{\\cal L}_{\\rm CVAE}=E_{z\\sim Q_{\\phi}(z|X,Y)}[-\\log P_{\\omega}(Y|X,z)]\\\\ +D_{KL}(Q_{\\phi}(z|X,Y)||P_{\\theta}(z|X)),\\end{array}\\tag{1}$$"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-82-Sentence-821 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-82-Sentence-821 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "$$\\begin{array}{c}{\\cal L}_{\\rm CVAE}=E_{z\\sim Q_{\\phi}(z|X,Y)}[-\\log P_{\\omega}(Y|X,z)]\\\\ +D_{KL}(Q_{\\phi}(z|X,Y)||P_{\\theta}(z|X)),\\end{array}\\tag{1}$$"@en ;
    askg-onto:inSentence "$$\\begin{array}{c}{\\cal L}_{\\rm CVAE}=E_{z\\sim Q_{\\phi}(z|X,Y)}[-\\log P_{\\omega}(Y|X,z)]\\\\ +D_{KL}(Q_{\\phi}(z|X,Y)||P_{\\theta}(z|X)),\\end{array}\\tag{1}$$"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%0A________%0A________cal_l_%0Dm_cvae,
        askg-data:Entity-d_klq_%09extphizxyp_%09hetazx,
        askg-data:Entity-e_z%09extsim_q_%09extphizxy,
        askg-data:Entity-model,
        askg-data:Entity-p_%09extomegayxz .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-83 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 3"@en ;
    domo:Text "where Pω(Y |*X, z*) is the likelihood of P(Y ) given latent variable z and conditioning variable X, the Kullback- Leibler Divergence DKL(Qφ(z|*X, Y* )||Pθ(z|X)) works as a regularization loss to reduce the gap between the prior Pθ(z|X) and the auxiliary posterior Qφ(z|*X, Y* ). In this way, CVAE aims to model the log likelihood P(Y ) under encoding error DKL(Qφ(z|*X, Y* )||Pθ(z|X)). Following the standard practice in conventional CVAE [50], we design a CVAE-based RGB-D saliency detection network, and describe each component of our model in the following."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-83-Sentence-831,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-83-Sentence-832,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-83-Sentence-833 ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-83-Sentence-831 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "where Pω(Y |*X, z*) is the likelihood of P(Y ) given latent variable z and conditioning variable X, the Kullback- Leibler Divergence DKL(Qφ(z|*X, Y* )||Pθ(z|X)) works as a regularization loss to reduce the gap between the prior Pθ(z|X) and the auxiliary posterior Qφ(z|*X, Y* )."@en ;
    askg-onto:inSentence "where Pω(Y |*X, z*) is the likelihood of P(Y ) given latent variable z and conditioning variable X, the Kullback- Leibler Divergence DKL(Qφ(z|*X, Y* )||Pθ(z|X)) works as a regularization loss to reduce the gap between the prior Pθ(z|X) and the auxiliary posterior Qφ(z|*X, Y* )."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-auxiliary_posterior_q%CF%86zx_y,
        askg-data:Entity-dklq%CF%86zx_y,
        askg-data:Entity-p%CE%B8zx,
        askg-data:Entity-p%CF%89y_x_z,
        askg-data:Entity-py,
        askg-data:Entity-regularization_loss .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-83-Sentence-832 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "In this way, CVAE aims to model the log likelihood P(Y ) under encoding error DKL(Qφ(z|*X, Y* )||Pθ(z|X))."@en ;
    askg-onto:inSentence "In this way, CVAE aims to model the log likelihood P(Y ) under encoding error DKL(Qφ(z|*X, Y* )||Pθ(z|X))."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-dkl,
        askg-data:Entity-encoding_error,
        askg-data:Entity-encoding_error_dklq%CF%86zx_y_p%CE%B8zx,
        askg-data:Entity-log_likelihood_py,
        askg-data:Entity-p%CE%B8zx,
        askg-data:Entity-py,
        askg-data:Entity-q%CF%86zx_y_ .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-83-Sentence-833 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Following the standard practice in conventional CVAE [50], we design a CVAE-based RGB-D saliency detection network, and describe each component of our model in the following."@en ;
    askg-onto:inSentence "Following the standard practice in conventional CVAE [50], we design a CVAE-based RGB-D saliency detection network, and describe each component of our model in the following."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-model,
        askg-data:Entity-rgb-d_saliency_detection_network .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-84 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 4"@en ;
    domo:Text "LatentNet: We define Pθ(z|X) as PriorNet that maps the input RGB-D image pair X to a low-dimensional latent feature space, where θ is the parameter set of PriorNet. With the same network structure and provided GT saliency map Y , we define Qφ(z|*X, Y* ) as PosteriorNet, with φ being the posterior net parameter set. In the LatentNet (Prior- Net and PosteriorNet), we use five convolutional layers to map the input RGB-D image X (or concatenation of X and Y for the PosteriorNet) to the latent Gaussian variable z ∼ N (µ, diag(σ 2)), where µ, σ ∈ R K, representing the"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-84-Sentence-841,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-84-Sentence-842,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-84-Sentence-843 ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-84-Sentence-841 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "LatentNet: We define Pθ(z|X) as PriorNet that maps the input RGB-D image pair X to a low-dimensional latent feature space, where θ is the parameter set of PriorNet."@en ;
    askg-onto:inSentence "LatentNet: We define Pθ(z|X) as PriorNet that maps the input RGB-D image pair X to a low-dimensional latent feature space, where θ is the parameter set of PriorNet."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-input_rgb-d_image_pair_x,
        askg-data:Entity-p%CE%B8zx,
        askg-data:Entity-parameter_set_%CE%B8,
        askg-data:Entity-priornet .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-84-Sentence-842 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "With the same network structure and provided GT saliency map Y , we define Qφ(z|*X, Y* ) as PosteriorNet, with φ being the posterior net parameter set."@en ;
    askg-onto:inSentence "With the same network structure and provided GT saliency map Y , we define Qφ(z|*X, Y* ) as PosteriorNet, with φ being the posterior net parameter set."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-posterior_net_parameter_set,
        askg-data:Entity-posteriornet,
        askg-data:Entity-q%CF%86zx_y_ .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-84-Sentence-843 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "In the LatentNet (Prior- Net and PosteriorNet), we use five convolutional layers to map the input RGB-D image X (or concatenation of X and Y for the PosteriorNet) to the latent Gaussian variable z ∼ N (µ, diag(σ 2)), where µ, σ ∈ R K, representing the"@en ;
    askg-onto:inSentence "In the LatentNet (Prior- Net and PosteriorNet), we use five convolutional layers to map the input RGB-D image X (or concatenation of X and Y for the PosteriorNet) to the latent Gaussian variable z ∼ N (µ, diag(σ 2)), where µ, σ ∈ R K, representing the"^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%C2%B5,
        askg-data:Entity-%CF%83,
        askg-data:Entity-concept,
        askg-data:Entity-k,
        askg-data:Entity-latent_gaussian_variable_z,
        askg-data:Entity-latentnet,
        askg-data:Entity-model,
        askg-data:Entity-rgb-d_image_x,
        askg-data:Entity-variable .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-85 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 5"@en ;
    domo:Text "![3_image_0.png](3_image_0.png)"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-85-Sentence-851 ;
    askg-onto:index "5"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-85-Sentence-851 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "![3_image_0.png](3_image_0.png)"@en ;
    askg-onto:inSentence "![3_image_0.png](3_image_0.png)"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-city,
        askg-data:Entity-company,
        askg-data:Entity-data,
        askg-data:Entity-dataset,
        askg-data:Entity-device,
        askg-data:Entity-finding,
        askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-publication,
        askg-data:Entity-research_area,
        askg-data:Entity-research_group,
        askg-data:Entity-software,
        askg-data:Entity-study,
        askg-data:Entity-technology,
        askg-data:Entity-tool,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-86 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 6"@en ;
    domo:Text "as shown in Fig. 4."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-86-Sentence-861,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-86-Sentence-862 ;
    askg-onto:index "6"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-86-Sentence-861 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "as shown in Fig."@en ;
    askg-onto:inSentence "as shown in Fig."^^xsd:string ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-86-Sentence-862 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "4."@en ;
    askg-onto:inSentence "4."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-article,
        askg-data:Entity-corpus,
        askg-data:Entity-dataset,
        askg-data:Entity-framework,
        askg-data:Entity-institution,
        askg-data:Entity-method,
        askg-data:Entity-model,
        askg-data:Entity-organization,
        askg-data:Entity-publication,
        askg-data:Entity-research_group,
        askg-data:Entity-software,
        askg-data:Entity-technology,
        askg-data:Entity-tool,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 7"@en ;
    domo:Text "Let us define parameter set of PriorNet and PosteriorNet as (µprior, σprior) and (µpost, σpost) respectively. The KL- Divergence in Eq. (1) is used to measure the distribution mismatch between the prior net Pθ(z|X) and posterior net Qφ(z|*X, Y* ), or how much information is lost when using Qφ(z|*X, Y* ) to represent Pθ(z|X). Typical using of CVAE involves multiple versions of ground truth Y [32] to produce informative z ∈ R K, with each position in z represents possible labeling variants or factors that may cause diverse saliency annotations. As we have only one version of GT, directly training with the provided single GT may fail to produce diverse predictions as the network will simply fit the provided annotation Y . Generate Multiple Predictions: To produce diverse and accurate predictions, we propose an iterative hiding technique inspired by [49] following the orientation shifting theory [26] to generate more annotations as shown in Fig. 5. We iteratively hide the salient region in the RGB image with mean of the training dataset. The RGB image and its corresponding GT are set as the starting point of the \"new label generation\" technique. We first hide the ground truth salient object in the RGB image, and feed the modified image to an existing RGB saliency detection model [42] to produce a saliency map and treat it as one candidate annotation. We repeat salient object hiding technique three times for each training image3to obtain four different sets of annotations in total (including the provided GT), and we term this dataset as \"AugedGT\", which is our training dataset."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-871,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-8710,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-8711,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-872,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-873,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-874,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-875,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-876,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-877,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-878,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-879 ;
    askg-onto:index "7"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-871 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Let us define parameter set of PriorNet and PosteriorNet as (µprior, σprior) and (µpost, σpost) respectively."@en ;
    askg-onto:inSentence "Let us define parameter set of PriorNet and PosteriorNet as (µprior, σprior) and (µpost, σpost) respectively."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%C2%B5post_%CF%83post,
        askg-data:Entity-%C2%B5prior_%CF%83prior,
        askg-data:Entity-posteriornet,
        askg-data:Entity-priornet .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-8710 a askg-onto:Sentence ;
    rdfs:label "Sentence 10"@en ;
    domo:Text "We first hide the ground truth salient object in the RGB image, and feed the modified image to an existing RGB saliency detection model [42] to produce a saliency map and treat it as one candidate annotation."@en ;
    askg-onto:inSentence "We first hide the ground truth salient object in the RGB image, and feed the modified image to an existing RGB saliency detection model [42] to produce a saliency map and treat it as one candidate annotation."^^xsd:string ;
    askg-onto:index "10"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-existing_rgb_saliency_detection_model,
        askg-data:Entity-saliency_map .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-8711 a askg-onto:Sentence ;
    rdfs:label "Sentence 11"@en ;
    domo:Text "We repeat salient object hiding technique three times for each training image3to obtain four different sets of annotations in total (including the provided GT), and we term this dataset as \"AugedGT\", which is our training dataset."@en ;
    askg-onto:inSentence "We repeat salient object hiding technique three times for each training image3to obtain four different sets of annotations in total (including the provided GT), and we term this dataset as \"AugedGT\", which is our training dataset."^^xsd:string ;
    askg-onto:index "11"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-augedgt,
        askg-data:Entity-salient_object_hiding_technique,
        askg-data:Entity-training_dataset,
        askg-data:Entity-training_image .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-872 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "The KL- Divergence in Eq."@en ;
    askg-onto:inSentence "The KL- Divergence in Eq."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-eq,
        askg-data:Entity-kl-divergence .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-873 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "(1) is used to measure the distribution mismatch between the prior net Pθ(z|X) and posterior net Qφ(z|*X, Y* ), or how much information is lost when using Qφ(z|*X, Y* ) to represent Pθ(z|X)."@en ;
    askg-onto:inSentence "(1) is used to measure the distribution mismatch between the prior net Pθ(z|X) and posterior net Qφ(z|*X, Y* ), or how much information is lost when using Qφ(z|*X, Y* ) to represent Pθ(z|X)."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-p%CE%B8zx,
        askg-data:Entity-q%CF%86zx_y .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-874 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "Typical using of CVAE involves multiple versions of ground truth Y [32] to produce informative z ∈ R K, with each position in z represents possible labeling variants or factors that may cause diverse saliency annotations."@en ;
    askg-onto:inSentence "Typical using of CVAE involves multiple versions of ground truth Y [32] to produce informative z ∈ R K, with each position in z represents possible labeling variants or factors that may cause diverse saliency annotations."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-factors,
        askg-data:Entity-ground_truth_y,
        askg-data:Entity-labeling_variants,
        askg-data:Entity-position_in_z,
        askg-data:Entity-saliency_annotations .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-875 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "As we have only one version of GT, directly training with the provided single GT may fail to produce diverse predictions as the network will simply fit the provided annotation Y ."@en ;
    askg-onto:inSentence "As we have only one version of GT, directly training with the provided single GT may fail to produce diverse predictions as the network will simply fit the provided annotation Y ."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt,
        askg-data:Entity-network,
        askg-data:Entity-provided_annotation_y,
        askg-data:Entity-single_gt .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-876 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "Generate Multiple Predictions: To produce diverse and accurate predictions, we propose an iterative hiding technique inspired by [49] following the orientation shifting theory [26] to generate more annotations as shown in Fig."@en ;
    askg-onto:inSentence "Generate Multiple Predictions: To produce diverse and accurate predictions, we propose an iterative hiding technique inspired by [49] following the orientation shifting theory [26] to generate more annotations as shown in Fig."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-iterative_hiding_technique,
        askg-data:Entity-orientation_shifting_theory .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-877 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "5."@en ;
    askg-onto:inSentence "5."^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-part,
        askg-data:Entity-triple .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-878 a askg-onto:Sentence ;
    rdfs:label "Sentence 8"@en ;
    domo:Text "We iteratively hide the salient region in the RGB image with mean of the training dataset."@en ;
    askg-onto:inSentence "We iteratively hide the salient region in the RGB image with mean of the training dataset."^^xsd:string ;
    askg-onto:index "8"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-salient_region,
        askg-data:Entity-training_dataset .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-87-Sentence-879 a askg-onto:Sentence ;
    rdfs:label "Sentence 9"@en ;
    domo:Text "The RGB image and its corresponding GT are set as the starting point of the \"new label generation\" technique."@en ;
    askg-onto:inSentence "The RGB image and its corresponding GT are set as the starting point of the \"new label generation\" technique."^^xsd:string ;
    askg-onto:index "9"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gt,
        askg-data:Entity-new_label_generation_technique,
        askg-data:Entity-rgb_image .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-88 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 8"@en ;
    domo:Text "During training, different annotations (as shown in Fig."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-88-Sentence-881 ;
    askg-onto:index "8"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-88-Sentence-881 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "During training, different annotations (as shown in Fig."@en ;
    askg-onto:inSentence "During training, different annotations (as shown in Fig."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-annotations,
        askg-data:Entity-training .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 9"@en ;
    domo:Text "5) in Qφ(z|*X, Y* ) can force the PriorNet Pθ(z|X) to encode labeling variants of a given input X. As we have already obtained diverse annotations with the proposed hiding technique, we are expecting the network to produce diverse predictions for images with complicated context. During testing, we can obtain one stochastic feature S s(input of the \"PredictionNet\") of channel size K each time we sample as shown in Fig. 3."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89-Sentence-891,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89-Sentence-892,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89-Sentence-893,
        askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89-Sentence-894 ;
    askg-onto:index "9"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89-Sentence-891 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "5) in Qφ(z|*X, Y* ) can force the PriorNet Pθ(z|X) to encode labeling variants of a given input X."@en ;
    askg-onto:inSentence "5) in Qφ(z|*X, Y* ) can force the PriorNet Pθ(z|X) to encode labeling variants of a given input X."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-input,
        askg-data:Entity-input_x,
        askg-data:Entity-labeling_variants,
        askg-data:Entity-priornet .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89-Sentence-892 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "As we have already obtained diverse annotations with the proposed hiding technique, we are expecting the network to produce diverse predictions for images with complicated context."@en ;
    askg-onto:inSentence "As we have already obtained diverse annotations with the proposed hiding technique, we are expecting the network to produce diverse predictions for images with complicated context."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-diverse_predictions,
        askg-data:Entity-hiding_technique,
        askg-data:Entity-images_with_complicated_context,
        askg-data:Entity-network .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89-Sentence-893 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "During testing, we can obtain one stochastic feature S s(input of the \"PredictionNet\") of channel size K each time we sample as shown in Fig."@en ;
    askg-onto:inSentence "During testing, we can obtain one stochastic feature S s(input of the \"PredictionNet\") of channel size K each time we sample as shown in Fig."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-channel_size_k,
        askg-data:Entity-feature,
        askg-data:Entity-model,
        askg-data:Entity-predictionnet,
        askg-data:Entity-size,
        askg-data:Entity-stochastic_feature_s .

askg-data:Paper-fdb482b8f2df5642-Section-8-Paragraph-89-Sentence-894 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "3."@en ;
    askg-onto:inSentence "3."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-company,
        askg-data:Entity-dataset,
        askg-data:Entity-institution,
        askg-data:Entity-method,
        askg-data:Entity-publication,
        askg-data:Entity-research_area,
        askg-data:Entity-research_group,
        askg-data:Entity-scientist,
        askg-data:Entity-technique,
        askg-data:Entity-technology,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-9 a askg-onto:Section ;
    rdfs:label "Section 9"@en ;
    domo:Text "3.2. Depthcorrectionnet"@en ;
    askg-onto:hasParagraph askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-910,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-92,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-93,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-94,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-96,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-97,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99 ;
    askg-onto:index "9"^^xsd:int ;
    askg-onto:level "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 1"@en ;
    domo:Text "Two main approaches are employed to acquire depth data for RGB-D saliency detection: through depth sensors such as Microsoft Kinect, e.g., DES [8], and NLPR [41] datasets; or computing depth from stereo cameras, examples of such datasets are SSB [40] and NJU2K [28]. Regardless of the capturing technique, noise is inherent in the depth data. We propose a semantic guided depth correction network to produce refined depth information as shown in Fig. 2, termed as \"DepthCorrectionNet\". The encoder part of the DepthCorrectionNet is the same as the \"SaliencyNet\", while the decoder part is composed of four sequential convolutional layers and bilinear upsampling operation."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-911,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-912,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-913,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-914,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-915 ;
    askg-onto:index "1"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-911 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Two main approaches are employed to acquire depth data for RGB-D saliency detection: through depth sensors such as Microsoft Kinect, e.g., DES [8], and NLPR [41] datasets; or computing depth from stereo cameras, examples of such datasets are SSB [40] and NJU2K [28]."@en ;
    askg-onto:inSentence "Two main approaches are employed to acquire depth data for RGB-D saliency detection: through depth sensors such as Microsoft Kinect, e.g., DES [8], and NLPR [41] datasets; or computing depth from stereo cameras, examples of such datasets are SSB [40] and NJU2K [28]."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-computing_depth,
        askg-data:Entity-datasets,
        askg-data:Entity-depth_data,
        askg-data:Entity-depth_sensors,
        askg-data:Entity-des,
        askg-data:Entity-microsoft_kinect,
        askg-data:Entity-nju2k,
        askg-data:Entity-nlpr,
        askg-data:Entity-rgb-d_saliency_detection,
        askg-data:Entity-ssb,
        askg-data:Entity-stereo_cameras .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-912 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Regardless of the capturing technique, noise is inherent in the depth data."@en ;
    askg-onto:inSentence "Regardless of the capturing technique, noise is inherent in the depth data."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth_data,
        askg-data:Entity-noise .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-913 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "We propose a semantic guided depth correction network to produce refined depth information as shown in Fig."@en ;
    askg-onto:inSentence "We propose a semantic guided depth correction network to produce refined depth information as shown in Fig."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-refined_depth_information,
        askg-data:Entity-semantic_guided_depth_correction_network .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-914 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "2, termed as \"DepthCorrectionNet\"."@en ;
    askg-onto:inSentence "2, termed as \"DepthCorrectionNet\"."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depthcorrectionnet,
        askg-data:Entity-model .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-91-Sentence-915 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "The encoder part of the DepthCorrectionNet is the same as the \"SaliencyNet\", while the decoder part is composed of four sequential convolutional layers and bilinear upsampling operation."@en ;
    askg-onto:inSentence "The encoder part of the DepthCorrectionNet is the same as the \"SaliencyNet\", while the decoder part is composed of four sequential convolutional layers and bilinear upsampling operation."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-bilinear_upsampling_operation,
        askg-data:Entity-decoder_part,
        askg-data:Entity-depthcorrectionnet,
        askg-data:Entity-four_sequential_convolutional_layers,
        askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-910 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 10"@en ;
    domo:Text "$$P_{g}^{mjv}(u,v)=\\frac{\\sum_{c=1}^{C}\\mathbf{1}^{c}(u,v)}{C}\\sum_{c=1}^{C}(P_{b}^{c}(u,v)\\}\\times\\mathbf{1}^{c}(u,v)).\\tag{4}$$"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-910-Sentence-9101 ;
    askg-onto:index "10"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-910-Sentence-9101 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "$$P_{g}^{mjv}(u,v)=\\frac{\\sum_{c=1}^{C}\\mathbf{1}^{c}(u,v)}{C}\\sum_{c=1}^{C}(P_{b}^{c}(u,v)\\}\\times\\mathbf{1}^{c}(u,v)).\\tag{4}$$"@en ;
    askg-onto:inSentence "$$P_{g}^{mjv}(u,v)=\\frac{\\sum_{c=1}^{C}\\mathbf{1}^{c}(u,v)}{C}\\sum_{c=1}^{C}(P_{b}^{c}(u,v)\\}\\times\\mathbf{1}^{c}(u,v)).\\tag{4}$$"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-model,
        askg-data:Entity-p_bc,
        askg-data:Entity-p_gmjv .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-92 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 2"@en ;
    domo:Text "We assume that edges of the depth map should be aligned with edges of the RGB image. We adopt the boundary IOU loss [39] as a regularizer for DepthCorrectionNet to achieve a refined depth, which is guided by intensity of the RGB image. The full loss for DepthCorrectionNet is defined as:"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-92-Sentence-921,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-92-Sentence-922,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-92-Sentence-923 ;
    askg-onto:index "2"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-92-Sentence-921 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "We assume that edges of the depth map should be aligned with edges of the RGB image."@en ;
    askg-onto:inSentence "We assume that edges of the depth map should be aligned with edges of the RGB image."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth_map,
        askg-data:Entity-rgb_image .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-92-Sentence-922 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "We adopt the boundary IOU loss [39] as a regularizer for DepthCorrectionNet to achieve a refined depth, which is guided by intensity of the RGB image."@en ;
    askg-onto:inSentence "We adopt the boundary IOU loss [39] as a regularizer for DepthCorrectionNet to achieve a refined depth, which is guided by intensity of the RGB image."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-boundary_iou_loss,
        askg-data:Entity-depthcorrectionnet .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-92-Sentence-923 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "The full loss for DepthCorrectionNet is defined as:"@en ;
    askg-onto:inSentence "The full loss for DepthCorrectionNet is defined as:"^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depthcorrectionnet,
        askg-data:Entity-full_loss .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-93 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 3"@en ;
    domo:Text "$${\\mathcal{L}}_{\\mathrm{{Depth}}}={\\mathcal{L}}_{s l}+{\\mathcal{L}}_{\\mathrm{{Loub}}},$$ LDepth = Lsl + LIoub, (2) where Lsl is the smooth `1 loss between the refined depth D0and the raw depth D, L*ioub* is the boundary IOU loss between the refined depth D0and intensity Ig of the RGB image I. Given the predicted depth map D0and intensity of RGB image Ig, we follow [39] to compute the first-order derivatives of D0and Ig. Subsequently, we calculate the magnitude gD0and gI of the gradients of D0and Ig, and define the boundary IOU loss as:"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-93-Sentence-931,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-93-Sentence-932,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-93-Sentence-933 ;
    askg-onto:index "3"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-93-Sentence-931 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "$${\\mathcal{L}}_{\\mathrm{{Depth}}}={\\mathcal{L}}_{s l}+{\\mathcal{L}}_{\\mathrm{{Loub}}},$$ LDepth = Lsl + LIoub, (2) where Lsl is the smooth `1 loss between the refined depth D0and the raw depth D, L*ioub* is the boundary IOU loss between the refined depth D0and intensity Ig of the RGB image I."@en ;
    askg-onto:inSentence "$${\\mathcal{L}}_{\\mathrm{{Depth}}}={\\mathcal{L}}_{s l}+{\\mathcal{L}}_{\\mathrm{{Loub}}},$$ LDepth = Lsl + LIoub, (2) where Lsl is the smooth `1 loss between the refined depth D0and the raw depth D, L*ioub* is the boundary IOU loss between the refined depth D0and intensity Ig of the RGB image I."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-boundary_iou_loss,
        askg-data:Entity-d,
        askg-data:Entity-d0,
        askg-data:Entity-lioub,
        askg-data:Entity-lsl,
        askg-data:Entity-smooth_1_loss .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-93-Sentence-932 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Given the predicted depth map D0and intensity of RGB image Ig, we follow [39] to compute the first-order derivatives of D0and Ig."@en ;
    askg-onto:inSentence "Given the predicted depth map D0and intensity of RGB image Ig, we follow [39] to compute the first-order derivatives of D0and Ig."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-depth_map_d0,
        askg-data:Entity-intensity_of_rgb_image_ig,
        askg-data:Entity-rgb_image_ig .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-93-Sentence-933 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Subsequently, we calculate the magnitude gD0and gI of the gradients of D0and Ig, and define the boundary IOU loss as:"@en ;
    askg-onto:inSentence "Subsequently, we calculate the magnitude gD0and gI of the gradients of D0and Ig, and define the boundary IOU loss as:"^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-boundary_iou_loss,
        askg-data:Entity-gradients,
        askg-data:Entity-magnitude_of_d0_and_ig .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-94 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 4"@en ;
    domo:Text "$$\\mathcal{L}_{\\mathrm{Ioub}}=1-2\\frac{|gD^{\\prime}\\cap gI|}{|gD^{\\prime}|+|gI|}.$$"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-94-Sentence-941 ;
    askg-onto:index "4"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-94-Sentence-941 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "$$\\mathcal{L}_{\\mathrm{Ioub}}=1-2\\frac{|gD^{\\prime}\\cap gI|}{|gD^{\\prime}|+|gI|}.$$"@en ;
    askg-onto:inSentence "$$\\mathcal{L}_{\\mathrm{Ioub}}=1-2\\frac{|gD^{\\prime}\\cap gI|}{|gD^{\\prime}|+|gI|}.$$"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%0Amathcall_mathrmioub .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 5"@en ;
    domo:Text "Saliency detection is subjective to some extent, and it is common to have multiple annotators to label one image, and the final ground truth saliency map is obtained through majority voting strategy [18]. Although it is well known in the saliency detection community about how the ground truth is acquired; yet, there exists no research on embedding this mechanism into deep saliency frameworks. Current models define saliency detection as a point estimation problem instead of a distribution estimation problem. We, instead, use CVAE to obtain the saliency distribution. Next, we embed saliency consensus into our probabilistic framework to compute the majority voting of different predictions in the testing stage as shown in Fig. 3."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-951,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-952,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-953,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-954,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-955,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-956 ;
    askg-onto:index "5"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-951 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Saliency detection is subjective to some extent, and it is common to have multiple annotators to label one image, and the final ground truth saliency map is obtained through majority voting strategy [18]."@en ;
    askg-onto:inSentence "Saliency detection is subjective to some extent, and it is common to have multiple annotators to label one image, and the final ground truth saliency map is obtained through majority voting strategy [18]."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-annotators,
        askg-data:Entity-final_ground_truth_saliency_map,
        askg-data:Entity-majority_voting_strategy,
        askg-data:Entity-method,
        askg-data:Entity-one_image,
        askg-data:Entity-saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-952 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "Although it is well known in the saliency detection community about how the ground truth is acquired; yet, there exists no research on embedding this mechanism into deep saliency frameworks."@en ;
    askg-onto:inSentence "Although it is well known in the saliency detection community about how the ground truth is acquired; yet, there exists no research on embedding this mechanism into deep saliency frameworks."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-deep_saliency_frameworks,
        askg-data:Entity-ground_truth_acquisition,
        askg-data:Entity-saliency_detection_community .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-953 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "Current models define saliency detection as a point estimation problem instead of a distribution estimation problem."@en ;
    askg-onto:inSentence "Current models define saliency detection as a point estimation problem instead of a distribution estimation problem."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-distribution_estimation_problem,
        askg-data:Entity-point_estimation_problem,
        askg-data:Entity-saliency_detection .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-954 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "We, instead, use CVAE to obtain the saliency distribution."@en ;
    askg-onto:inSentence "We, instead, use CVAE to obtain the saliency distribution."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-cvae,
        askg-data:Entity-saliency_distribution .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-955 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "Next, we embed saliency consensus into our probabilistic framework to compute the majority voting of different predictions in the testing stage as shown in Fig."@en ;
    askg-onto:inSentence "Next, we embed saliency consensus into our probabilistic framework to compute the majority voting of different predictions in the testing stage as shown in Fig."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-different_predictions,
        askg-data:Entity-fig,
        askg-data:Entity-majority_voting,
        askg-data:Entity-probabilistic_framework,
        askg-data:Entity-saliency_consensus,
        askg-data:Entity-testing_stage .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-95-Sentence-956 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "3."@en ;
    askg-onto:inSentence "3."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-finding,
        askg-data:Entity-triple .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-96 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 6"@en ;
    domo:Text "During testing, we sample PriorNet with fixed µprior and σprior to obtain a stochastic feature S s. With each S sand deterministic feature S dfrom SaliencyNet, we obtain one"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-96-Sentence-961,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-96-Sentence-962 ;
    askg-onto:index "6"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-96-Sentence-961 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "During testing, we sample PriorNet with fixed µprior and σprior to obtain a stochastic feature S s."@en ;
    askg-onto:inSentence "During testing, we sample PriorNet with fixed µprior and σprior to obtain a stochastic feature S s."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-priornet,
        askg-data:Entity-stochastic_feature_s_s .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-96-Sentence-962 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "With each S sand deterministic feature S dfrom SaliencyNet, we obtain one"@en ;
    askg-onto:inSentence "With each S sand deterministic feature S dfrom SaliencyNet, we obtain one"^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-saliencynet .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-97 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 7"@en ;
    domo:Text "$$({\\mathfrak{I}})$$"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-97-Sentence-971 ;
    askg-onto:index "7"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-97-Sentence-971 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "$$({\\mathfrak{I}})$$"@en ;
    askg-onto:inSentence "$$({\\mathfrak{I}})$$"^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-%09exti,
        askg-data:Entity-data_analysis,
        askg-data:Entity-regression_analysis,
        askg-data:Entity-statistical_method,
        askg-data:Entity-statistical_methods .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 8"@en ;
    domo:Text ". (3) version of saliency prediction P. To obtain C different predictions P 1*, ..., P* C , we sample PriorNet C times. We simultaneously feed these multiple predictions to the saliency consensus module to obtain the consensus of predictions."@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98-Sentence-981,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98-Sentence-982,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98-Sentence-983,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98-Sentence-984 ;
    askg-onto:index "8"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98-Sentence-981 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "."@en ;
    askg-onto:inSentence "."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-article,
        askg-data:Entity-city,
        askg-data:Entity-company,
        askg-data:Entity-database,
        askg-data:Entity-dataset,
        askg-data:Entity-experiment,
        askg-data:Entity-method,
        askg-data:Entity-research_group,
        askg-data:Entity-scientist,
        askg-data:Entity-study,
        askg-data:Entity-technology,
        askg-data:Entity-university .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98-Sentence-982 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "(3) version of saliency prediction P."@en ;
    askg-onto:inSentence "(3) version of saliency prediction P."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-saliency_prediction,
        askg-data:Entity-saliency_prediction_p .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98-Sentence-983 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "To obtain C different predictions P 1*, ..., P* C , we sample PriorNet C times."@en ;
    askg-onto:inSentence "To obtain C different predictions P 1*, ..., P* C , we sample PriorNet C times."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-c,
        askg-data:Entity-c_times,
        askg-data:Entity-p_1__p_c,
        askg-data:Entity-priornet .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-98-Sentence-984 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "We simultaneously feed these multiple predictions to the saliency consensus module to obtain the consensus of predictions."@en ;
    askg-onto:inSentence "We simultaneously feed these multiple predictions to the saliency consensus module to obtain the consensus of predictions."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-consensus_of_predictions,
        askg-data:Entity-predictions,
        askg-data:Entity-saliency_consensus_module .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99 a askg-onto:Paragraph ;
    rdfs:label "Paragraph 9"@en ;
    domo:Text "Given multiple predictions {P c} C c=1, where P c ∈ [0, 1], we first compute the binary4 version P c b of the predictions by performing adaptive threshold [4] on P c. For each pixel (*u, v*), we obtain a C dimensional feature vector Pu,v ∈ {0, 1}. We define P mjv b ∈ {0, 1} as a one-channel saliency map representing majority voting of Pu,v. We define an indicator 1 c(*u, v*) = 1(P c b(u, v) = P mjv b(*u, v*)) representing whether the binary prediction is consistent with the majority voting of the predictions. If P c b(u, v) = P mjv b(*u, v*), then 1 c(*u, v*) = 1. Otherwise, 1 c(*u, v*) = 0. We obtain one gray saliency map after saliency consensus as:"@en ;
    askg-onto:hasSentence askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-991,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-992,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-993,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-994,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-995,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-996,
        askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-997 ;
    askg-onto:index "9"^^xsd:int .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-991 a askg-onto:Sentence ;
    rdfs:label "Sentence 1"@en ;
    domo:Text "Given multiple predictions {P c} C c=1, where P c ∈ [0, 1], we first compute the binary4 version P c b of the predictions by performing adaptive threshold [4] on P c."@en ;
    askg-onto:inSentence "Given multiple predictions {P c} C c=1, where P c ∈ [0, 1], we first compute the binary4 version P c b of the predictions by performing adaptive threshold [4] on P c."^^xsd:string ;
    askg-onto:index "1"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-adaptive_threshold,
        askg-data:Entity-binary_version,
        askg-data:Entity-predictions .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-992 a askg-onto:Sentence ;
    rdfs:label "Sentence 2"@en ;
    domo:Text "For each pixel (*u, v*), we obtain a C dimensional feature vector Pu,v ∈ {0, 1}."@en ;
    askg-onto:inSentence "For each pixel (*u, v*), we obtain a C dimensional feature vector Pu,v ∈ {0, 1}."^^xsd:string ;
    askg-onto:index "2"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-pixel .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-993 a askg-onto:Sentence ;
    rdfs:label "Sentence 3"@en ;
    domo:Text "We define P mjv b ∈ {0, 1} as a one-channel saliency map representing majority voting of Pu,v."@en ;
    askg-onto:inSentence "We define P mjv b ∈ {0, 1} as a one-channel saliency map representing majority voting of Pu,v."^^xsd:string ;
    askg-onto:index "3"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-majority_voting_of_puv,
        askg-data:Entity-one-channel_saliency_map,
        askg-data:Entity-p_mjv_b .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-994 a askg-onto:Sentence ;
    rdfs:label "Sentence 4"@en ;
    domo:Text "We define an indicator 1 c(*u, v*) = 1(P c b(u, v) = P mjv b(*u, v*)) representing whether the binary prediction is consistent with the majority voting of the predictions."@en ;
    askg-onto:inSentence "We define an indicator 1 c(*u, v*) = 1(P c b(u, v) = P mjv b(*u, v*)) representing whether the binary prediction is consistent with the majority voting of the predictions."^^xsd:string ;
    askg-onto:index "4"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-binary_prediction,
        askg-data:Entity-majority_voting .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-995 a askg-onto:Sentence ;
    rdfs:label "Sentence 5"@en ;
    domo:Text "If P c b(u, v) = P mjv b(*u, v*), then 1 c(*u, v*) = 1."@en ;
    askg-onto:inSentence "If P c b(u, v) = P mjv b(*u, v*), then 1 c(*u, v*) = 1."^^xsd:string ;
    askg-onto:index "5"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-1,
        askg-data:Entity-1_cu_v,
        askg-data:Entity-p_c_bu_v,
        askg-data:Entity-p_mjv_bu_v .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-996 a askg-onto:Sentence ;
    rdfs:label "Sentence 6"@en ;
    domo:Text "Otherwise, 1 c(*u, v*) = 0."@en ;
    askg-onto:inSentence "Otherwise, 1 c(*u, v*) = 0."^^xsd:string ;
    askg-onto:index "6"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-0,
        askg-data:Entity-1_cu_v .

askg-data:Paper-fdb482b8f2df5642-Section-9-Paragraph-99-Sentence-997 a askg-onto:Sentence ;
    rdfs:label "Sentence 7"@en ;
    domo:Text "We obtain one gray saliency map after saliency consensus as:"@en ;
    askg-onto:inSentence "We obtain one gray saliency map after saliency consensus as:"^^xsd:string ;
    askg-onto:index "7"^^xsd:int ;
    askg-onto:mentions askg-data:Entity-gray_saliency_map,
        askg-data:Entity-saliency_consensus .

askg-data:Entity- rdfs:label ""@en,
        "↑"@en,
        "↓"@en ;
    askg-onto:entityType "Finding"@en,
        "Metric"@en .

askg-data:Entity-10 rdfs:label "10"@en,
        "10%"@en ;
    askg-onto:entityType "Metric"@en,
        "Rate"@en .

askg-data:Entity-1_cu_v rdfs:label "1 c(*u, v*)"@en ;
    askg-onto:entityType "Concept"@en,
        "Metric"@en .

askg-data:Entity-2014 rdfs:label "2014"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-26 rdfs:label "26"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-8 rdfs:label "8"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-aaai rdfs:label "AAAI"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-benchmark rdfs:label "Benchmark"@en,
        "benchmark"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-c rdfs:label "C"@en,
        "C."@en ;
    askg-onto:entityType "Metric"@en,
        "Person"@en .

askg-data:Entity-camouflaged_object_detection rdfs:label "Camouflaged Object Detection"@en,
        "camouflaged object detection"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-chen rdfs:label "Chen"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-christof_koch rdfs:label "Christof Koch"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-conditional_variational_autoencoder rdfs:label "conditional variational autoencoder"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-conditional_variational_autoencoders rdfs:label "conditional variational autoencoders"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-danilo_jimenez_rezende rdfs:label "Danilo Jimenez Rezende"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-database rdfs:label "Database"@en ;
    askg-onto:entityType "Database"@en .

askg-data:Entity-deep_learning rdfs:label "Deep Learning"@en ;
    askg-onto:entityType "Concept"@en,
        "Model"@en .

askg-data:Entity-denseaspp rdfs:label "DenseASPP"@en ;
    askg-onto:entityType "Framework"@en,
        "Model"@en .

askg-data:Entity-depth_correction_network rdfs:label "depth correction network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-dingwen_zhang rdfs:label "Dingwen Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-dmra rdfs:label "DMRA"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-dropout rdfs:label "dropout"@en ;
    askg-onto:entityType "Method"@en,
        "Technique"@en .

askg-data:Entity-encoder rdfs:label "encoder"@en ;
    askg-onto:entityType "Concept"@en,
        "Model"@en,
        "Technique"@en .

askg-data:Entity-experiments rdfs:label "experiments"@en ;
    askg-onto:entityType "Experiment"@en .

askg-data:Entity-fatemeh_sadat_saleh rdfs:label "Fatemeh Sadat Saleh"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-gangshan_wu rdfs:label "Gangshan Wu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-gt_saliency_map rdfs:label "GT saliency map"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-huazhu_fu rdfs:label "Huazhu Fu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-human_uncertainty rdfs:label "human uncertainty"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-iclr rdfs:label "ICLR"@en ;
    askg-onto:entityType "Organization"@en,
        "Publication"@en .

askg-data:Entity-ig rdfs:label "Ig"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-input rdfs:label "input"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-input_x rdfs:label "input X"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-jia-xing_zhao rdfs:label "Jia-Xing Zhao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jiang-jiang_liu rdfs:label "Jiang-Jiang Liu"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-label_generation rdfs:label "Label Generation"@en,
        "label generation"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-latent_variable_z rdfs:label "latent variable z"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-laurent_itti rdfs:label "Laurent Itti"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ling_shao rdfs:label "Ling Shao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-m rdfs:label "M"@en,
        "M."@en ;
    askg-onto:entityType "Metric"@en,
        "Person"@en .

askg-data:Entity-m3 rdfs:label "M3"@en ;
    askg-onto:entityType "Metric"@en,
        "Model"@en .

askg-data:Entity-m7 rdfs:label "M7"@en ;
    askg-onto:entityType "Metric"@en,
        "Model"@en .

askg-data:Entity-m9 rdfs:label "M9"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-monte-carlo_dropout rdfs:label "Monte-Carlo Dropout"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-new_label_generation_technique rdfs:label "new label generation technique"@en ;
    askg-onto:entityType "Technique"@en .

askg-data:Entity-nick_barnes rdfs:label "Nick Barnes"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-noise rdfs:label "noise"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-olivier_le_meur rdfs:label "Olivier Le Meur"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-orientation_shifting_theory rdfs:label "orientation shifting theory"@en ;
    askg-onto:entityType "Theory"@en .

askg-data:Entity-our_approach rdfs:label "our approach"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-ours rdfs:label "ours"@en ;
    askg-onto:entityType "Method"@en,
        "Metric"@en .

askg-data:Entity-output_variable_y rdfs:label "output variable Y"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-p%CF%89y_x_z rdfs:label "Pω(Y |*X, z*)"@en ;
    askg-onto:entityType "Concept"@en,
        "Model"@en .

askg-data:Entity-performance rdfs:label "performance"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-pipeline rdfs:label "pipeline"@en ;
    askg-onto:entityType "Concept"@en,
        "Framework"@en .

askg-data:Entity-posterior_of_z rdfs:label "posterior of z"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-py rdfs:label "P(Y)"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-q%CF%86zx_y rdfs:label "Qφ(z|*X, Y*)"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-q%CF%86zx_y_ rdfs:label "Qφ(z|*X, Y* )"@en ;
    askg-onto:entityType "Concept"@en,
        "Model"@en .

askg-data:Entity-qingming_huang rdfs:label "Qingming Huang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-regularizer rdfs:label "regularizer"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-rgb-d_image rdfs:label "RGB-D image"@en,
        "rgb-d image"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-rgb-d_image_pair rdfs:label "RGB-D image pair"@en ;
    askg-onto:entityType "Concept"@en,
        "Dataset"@en .

askg-data:Entity-rgb-d_saliency_detection_network rdfs:label "RGB-D saliency detection network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-rgb_and_depth_information rdfs:label "RGB and depth information"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-s rdfs:label "S"@en,
        "S."@en ;
    askg-onto:entityType "Concept"@en,
        "Person"@en .

askg-data:Entity-saliency_prediction rdfs:label "saliency prediction"@en ;
    askg-onto:entityType "Finding"@en,
        "Model"@en .

askg-data:Entity-salient_objects rdfs:label "salient objects"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-salient_region rdfs:label "salient region"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-single_channel_saliency_map rdfs:label "single channel saliency map"@en ;
    askg-onto:entityType "Concept"@en,
        "Finding"@en .

askg-data:Entity-smoothness_loss rdfs:label "smoothness loss"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-software rdfs:label "Software"@en ;
    askg-onto:entityType "Software"@en .

askg-data:Entity-sota_models rdfs:label "SOTA models"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-statistics rdfs:label "Statistics"@en,
        "statistics"@en ;
    askg-onto:entityType "Concept"@en,
        "Measure"@en .

askg-data:Entity-stochastic_feature_s rdfs:label "stochastic feature S"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-tensorflow rdfs:label "TensorFlow"@en ;
    askg-onto:entityType "Software"@en .

askg-data:Entity-testing_stage rdfs:label "testing stage"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-the_network rdfs:label "the network"@en ;
    askg-onto:entityType "System"@en .

askg-data:Entity-tong_zhang rdfs:label "Tong Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-training rdfs:label "training"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-triple rdfs:label "triple"@en ;
    askg-onto:entityType "Concept"@en,
        "Finding"@en .

askg-data:Entity-triple_data rdfs:label "triple data"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-variational_autoencoders rdfs:label "Variational Autoencoders"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-vgg16_network rdfs:label "VGG16 network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-visual_saliency_detection rdfs:label "visual saliency detection"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-wenguan_wang rdfs:label "Wenguan Wang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xiaochun_cao rdfs:label "Xiaochun Cao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-xiaojin_gong rdfs:label "Xiaojin Gong"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zhao_zhang rdfs:label "Zhao Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-2018 rdfs:label "2018"@en ;
    askg-onto:entityType "Metric"@en,
        "Publication"@en .

askg-data:Entity-2020 rdfs:label "2020"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-best_performance rdfs:label "best performance"@en ;
    askg-onto:entityType "Finding"@en,
        "Metric"@en,
        "Result"@en .

askg-data:Entity-city rdfs:label "City"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-company rdfs:label "Company"@en ;
    askg-onto:entityType "Company"@en .

askg-data:Entity-competing_methods rdfs:label "competing methods"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-cvae_based_model rdfs:label "CVAE based model"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-data rdfs:label "Data"@en,
        "data"@en ;
    askg-onto:entityType "Corpus"@en,
        "Dataset"@en .

askg-data:Entity-datasets rdfs:label "datasets"@en ;
    askg-onto:entityType "Concept"@en,
        "Dataset"@en .

askg-data:Entity-des rdfs:label "DES"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-experiment rdfs:label "Experiment"@en ;
    askg-onto:entityType "Experiment"@en .

askg-data:Entity-f-measure rdfs:label "F-measure"@en ;
    askg-onto:entityType "Measure"@en,
        "Metric"@en .

askg-data:Entity-finding rdfs:label "Finding"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-ge-peng_ji rdfs:label "Ge-Peng Ji"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-gt_saliency_maps rdfs:label "GT saliency maps"@en ;
    askg-onto:entityType "Concept"@en,
        "Dataset"@en,
        "Finding"@en .

askg-data:Entity-institution rdfs:label "Institution"@en ;
    askg-onto:entityType "Institution"@en .

askg-data:Entity-jianbing_shen rdfs:label "Jianbing Shen"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-jing_zhang rdfs:label "Jing Zhang"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-lfsd rdfs:label "LFSD"@en ;
    askg-onto:entityType "Dataset"@en,
        "Metric"@en .

askg-data:Entity-m-head rdfs:label "M-head"@en ;
    askg-onto:entityType "Method"@en,
        "Model"@en .

askg-data:Entity-m1 rdfs:label "M1"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-m4 rdfs:label "M4"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-m6 rdfs:label "M6"@en ;
    askg-onto:entityType "Metric"@en,
        "Model"@en .

askg-data:Entity-m8 rdfs:label "M8"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-mc-dropout rdfs:label "MC-dropout"@en ;
    askg-onto:entityType "Method"@en,
        "Model"@en .

askg-data:Entity-network rdfs:label "network"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-nju2k rdfs:label "NJU2K"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-our_method rdfs:label "our method"@en ;
    askg-onto:entityType "Method"@en .

askg-data:Entity-point_estimation_problem rdfs:label "point estimation problem"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-pytorch rdfs:label "PyTorch"@en,
        "Pytorch"@en ;
    askg-onto:entityType "Framework"@en,
        "Software"@en .

askg-data:Entity-research_area rdfs:label "Research Area"@en ;
    askg-onto:entityType "Research Area"@en .

askg-data:Entity-rgb_image rdfs:label "RGB image"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-rgbd_salient_object_detection rdfs:label "RGBD Salient Object Detection"@en,
        "RGBD salient object detection"@en,
        "Rgbd salient object detection"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-tongwei_ren rdfs:label "Tongwei Ren"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-university_of_california_san_francisco rdfs:label "University of California, San Francisco"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-yang_cao rdfs:label "Yang Cao"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-year rdfs:label "Year"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-youfu_li rdfs:label "Youfu Li"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-yuchao_dai rdfs:label "Yuchao Dai"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-zheng_lin rdfs:label "Zheng Lin"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-2017 rdfs:label "2017"@en ;
    askg-onto:entityType "Metric"@en,
        "Publication"@en .

askg-data:Entity-ali_borji rdfs:label "Ali Borji"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-boundary_iou_loss rdfs:label "boundary IOU loss"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en,
        "Metric"@en .

askg-data:Entity-data_visualization rdfs:label "data visualization"@en ;
    askg-onto:entityType "Concept"@en,
        "Finding"@en .

askg-data:Entity-diverse_predictions rdfs:label "diverse predictions"@en ;
    askg-onto:entityType "Finding"@en .

askg-data:Entity-e-measure rdfs:label "E-measure"@en ;
    askg-onto:entityType "Measure"@en,
        "Metric"@en .

askg-data:Entity-gt rdfs:label "GT"@en ;
    askg-onto:entityType "Concept"@en,
        "Model"@en .

askg-data:Entity-hao_chen rdfs:label "Hao Chen"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ieee_tip rdfs:label "IEEE TIP"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-k rdfs:label "K"@en ;
    askg-onto:entityType "Concept"@en,
        "Metric"@en .

askg-data:Entity-m5 rdfs:label "M5"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-majority_voting rdfs:label "majority voting"@en ;
    askg-onto:entityType "Concept"@en,
        "Finding"@en,
        "Method"@en .

askg-data:Entity-nlpr rdfs:label "NLPR"@en ;
    askg-onto:entityType "Dataset"@en,
        "Metric"@en .

askg-data:Entity-rgb-d_images rdfs:label "RGB-D images"@en ;
    askg-onto:entityType "Concept"@en,
        "Dataset"@en,
        "Technology"@en .

askg-data:Entity-saliency_consensus rdfs:label "saliency consensus"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-saliency_maps rdfs:label "saliency maps"@en ;
    askg-onto:entityType "Concept"@en,
        "Finding"@en .

askg-data:Entity-sip rdfs:label "SIP"@en ;
    askg-onto:entityType "Dataset"@en,
        "Metric"@en .

askg-data:Entity-2019 rdfs:label "2019"@en ;
    askg-onto:entityType "Metric"@en,
        "Publication"@en .

askg-data:Entity-article rdfs:label "Article"@en ;
    askg-onto:entityType "Article"@en,
        "Publication"@en .

askg-data:Entity-depth_data rdfs:label "depth data"@en ;
    askg-onto:entityType "Concept"@en,
        "Dataset"@en,
        "Finding"@en .

askg-data:Entity-eccv rdfs:label "ECCV"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-fig rdfs:label "Fig"@en,
        "Fig."@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-hha rdfs:label "HHA"@en ;
    askg-onto:entityType "Concept"@en,
        "Model"@en,
        "Technology"@en .

askg-data:Entity-labeling_variants rdfs:label "labeling variants"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-multiple_predictions rdfs:label "multiple predictions"@en ;
    askg-onto:entityType "Concept"@en,
        "Finding"@en .

askg-data:Entity-organization rdfs:label "Organization"@en ;
    askg-onto:entityType "Organization"@en .

askg-data:Entity-p%CE%B8zx rdfs:label "Pθ(z|X)"@en ;
    askg-onto:entityType "Concept"@en,
        "Model"@en .

askg-data:Entity-predictions rdfs:label "Predictions"@en,
        "predictions"@en ;
    askg-onto:entityType "Concept"@en,
        "Finding"@en .

askg-data:Entity-ssb rdfs:label "SSB"@en ;
    askg-onto:entityType "Dataset"@en,
        "Metric"@en .

askg-data:Entity-tool rdfs:label "Tool"@en,
        "tool"@en ;
    askg-onto:entityType "Tool"@en .

askg-data:Entity-training_dataset rdfs:label "training dataset"@en ;
    askg-onto:entityType "Dataset"@en .

askg-data:Entity-ieee_iccv rdfs:label "IEEE ICCV"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-predictionnet rdfs:label "PredictionNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-rgb-d_salient_object_detection rdfs:label "RGB-D Salient Object Detection"@en,
        "RGB-D salient object detection"@en ;
    askg-onto:entityType "Concept"@en,
        "Dataset"@en,
        "Method"@en .

askg-data:Entity-saliency_consensus_module rdfs:label "Saliency Consensus Module"@en,
        "saliency consensus module"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-study rdfs:label "Study"@en ;
    askg-onto:entityType "Study"@en .

askg-data:Entity-uc-net rdfs:label "UC-Net"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-vae rdfs:label "VAE"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-latentnet rdfs:label "LatentNet"@en ;
    askg-onto:entityType "Framework"@en,
        "Model"@en .

askg-data:Entity-metric rdfs:label "Metric"@en ;
    askg-onto:entityType "Metric"@en .

askg-data:Entity-posteriornet rdfs:label "PosteriorNet"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-depthcorrectionnet rdfs:label "DepthCorrectionNet"@en ;
    askg-onto:entityType "Framework"@en,
        "Model"@en .

askg-data:Entity-saliency_detection rdfs:label "Saliency Detection"@en,
        "Saliency detection"@en,
        "saliency detection"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-saliency_map rdfs:label "saliency map"@en ;
    askg-onto:entityType "Concept"@en,
        "Finding"@en,
        "Method"@en .

askg-data:Entity-salient_object_detection rdfs:label "Salient Object Detection"@en,
        "salient object detection"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-technique rdfs:label "Technique"@en,
        "technique"@en ;
    askg-onto:entityType "Concept"@en,
        "Technique"@en .

askg-data:Entity-technology rdfs:label "Technology"@en,
        "technology"@en ;
    askg-onto:entityType "Concept"@en,
        "Technology"@en .

askg-data:Entity-university rdfs:label "University"@en ;
    askg-onto:entityType "University"@en .

askg-data:Entity-framework rdfs:label "Framework"@en,
        "framework"@en ;
    askg-onto:entityType "Concept"@en,
        "Framework"@en .

askg-data:Entity-research_group rdfs:label "Research Group"@en,
        "research group"@en ;
    askg-onto:entityType "Research Group"@en .

askg-data:Entity-rgb-d_saliency_detection rdfs:label "RGB-D Saliency Detection"@en,
        "RGB-D saliency detection"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en,
        "Technique"@en .

askg-data:Entity-priornet rdfs:label "PriorNet"@en ;
    askg-onto:entityType "Framework"@en,
        "Model"@en .

askg-data:Entity-concept rdfs:label "Concept"@en,
        "concept"@en ;
    askg-onto:entityType "Concept"@en .

askg-data:Entity-dataset rdfs:label "Dataset"@en,
        "dataset"@en ;
    askg-onto:entityType "Concept"@en,
        "Dataset"@en .

askg-data:Entity-deng-ping_fan rdfs:label "Deng-Ping Fan"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-ming-ming_cheng rdfs:label "Ming-Ming Cheng"@en ;
    askg-onto:entityType "Person"@en .

askg-data:Entity-saliencynet rdfs:label "SaliencyNet"@en ;
    askg-onto:entityType "Framework"@en,
        "Model"@en .

askg-data:Entity-scientist rdfs:label "Scientist"@en,
        "scientist"@en ;
    askg-onto:entityType "Scientist"@en .

askg-data:Entity-cvae rdfs:label "CVAE"@en ;
    askg-onto:entityType "Model"@en .

askg-data:Entity-ieee_cvpr rdfs:label "IEEE CVPR"@en ;
    askg-onto:entityType "Organization"@en,
        "Publication"@en .

askg-data:Entity-model rdfs:label "Model"@en,
        "model"@en ;
    askg-onto:entityType "Concept"@en,
        "Model"@en .

askg-data:Entity-method rdfs:label "Method"@en,
        "method"@en ;
    askg-onto:entityType "Concept"@en,
        "Method"@en .

askg-data:Entity-publication rdfs:label "Publication"@en,
        "publication"@en ;
    askg-onto:entityType "Publication"@en .

askg-data:Entity-person rdfs:label "Person"@en,
        "person"@en ;
    askg-onto:entityType "Concept"@en,
        "Person"@en .

