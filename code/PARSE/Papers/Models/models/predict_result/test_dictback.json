{"SOLUTION": {"evaluation metrics": {"position": [[65, 66], [267, 268], [307, 308]], "sentence": ["automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task", "and accurate inexpensive automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task evaluation metrics to encourage rapid progress", "there is an urgent to need develop new automated and accurate inexpensive automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task evaluation metrics to encourage rapid progress evaluation metrics for this task [8,9]"]}, "new caption evaluation metric": {"position": [[106, 109]], "sentence": ["and propose a automated new caption evaluation metric defined over scene graphs spice. coined extensive evaluations across a range of models and indicate"]}, "extensive evaluations": {"position": [[116, 117]], "sentence": ["and propose a automated new caption evaluation metric defined over scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions"]}, "generator": {"position": [[177, 177]], "sentence": ["spice can questions answer such as which caption generator best understands colors? can and caption generators recently introduction count? there has been considerable interest"]}, "caption generators": {"position": [[183, 184]], "sentence": ["spice can questions answer such as which caption generator best understands colors? can and caption generators recently introduction count? there has been considerable interest in joint visual and linguistic problems"]}, "captions": {"position": [[539, 539]], "sentence": ["of a tennis (b) court. a giraffe standing on top of a green the field. captions describe two very different images"]}, "gram phrase": {"position": [[571, 572]], "sentence": ["mentioned gram metrics produces a similarity high score due to the presence of the long gram phrase standing on of top a in both captions"]}, "semantic propositional image caption": {"position": [[657, 660]], "sentence": ["automatic metrics, evaluation in this work we hypothesize that semantic propositional content is \\\\fspice: an semantic propositional image caption important 3 evaluation component of human caption evaluation"]}, "a evaluator": {"position": [[694, 695]], "sentence": ["we expect that human a evaluator might consider the truth value of each of the semantic propositions contained thereinsuch as there"]}, "truth value": {"position": [[699, 700]], "sentence": ["we expect that human a evaluator might consider the truth value of each of the semantic propositions contained thereinsuch as there is a girl"]}, "of the semantic propositions": {"position": [[703, 706]], "sentence": ["we expect that human a evaluator might consider the truth value of each of the semantic propositions contained thereinsuch as there is a girl"]}, "syntactic idiosyncrasies": {"position": [[816, 817]], "sentence": ["abstracting away most the of lexical and syntactic idiosyncrasies of natural language in the process"]}, "the event structure": {"position": [[1471, 1473]], "sentence": ["srls are used to try to capture basic the event structure of sentences who did what to whom"]}, "the verbs at": {"position": [[1507, 1509]], "sentence": ["sentence similarity is calculated by first matching semantic frames across sentences by starting with the verbs at their however, head. this approach does not easily transfer to image caption evaluation"]}, "semantic structures": {"position": [[1643, 1644]], "sentence": ["or similar semantic structures"]}, "semantic propositional image caption [18,16]. 5": {"position": [[1689, 1694]], "sentence": ["these papers have demonstrated that semantic graphs can be parsed from natural language \\\\fspice: descriptions semantic propositional image caption [18,16]. 5 evaluation the task of transforming a sentence into its meaning representation also has received considerable"]}, "meaning representation": {"position": [[1704, 1705]], "sentence": ["semantic propositional image caption [18,16]. 5 evaluation the task of transforming a sentence into its meaning representation also has received considerable attention within the computational linguistics community"]}, "framework": {"position": [[1723, 1723]], "sentence": ["recent work has proposed a common framework for semantic graphs an called abstract meaning representation amr [24]"]}, "abstract meaning representation amr": {"position": [[1729, 1732]], "sentence": ["recent work has proposed a common framework for semantic graphs an called abstract meaning representation amr [24]"]}, "structure": {"position": [[2382, 2382]], "sentence": ["resolve pronouns and handle plural the nouns. resulting tree structure is then parsed according to nine simple rules linguistic to extract lemmatized objects"]}, "plural nouns transformation": {"position": [[2472, 2474]], "sentence": ["we drop the plural nouns transformation that duplicates nodes individual of the graph according to the value of their numeric modifier"]}, "associated relations": {"position": [[2530, 2531]], "sentence": ["that ensures that nouns will always appear as objects the in scene grapheven if no associated relations can identifiedas graph disconnected nodes are easily handled by our semantic proposition score notwithstanding calculation."]}, "disconnected nodes": {"position": [[2535, 2536]], "sentence": ["always appear as objects the in scene grapheven if no associated relations can identifiedas graph disconnected nodes are easily handled by our semantic proposition score notwithstanding calculation. the use of the stanford"]}, "tuples: (girl)": {"position": [[2728, 2729]], "sentence": ["the scene graph in figure would 1 be represented with the following { tuples: (girl)"]}, "inflectional forms": {"position": [[2869, 2870]], "sentence": ["are considered to be matched if their lemmatized word forms are equalallowing terms with different inflectional forms matchor to if they are found in the same wordnet unlike sysnet. smatch [27]"]}, "amr graphs": {"position": [[2899, 2900]], "sentence": ["a recently proposed metric for evaluating amr that parsers considers multiple alignments of amr graphs"]}, "a tuple": {"position": [[2926, 2927]], "sentence": ["stephen partial gould credit when only one element of a tuple is incorrect"]}, "dataset statisticssuch": {"position": [[2990, 2991]], "sentence": ["spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to both small and large whenever gameability"]}, "model judgement": {"position": [[3177, 3178]], "sentence": ["to human model judgement in a particular task as closely as possible"]}, "semantic propositional image": {"position": [[3383, 3384]], "sentence": ["we generate pairwise rankings by comparing \\\\fspice: scores semantic propositional image caption table 9 evaluation 1"]}, "correctness": {"position": [[3551, 3551]], "sentence": ["better or equal to human percentage caption. of captions that pass the turing average test. correctness of the captions on a scale 15 incorrect average correct). detail of the captions from"]}, "average correct). detail": {"position": [[3560, 3562]], "sentence": ["that pass the turing average test. correctness of the captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of captions that are similar"]}, "http://panderson.me/spice 2": {"position": [[3833, 3834]], "sentence": ["using our peter \\\\f10 http://panderson.me/spice 2 code anderson"]}, "candidate sentence pairs": {"position": [[4262, 4264]], "sentence": ["candidate sentence pairs were generated from both human model and captions"]}, "model system-level": {"position": [[4290, 4291]], "sentence": ["and model model system-level 4.2 (mm). in correlation table 1 we report system level correlations between metrics and human"]}, "metrics and human judgments": {"position": [[4304, 4307]], "sentence": ["model model system-level 4.2 (mm). in correlation table 1 we report system level correlations between metrics and human judgments over entries in the 2015 coco captioning challenge [6]"]}, "spice semantic": {"position": [[4336, 4336]], "sentence": ["\\\\fspice: spice semantic propositional image caption fig. 11 evaluation 3"]}, "metrics": {"position": [[4571, 4571]], "sentence": ["counting and other existing questions gram evaluation metrics have little to offer in terms of the understanding relative strengths and weaknesses"]}, "attribute tuples": {"position": [[4624, 4625]], "sentence": ["relation and attribute tuples"]}, "human baseline": {"position": [[4641, 4642]], "sentence": ["count and size are attribute although subcategories. the best models outperform the human baseline in their use of object attributes, color none of the models exhibits a convincing ability"]}, "color": {"position": [[4649, 4649]], "sentence": ["although subcategories. the best models outperform the human baseline in their use of object attributes, color none of the models exhibits a convincing ability to human count msr [6] google [38]"]}, "of the models": {"position": [[4651, 4653]], "sentence": ["the best models outperform the human baseline in their use of object attributes, color none of the models exhibits a convincing ability to human count msr [6] google [38] msr [39] captivator berkeley"]}, "[6] google [38] msr [39] captivator berkeley [40] lrcn montreal/toronto [1]": {"position": [[4662, 4672]], "sentence": ["object attributes, color none of the models exhibits a convincing ability to human count msr [6] google [38] msr [39] captivator berkeley [40] lrcn montreal/toronto [1] m-rnn [2] nearest [41] neighbor m-rnn [42] brno mil picsom [43] university mlbl [44] neuraltalk"]}, "acvt [36] spice random bigeye object relation attribute color count": {"position": [[4690, 4699]], "sentence": ["nearest [41] neighbor m-rnn [42] brno mil picsom [43] university mlbl [44] neuraltalk [45] tsinghua acvt [36] spice random bigeye object relation attribute color count 0.074 size 0.055 0.054 0.023 0.190 0 095 0.063 0.039 0.018 0.176 0.064 0.026 0"]}, "0.055": {"position": [[4702, 4702]], "sentence": ["neuraltalk [45] tsinghua acvt [36] spice random bigeye object relation attribute color count 0.074 size 0.055 0.054 0.023 0.190 0 095 0.063 0.039 0.018 0.176 0.064 0.026 0 033 0.060 0.039"]}, "0.176 0.064 0.026 0 033 0.060 0.039 0.018": {"position": [[4711, 4718]], "sentence": ["relation attribute color count 0.074 size 0.055 0.054 0.023 0.190 0 095 0.063 0.039 0.018 0.176 0.064 0.026 0 033 0.060 0.039 0.018 0.173 0.063 0.019 0 005 0.054 0.032 0.019 0.174 0.062 0.009 0 008 0.030 0.026"]}, "composite scores,": {"position": [[5101, 5102]], "sentence": ["all values not shown are less than flickr 0.001 8k composite scores, - 0.73 inter-human 0.39 0.36 0.35 0.28 0.18 0.26 0.45 0.44 0.42 0.32 0.14 0.32"]}, "0.39 0.36 0.35 0.28 0.18 0.26 0.45 0.44 0.42 0.32": {"position": [[5106, 5115]], "sentence": ["all values not shown are less than flickr 0.001 8k composite scores, - 0.73 inter-human 0.39 0.36 0.35 0.28 0.18 0.26 0.45 0.44 0.42 0.32 0.14 0.32 spice cider meteor rouge-l bleu-4 bleu-1 [35] compared to 0 44 for cider"]}, "correct human captions": {"position": [[5551, 5553]], "sentence": ["bleu performs best given two correct human captions existing 78.8 78.9 80.8 77.3 73.7 77.7 68.2 64.6 66.8 60.3 58.0 60.1 87.5 91.0"]}, "78.8 78.9 80.8 77.3": {"position": [[5555, 5558]], "sentence": ["bleu performs best given two correct human captions existing 78.8 78.9 80.8 77.3 73.7 77.7 68.2 64.6 66.8 60.3 58.0 60.1 87.5 91.0 94.2 91.7 87.2 90.7 96.3"]}, "rouge-l bleu-2 bleu-1": {"position": [[5593, 5595]], "sentence": ["95.2 63.3 61.9 64.0 61.7 56.6 64.9 all mm hm hi hc spice cider meteor rouge-l bleu-2 bleu-1 (hc) gram metrics such as bleu"]}, "gram metrics": {"position": [[5597, 5598]], "sentence": ["61.7 56.6 64.9 all mm hm hi hc spice cider meteor rouge-l bleu-2 bleu-1 (hc) gram metrics such as bleu"]}, "powerful parsers": {"position": [[5629, 5630]], "sentence": ["that significant challenges still remain in semantic and parsing, hope that the development of more powerful parsers will underpin improvements further to the metric"]}, "upper bound": {"position": [[5651, 5652]], "sentence": ["in future work we hope to use human to annotators establish an upper bound for how closely spice approximates human judgments given perfect semantic parsing"]}, "closely spice approximates human judgments": {"position": [[5655, 5659]], "sentence": ["future work we hope to use human to annotators establish an upper bound for how closely spice approximates human judgments given perfect semantic parsing"]}}, "RESEARCH_PROBLEM": {"semantic propositional image peter": {"position": [[6, 9]], "sentence": ["text 29 jul spice: 1 2016 semantic propositional image peter evaluation caption anderson1"]}, "abstract. mark.johnson@mq.edu.au": {"position": [[41, 42]], "sentence": ["abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions"]}, "image captions": {"position": [[54, 55], [207, 208], [424, 425], [806, 807], [1144, 1145], [1530, 1531], [1598, 1599], [2464, 2465], [5432, 5433]], "sentence": ["abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions", "such as the task of automatically generating abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions image captions [1,2]", "illustrates our methods main principle which uses semantic propositional content to assess the quality of such as the task of automatically generating abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions image captions [1,2] image captions", "the scene graph explicitly encodes the attributes objects, and relationships found in illustrates our methods main principle which uses semantic propositional content to assess the quality of such as the task of automatically generating abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions image captions [1,2] image captions image captions", "work dedicated to the development of that metrics can be used for automatic evaluation of the scene graph explicitly encodes the attributes objects, and relationships found in illustrates our methods main principle which uses semantic propositional content to assess the quality of such as the task of automatically generating abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions image captions [1,2] image captions image captions image captions", "verbs as are frequently absent from work dedicated to the development of that metrics can be used for automatic evaluation of the scene graph explicitly encodes the attributes objects, and relationships found in illustrates our methods main principle which uses semantic propositional content to assess the quality of such as the task of automatically generating abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions image captions [1,2] image captions image captions image captions image captions or not meaningful e g", "closest work to ours is probably the bag of aggregated tuples semantic bast metric for verbs as are frequently absent from work dedicated to the development of that metrics can be used for automatic evaluation of the scene graph explicitly encodes the attributes objects, and relationships found in illustrates our methods main principle which uses semantic propositional content to assess the quality of such as the task of automatically generating abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions image captions [1,2] image captions image captions image captions image captions or not meaningful e g image captions", "be found in the original spice paper. slightly modifies the original parser to better evaluate closest work to ours is probably the bag of aggregated tuples semantic bast metric for verbs as are frequently absent from work dedicated to the development of that metrics can be used for automatic evaluation of the scene graph explicitly encodes the attributes objects, and relationships found in illustrates our methods main principle which uses semantic propositional content to assess the quality of such as the task of automatically generating abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions image captions [1,2] image captions image captions image captions image captions or not meaningful e g image captions image captions", "a novel semantic evaluation metric that measures effectively how be found in the original spice paper. slightly modifies the original parser to better evaluate closest work to ours is probably the bag of aggregated tuples semantic bast metric for verbs as are frequently absent from work dedicated to the development of that metrics can be used for automatic evaluation of the scene graph explicitly encodes the attributes objects, and relationships found in illustrates our methods main principle which uses semantic propositional content to assess the quality of such as the task of automatically generating abstract. mark.johnson@mq.edu.au australia there is considerable interest in the task of generating automatically image captions image captions [1,2] image captions image captions image captions image captions or not meaningful e g image captions image captions image captions recover objects"]}, "simulating human judgment": {"position": [[83, 85]], "sentence": ["primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task of simulating human judgment"]}, "human caption evaluation": {"position": [[98, 100], [666, 668]], "sentence": ["we hypothesize that semantic propositional content is important an component of human caption evaluation", "semantic propositional content is \\\\fspice: an semantic propositional image caption important 3 evaluation component of we hypothesize that semantic propositional content is important an component of human caption evaluation human caption evaluation"]}, "scene graphs spice.": {"position": [[112, 113]], "sentence": ["and propose a automated new caption evaluation metric defined over scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures"]}, "judgments": {"position": [[130, 130], [3718, 3718], [4141, 4141], [4351, 4351]], "sentence": ["coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of", "a total of 255 000 human coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of judgments were collected", "vehicles and household the objects. dataset also includes human a total of 255 000 human coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of judgments were collected judgments over 4 000 candidate pairs. sentence however", "human vehicles and household the objects. dataset also includes human a total of 255 000 human coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of judgments were collected judgments over 4 000 candidate pairs. sentence however judgments for the 15 entries in the 2015 captioning coco challenge"]}, "joint visual and linguistic problems": {"position": [[194, 198]], "sentence": ["understands colors? can and caption generators recently introduction count? there has been considerable interest in joint visual and linguistic problems"]}, "generating image": {"position": [[206, 206]], "sentence": ["such as the task of automatically generating image captions [1,2]"]}, "evaluating image captions": {"position": [[290, 292]], "sentence": ["unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8]"]}, "automatic image": {"position": [[321, 321], [1058, 1058]], "sentence": ["in this we paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic", "a principled metric for in this we paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic automatic image caption evaluation that compares semantic propositional 2. content; we show that spice outperforms metrics"]}, "image caption evaluation": {"position": [[322, 324], [1059, 1061], [1520, 1522]], "sentence": ["in this we paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content", "a principled metric for automatic in this we paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content image caption evaluation that compares semantic propositional 2. content; we show that spice outperforms metrics bleu", "starting with the verbs at their however, head. this approach does not easily transfer to a principled metric for automatic in this we paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content image caption evaluation that compares semantic propositional 2. content; we show that spice outperforms metrics bleu image caption evaluation"]}, "captions": {"position": [[332, 332], [386, 386], [430, 430], [1016, 1016], [1811, 1811], [2614, 2614], [3544, 3544], [3554, 3554], [3565, 3565], [4162, 4162], [4260, 4260], [4382, 4382], [4495, 4495], [4501, 4501], [5491, 5491], [5524, 5524], [5779, 5779]], "sentence": ["paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content", "rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions", "reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects", "spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions", "theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s", "in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all", "p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect", "to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details", "test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over", "amt workers were not asked to evaluate test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over captions against images", "sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference amt workers were not asked to evaluate test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over captions against images captions", "only spice scores human generated sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference amt workers were not asked to evaluate test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over captions against images captions captions higher significantly than challenge entries", "spice is the only metric to correctly rank humangenerated only spice scores human generated sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference amt workers were not asked to evaluate test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over captions against images captions captions higher significantly than challenge entries captions firstcider and meteor rank human captions 7th 4th, and respectively", "spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human spice is the only metric to correctly rank humangenerated only spice scores human generated sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference amt workers were not asked to evaluate test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over captions against images captions captions higher significantly than challenge entries captions firstcider and meteor rank human captions 7th 4th, and respectively captions 7th 4th, and respectively", "pairwise classification accuracy of automated metrics at matching human judgment with reference table spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human spice is the only metric to correctly rank humangenerated only spice scores human generated sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference amt workers were not asked to evaluate test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over captions against images captions captions higher significantly than challenge entries captions firstcider and meteor rank human captions 7th 4th, and respectively captions 7th 4th, and respectively captions 4", "spice is best at human matching judgments on pairs of model generated pairwise classification accuracy of automated metrics at matching human judgment with reference table spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human spice is the only metric to correctly rank humangenerated only spice scores human generated sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference amt workers were not asked to evaluate test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over captions against images captions captions higher significantly than challenge entries captions firstcider and meteor rank human captions 7th 4th, and respectively captions 7th 4th, and respectively captions 4 captions (mm)", "devlin and qi wu for providing \\\\fspice: model- semantic propositional image caption generated 15 evaluation spice is best at human matching judgments on pairs of model generated pairwise classification accuracy of automated metrics at matching human judgment with reference table spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human spice is the only metric to correctly rank humangenerated only spice scores human generated sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference amt workers were not asked to evaluate test. correctness of the captions on a scale 15 incorrect average correct). detail of the to human percentage caption. of captions that pass the turing average test. correctness of the p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of in principle it could be used to evaluate theme spice 3 [16,17,18]. given metric a candidate caption c and a set of reference spice outperforms existing gram metrics in terms of agreement with human of evaluations model generated reference and candidate rouge cider [11], or meteor to evaluate paper, present a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content captions captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects captions captions s captions on scene datasets graph [16 15 28] that have no reference captions at all captions that pass the turing average test. correctness of the captions on a scale 15 incorrect captions on a scale 15 incorrect average correct). detail of the captions from 15 lacking details captions from 15 lacking details very percentage detailed). of captions that are similar to human over captions against images captions captions higher significantly than challenge entries captions firstcider and meteor rank human captions 7th 4th, and respectively captions 7th 4th, and respectively captions 4 captions (mm) captions for evaluation"]}, "scene graphs": {"position": [[440, 441], [829, 830], [1637, 1638], [2053, 2054], [2278, 2279]], "sentence": ["reference and candidate captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects (red)", "work recent has demonstrated reference and candidate captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects (red) scene graphs to be a highly effective representation performing for complex image retrieval queries [15 16]", "and the proposed metric was not evaluated against human judgments existing or semantic 2.2 metrics. work recent has demonstrated reference and candidate captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects (red) scene graphs to be a highly effective representation performing for complex image retrieval queries [15 16] scene graphs graphs", "semantic 3.1 captions. parsingcaptions to scene we graphs define the subtask of parsing captions to and the proposed metric was not evaluated against human judgments existing or semantic 2.2 metrics. work recent has demonstrated reference and candidate captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects (red) scene graphs to be a highly effective representation performing for complex image retrieval queries [15 16] scene graphs graphs scene graphs as follows", "duplication of instances object was necessary to enable semantic 3.1 captions. parsingcaptions to scene we graphs define the subtask of parsing captions to and the proposed metric was not evaluated against human judgments existing or semantic 2.2 metrics. work recent has demonstrated reference and candidate captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects (red) scene graphs to be a highly effective representation performing for complex image retrieval queries [15 16] scene graphs graphs scene graphs as follows scene graphs to be grounded to image in regions. our work"]}, "quality caption": {"position": [[456, 457]], "sentence": ["quality caption is determined using an score calculated over tuples in the candidate reference and scene marily"]}, "captions (c": {"position": [[585, 586]], "sentence": ["now consider the captions (c d) obtained from the (c) image: same a shiny metal pot filled with some diced"]}, "caption quality": {"position": [[774, 775], [1939, 1940]], "sentence": ["we estimate caption quality by transforming both candidate and reference captions into a graph based semantic representation called a", "there are other components of linguistic meaningsuch as figureground relationshipsthat are almost certainly relevant to we estimate caption quality by transforming both candidate and reference captions into a graph based semantic representation called a caption quality"]}, "scene graph": {"position": [[795, 796], [1959, 1960], [1983, 1984], [2193, 2194]], "sentence": ["the scene graph explicitly encodes the attributes objects, and relationships found in image captions", "our choice of semantic representation is the the scene graph explicitly encodes the attributes objects, and relationships found in image captions scene graph", "vision existing datasets [16 15 28] and the recently released visual genome dataset the [29]. our choice of semantic representation is the the scene graph explicitly encodes the attributes objects, and relationships found in image captions scene graph scene graph of candidate caption c is denoted by g(c)", "an etc. example of a parsed vision existing datasets [16 15 28] and the recently released visual genome dataset the [29]. our choice of semantic representation is the the scene graph explicitly encodes the attributes objects, and relationships found in image captions scene graph scene graph of candidate caption c is denoted by g(c) scene graph is illustrated in figure peter \\\\f6 2. anderson"]}, "complex image": {"position": [[839, 839]], "sentence": ["work recent has demonstrated scene graphs to be a highly effective representation performing for complex image retrieval queries [15 16]"]}, "image retrieval queries": {"position": [[840, 842]], "sentence": ["work recent has demonstrated scene graphs to be a highly effective representation performing for complex image retrieval queries [15 16]"]}, "caption": {"position": [[856, 856], [862, 862], [891, 891], [1804, 1804], [3755, 3755], [4173, 4173], [4198, 4198], [5083, 5083], [5494, 5494]], "sentence": ["and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph", "we demonstrate advantages similar when using this representation for caption to evaluation. parse an image and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph caption into a scene graph", "syntactic between dependencies words in the we demonstrate advantages similar when using this representation for caption to evaluation. parse an image and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph caption into a scene graph caption are established using a dependency parser pre-trained [19] on a large dataset", "parsing graphs appears to be a common theme spice 3 [16,17,18]. given metric a candidate syntactic between dependencies words in the we demonstrate advantages similar when using this representation for caption to evaluation. parse an image and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph caption into a scene graph caption are established using a dependency parser pre-trained [19] on a large dataset caption c and a set of reference captions s", "questions the capture the dimensions of overall parsing graphs appears to be a common theme spice 3 [16,17,18]. given metric a candidate syntactic between dependencies words in the we demonstrate advantages similar when using this representation for caption to evaluation. parse an image and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph caption into a scene graph caption are established using a dependency parser pre-trained [19] on a large dataset caption c and a set of reference captions s caption quality m2)", "they were asked to evaluate questions the capture the dimensions of overall parsing graphs appears to be a common theme spice 3 [16,17,18]. given metric a candidate syntactic between dependencies words in the we demonstrate advantages similar when using this representation for caption to evaluation. parse an image and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph caption into a scene graph caption are established using a dependency parser pre-trained [19] on a large dataset caption c and a set of reference captions s caption quality m2) caption by triples identifying which of the sentences", "is more similar to sentence where a?, sentence a is a reference they were asked to evaluate questions the capture the dimensions of overall parsing graphs appears to be a common theme spice 3 [16,17,18]. given metric a candidate syntactic between dependencies words in the we demonstrate advantages similar when using this representation for caption to evaluation. parse an image and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph caption into a scene graph caption are established using a dependency parser pre-trained [19] on a large dataset caption c and a set of reference captions s caption quality m2) caption by triples identifying which of the sentences caption", "at the is more similar to sentence where a?, sentence a is a reference they were asked to evaluate questions the capture the dimensions of overall parsing graphs appears to be a common theme spice 3 [16,17,18]. given metric a candidate syntactic between dependencies words in the we demonstrate advantages similar when using this representation for caption to evaluation. parse an image and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph caption into a scene graph caption are established using a dependency parser pre-trained [19] on a large dataset caption c and a set of reference captions s caption quality m2) caption by triples identifying which of the sentences caption caption level spice modestly outperforms existing metrics", "at the is more similar to sentence where a?, sentence a is a reference they were asked to evaluate questions the capture the dimensions of overall parsing graphs appears to be a common theme spice 3 [16,17,18]. given metric a candidate syntactic between dependencies words in the we demonstrate advantages similar when using this representation for caption to evaluation. parse an image and we demonstrate advantages similar when using this representation for caption to evaluation. parse an image caption into a scene graph caption into a scene graph caption are established using a dependency parser pre-trained [19] on a large dataset caption c and a set of reference captions s caption quality m2) caption by triples identifying which of the sentences caption caption level spice modestly outperforms existing metrics caption level classification accuracy of evaluation metrics at matching human judgment on pascal with 5 reference"]}, "graph": {"position": [[866, 866], [970, 970], [2234, 2234], [2315, 2315], [2406, 2406], [2663, 2663], [2717, 2717]], "sentence": ["when using this representation for caption to evaluation. parse an image caption into a scene graph", "computes score defined over the conjunction of logical tuples representing propositions semantic in the scene when using this representation for caption to evaluation. parse an image caption into a scene graph graph (e g , figure 1 right)", "a scene graph right parsed from a set of reference captions image our (left) scene computes score defined over the conjunction of logical tuples representing propositions semantic in the scene when using this representation for caption to evaluation. parse an image caption into a scene graph graph (e g , figure 1 right) graph implementation departs slightly from previous work in image retrieval [15 16]", "simplifies it scene a scene graph right parsed from a set of reference captions image our (left) scene computes score defined over the conjunction of logical tuples representing propositions semantic in the scene when using this representation for caption to evaluation. parse an image caption into a scene graph graph (e g , figure 1 right) graph implementation departs slightly from previous work in image retrieval [15 16] graph alignment and ensures that each incorrect numeric modifier is only counted as a single to", "which comprise together the scene simplifies it scene a scene graph right parsed from a set of reference captions image our (left) scene computes score defined over the conjunction of logical tuples representing propositions semantic in the scene when using this representation for caption to evaluation. parse an image caption into a scene graph graph (e g , figure 1 right) graph implementation departs slightly from previous work in image retrieval [15 16] graph alignment and ensures that each incorrect numeric modifier is only counted as a single to graph", "we view semantic the relations in the scene which comprise together the scene simplifies it scene a scene graph right parsed from a set of reference captions image our (left) scene computes score defined over the conjunction of logical tuples representing propositions semantic in the scene when using this representation for caption to evaluation. parse an image caption into a scene graph graph (e g , figure 1 right) graph implementation departs slightly from previous work in image retrieval [15 16] graph alignment and ensures that each incorrect numeric modifier is only counted as a single to graph graph as a conjunction of logical propositions", "the scene we view semantic the relations in the scene which comprise together the scene simplifies it scene a scene graph right parsed from a set of reference captions image our (left) scene computes score defined over the conjunction of logical tuples representing propositions semantic in the scene when using this representation for caption to evaluation. parse an image caption into a scene graph graph (e g , figure 1 right) graph implementation departs slightly from previous work in image retrieval [15 16] graph alignment and ensures that each incorrect numeric modifier is only counted as a single to graph graph as a conjunction of logical propositions graph in figure would 1 be represented with the following { tuples: (girl)"]}, "graphs": {"position": [[938, 938], [2036, 2036], [6355, 6355]], "sentence": ["we map from dependency trees to scene graphs using a system rule-based [16]", "next present we the semantic parsing step to generate scene we map from dependency trees to scene graphs using a system rule-based [16] graphs from semantic 3.1 captions. parsingcaptions to scene we graphs define the subtask of parsing captions", "c d : generating semantically precise scene next present we the semantic parsing step to generate scene we map from dependency trees to scene graphs using a system rule-based [16] graphs from semantic 3.1 captions. parsingcaptions to scene we graphs define the subtask of parsing captions graphs from textual descriptions for improved image retrieval"]}, "semantic propositional image": {"position": [[984, 985], [2493, 2494], [4337, 4338]], "sentence": ["we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations", "\\\\fspice: instead, we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations semantic propositional image caption numeric 7 evaluation modifiers are encoded as object attributes", "\\\\fspice: spice \\\\fspice: instead, we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations semantic propositional image caption numeric 7 evaluation modifiers are encoded as object attributes semantic propositional image caption fig. 11 evaluation 3"]}, "image caption": {"position": [[986, 987], [1324, 1325], [1339, 1340], [1768, 1769], [3385, 3386], [5061, 5062], [5774, 5775], [7157, 7158]], "sentence": ["we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations", "and were subsequently for adopted we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations image caption several evaluation. studies have analyzed the performance of gram metrics when for used image caption", "image caption several evaluation. studies have analyzed the performance of gram metrics when for used and were subsequently for adopted we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations image caption several evaluation. studies have analyzed the performance of gram metrics when for used image caption image caption evaluation", "we found that amr representations using similarity smatch performed poorly as image caption several evaluation. studies have analyzed the performance of gram metrics when for used and were subsequently for adopted we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations image caption several evaluation. studies have analyzed the performance of gram metrics when for used image caption image caption evaluation image caption representations", "we generate pairwise rankings by comparing \\\\fspice: scores semantic propositional we found that amr representations using similarity smatch performed poorly as image caption several evaluation. studies have analyzed the performance of gram metrics when for used and were subsequently for adopted we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations image caption several evaluation. studies have analyzed the performance of gram metrics when for used image caption image caption evaluation image caption representations image caption table 9 evaluation 1", "achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional we generate pairwise rankings by comparing \\\\fspice: scores semantic propositional we found that amr representations using similarity smatch performed poorly as image caption several evaluation. studies have analyzed the performance of gram metrics when for used and were subsequently for adopted we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations image caption several evaluation. studies have analyzed the performance of gram metrics when for used image caption image caption evaluation image caption representations image caption table 9 evaluation 1 image caption table 13 evaluation 3", "jacob devlin and qi wu for providing \\\\fspice: model- semantic propositional achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional we generate pairwise rankings by comparing \\\\fspice: scores semantic propositional we found that amr representations using similarity smatch performed poorly as image caption several evaluation. studies have analyzed the performance of gram metrics when for used and were subsequently for adopted we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations image caption several evaluation. studies have analyzed the performance of gram metrics when for used image caption image caption evaluation image caption representations image caption table 9 evaluation 1 image caption table 13 evaluation 3 image caption generated 15 evaluation captions for evaluation", "\\\\fspice: (2015) semantic propositional jacob devlin and qi wu for providing \\\\fspice: model- semantic propositional achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional we generate pairwise rankings by comparing \\\\fspice: scores semantic propositional we found that amr representations using similarity smatch performed poorly as image caption several evaluation. studies have analyzed the performance of gram metrics when for used and were subsequently for adopted we dub this spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations image caption several evaluation. studies have analyzed the performance of gram metrics when for used image caption image caption evaluation image caption representations image caption table 9 evaluation 1 image caption table 13 evaluation 3 image caption generated 15 evaluation captions for evaluation image caption 40. 17 evaluation devlin"]}, "related caption": {"position": [[1117, 1117]], "sentence": ["as which caption generator best understands colors? can and caption generators background 2 count? and related caption 2.1 work evaluation there metrics is a considerable amount of work dedicated to the"]}, "caption 2.1 work": {"position": [[1118, 1119]], "sentence": ["which caption generator best understands colors? can and caption generators background 2 count? and related caption 2.1 work evaluation there metrics is a considerable amount of work dedicated to the development of"]}, "work evaluation": {"position": [[1120, 1121]], "sentence": ["generator best understands colors? can and caption generators background 2 count? and related caption 2.1 work evaluation there metrics is a considerable amount of work dedicated to the development of that metrics"]}, "sentence": {"position": [[1174, 1174]], "sentence": ["stephen metrics gould are posed as similarity measures that compare a candidate sentence a to set of reference or ground truth sentences"]}, "caption evaluation": {"position": [[1193, 1194], [1850, 1851], [1858, 1859]], "sentence": ["most of the metrics in common for use caption evaluation are based on gram matching", "for the purposes of most of the metrics in common for use caption evaluation are based on gram matching caption evaluation the is image disregarded", "posing for the purposes of most of the metrics in common for use caption evaluation are based on gram matching caption evaluation the is image disregarded caption evaluation as a purely linguistic task similar machine to translation mt evaluation"]}, "align sentences": {"position": [[1237, 1238]], "sentence": ["and synonym paraphrase matches between grams to align sentences"]}, "text summaries f-measures.": {"position": [[1263, 1264]], "sentence": ["rouge is [11] a package of a measures for automatic evaluation of text summaries f-measures. using cider applies term frequency inverse document frequency tfidf weights to grams in the"]}, "translations": {"position": [[1316, 1316]], "sentence": ["these methods were originally developed for the evaluation text of summaries or machine translations (mt)"]}, "mt evaluation": {"position": [[1434, 1435]], "sentence": ["greater agreement with human consensus than and bleu rouge within [12]. the context of automatic mt evaluation"]}, "sentence similarity": {"position": [[1493, 1494]], "sentence": ["sentence similarity is calculated by first matching semantic frames across sentences by starting with the verbs at"]}, "noun object matching": {"position": [[1574, 1576]], "sentence": ["which allow for noun object matching between captions"]}, "identify semantic": {"position": [[1618, 1618]], "sentence": ["this work required collection the of a purpose built dataset in order to learn to identify semantic tuples"]}, "semantic tuples": {"position": [[1619, 1620]], "sentence": ["work required collection the of a purpose built dataset in order to learn to identify semantic tuples"]}, "and": {"position": [[1660, 1660], [3437, 3437]], "sentence": ["have been used in a number recent of works within the context of image and video retrieval systems to performance improve on complex queries [18,15,16]", "representing detailedness correctness, have been used in a number recent of works within the context of image and video retrieval systems to performance improve on complex queries [18,15,16] and percentage m5 m4 m3 m2 m1 (0.000) (0.000) (0.007) (0.001) (0.142) (0.265) (0.091) 0.88 0.84"]}, "retrieval": {"position": [[1662, 1662]], "sentence": ["been used in a number recent of works within the context of image and video retrieval systems to performance improve on complex queries [18,15,16]"]}, "complex queries": {"position": [[1668, 1669]], "sentence": ["of works within the context of image and video retrieval systems to performance improve on complex queries [18,15,16]"]}, "transforming a sentence": {"position": [[1699, 1701]], "sentence": ["from natural language \\\\fspice: descriptions semantic propositional image caption [18,16]. 5 evaluation the task of transforming a sentence into its meaning representation also has received considerable attention within the computational linguistics community"]}, "computational linguistics": {"position": [[1713, 1714]], "sentence": ["of transforming a sentence into its meaning representation also has received considerable attention within the computational linguistics community"]}, "semantic graphs": {"position": [[1725, 1726]], "sentence": ["recent work has proposed a common framework for semantic graphs an called abstract meaning representation amr [24]"]}, "semantic parsing graphs": {"position": [[1788, 1790]], "sentence": ["the use of dependency trees as the starting point for semantic parsing graphs appears to be a common theme spice 3 [16,17,18]. given metric a candidate caption c"]}, "sm": {"position": [[1823, 1823]], "sentence": ["sm associated } with an image"]}, "translation mt": {"position": [[1868, 1869]], "sentence": ["posing caption evaluation as a purely linguistic task similar machine to translation mt evaluation"]}, "image": {"position": [[1898, 1898]], "sentence": ["our approach is suited better to evaluating computer generated image first, captions. we transform both candidate caption and reference captions into an intermediate representation that"]}, "captions.": {"position": [[1900, 1900]], "sentence": ["our approach is suited better to evaluating computer generated image first, captions. we transform both candidate caption and reference captions into an intermediate representation that encodes semantic"]}, "candidate caption": {"position": [[1904, 1905]], "sentence": ["our approach is suited better to evaluating computer generated image first, captions. we transform both candidate caption and reference captions into an intermediate representation that encodes semantic propositional content"]}, "reference captions": {"position": [[1907, 1908], [4082, 4083], [5280, 5281]], "sentence": ["suited better to evaluating computer generated image first, captions. we transform both candidate caption and reference captions into an intermediate representation that encodes semantic propositional content", "candidate captions were from sourced the human suited better to evaluating computer generated image first, captions. we transform both candidate caption and reference captions into an intermediate representation that encodes semantic propositional content reference captions and two recent captioning models pascal-50s [36,35]. to create the pascal dataset [12]", "each candidate pair most commonly preferred by human to evaluators. help quantify the impact of candidate captions were from sourced the human suited better to evaluating computer generated image first, captions. we transform both candidate caption and reference captions into an intermediate representation that encodes semantic propositional content reference captions and two recent captioning models pascal-50s [36,35]. to create the pascal dataset [12] reference captions on performance"]}, "semantic propositional": {"position": [[1915, 1916]], "sentence": ["captions. we transform both candidate caption and reference captions into an intermediate representation that encodes semantic propositional content"]}, "linguistic meaningsuch": {"position": [[1929, 1930]], "sentence": ["while are we aware that there are other components of linguistic meaningsuch as figureground relationshipsthat are almost certainly relevant to caption quality"]}, "semantic meaning": {"position": [[1949, 1950]], "sentence": ["this in work we focus exclusively on semantic meaning"]}, "candidate caption c": {"position": [[1986, 1987]], "sentence": ["[16 15 28] and the recently released visual genome dataset the [29]. scene graph of candidate caption c is denoted by g(c)"]}, "graph the": {"position": [[1998, 1999]], "sentence": ["and the scene for graph the reference captions s is denoted by g(s)"]}, "captions s": {"position": [[2001, 2002]], "sentence": ["and the scene for graph the reference captions s is denoted by g(s)"]}, "si": {"position": [[2015, 2015]], "sentence": ["formed as the union of graphs scene si for each si s and combining synonymous object nodes"]}, "captions. parsingcaptions": {"position": [[2040, 2041]], "sentence": ["next present we the semantic parsing step to generate scene graphs from semantic 3.1 captions. parsingcaptions to scene we graphs define the subtask of parsing captions to scene graphs as follows"]}, "parsing captions": {"position": [[2050, 2051]], "sentence": ["scene graphs from semantic 3.1 captions. parsingcaptions to scene we graphs define the subtask of parsing captions to scene graphs as follows"]}, "object classes": {"position": [[2062, 2063]], "sentence": ["given set a of object classes c"]}, "g(c)": {"position": [[2091, 2091]], "sentence": ["we parse c to a scene g(c) graph: ho(c)"]}, "mentions": {"position": [[2106, 2106]], "sentence": ["where (1) k(c)i c is the set of object mentions in c"]}, "relations": {"position": [[2118, 2118]], "sentence": ["r is set the of hyper edges representing relations between objects"]}, "relation attributes": {"position": [[2170, 2171]], "sentence": ["relation attributes and that can be represented"]}, "parsed scene": {"position": [[2192, 2192]], "sentence": ["an etc. example of a parsed scene graph is illustrated in figure peter \\\\f6 2. anderson"]}, "graph right": {"position": [[2221, 2221]], "sentence": ["a typical example of a scene graph right parsed from a set of reference captions image our (left) scene graph implementation departs"]}, "right parsed": {"position": [[2222, 2223]], "sentence": ["a typical example of a scene graph right parsed from a set of reference captions image our (left) scene graph implementation departs slightly from"]}, "image retrieval": {"position": [[2242, 2243], [6319, 6320], [6361, 6362]], "sentence": ["of reference captions image our (left) scene graph implementation departs slightly from previous work in image retrieval [15 16]", "l : of reference captions image our (left) scene graph implementation departs slightly from previous work in image retrieval [15 16] image retrieval using scene graphs", "c d : generating semantically precise scene graphs from textual descriptions for improved l : of reference captions image our (left) scene graph implementation departs slightly from previous work in image retrieval [15 16] image retrieval using scene graphs image retrieval"]}, "scene graph parser": {"position": [[2348, 2350]], "sentence": ["we adopt a variant of the rule based version the of stanford scene graph parser [16]"]}, "plural": {"position": [[2377, 2377]], "sentence": ["resolve pronouns and handle plural the nouns. resulting tree structure is then parsed according to nine simple rules linguistic to"]}, "lemmatized": {"position": [[2394, 2394]], "sentence": ["nouns. resulting tree structure is then parsed according to nine simple rules linguistic to extract lemmatized objects"]}, "object": {"position": [[2436, 2436]], "sentence": ["results which in the object mention girl with attribute young"]}, "image caption numeric": {"position": [[2495, 2496]], "sentence": ["\\\\fspice: instead, semantic propositional image caption numeric 7 evaluation modifiers are encoded as object attributes"]}, "numeric 7": {"position": [[2497, 2498]], "sentence": ["\\\\fspice: instead, semantic propositional image caption numeric 7 evaluation modifiers are encoded as object attributes"]}, "modifiers": {"position": [[2500, 2500]], "sentence": ["\\\\fspice: instead, semantic propositional image caption numeric 7 evaluation modifiers are encoded as object attributes"]}, "graph parser": {"position": [[2553, 2554]], "sentence": ["easily handled by our semantic proposition score notwithstanding calculation. the use of the stanford scene graph parser"]}, "semantic parsing": {"position": [[2582, 2583], [6558, 6559]], "sentence": ["it is our that hope ongoing advances in syntactic and semantic parsing will allow spice to further be improved in future releases", "d : shallow it is our that hope ongoing advances in syntactic and semantic parsing will allow spice to further be improved in future releases semantic parsing using support vector machines"]}, "evaluate captions": {"position": [[2613, 2613]], "sentence": ["in principle it could be used to evaluate captions on scene datasets graph [16 15 28] that have no reference captions at all"]}, "tuples": {"position": [[2681, 2681]], "sentence": ["tuples. or we define the function t that returns logical tuples from a scene t as: graph"]}, "the wmt": {"position": [[3287, 3287]], "sentence": ["further described our below. choice of correlation coefficients is consistent with an emerging consensus from the wmt metrics shared task [33 34] for scoring machine metrics. translation to evaluate system level"]}, "wmt metrics shared task": {"position": [[3288, 3291]], "sentence": ["described our below. choice of correlation coefficients is consistent with an emerging consensus from the wmt metrics shared task [33 34] for scoring machine metrics. translation to evaluate system level correlation"]}, "scoring machine metrics. translation": {"position": [[3295, 3298]], "sentence": ["is consistent with an emerging consensus from the wmt metrics shared task [33 34] for scoring machine metrics. translation to evaluate system level correlation"]}, "level": {"position": [[3302, 3302]], "sentence": ["the wmt metrics shared task [33 34] for scoring machine metrics. translation to evaluate system level correlation"]}, "human captions": {"position": [[3408, 3409], [5538, 5539]], "sentence": ["level pearsons correlation between evaluation metrics and judgments human for the 15 competition entries plus human captions in the 2015 captioning coco challenge [6]", "meteor is best differentiating at human and model captions hm and level pearsons correlation between evaluation metrics and judgments human for the 15 competition entries plus human captions in the 2015 captioning coco challenge [6] human captions where one incorrect is (hi)"]}, "captioning coco challenge": {"position": [[3413, 3415]], "sentence": ["metrics and judgments human for the 15 competition entries plus human captions in the 2015 captioning coco challenge [6]"]}, "m5 m4": {"position": [[3439, 3439]], "sentence": ["representing detailedness correctness, and percentage m5 m4 m3 m2 m1 (0.000) (0.000) (0.007) (0.001) (0.142) (0.265) (0.091) 0.88 0.84 0.43 0.53"]}, "m4 m3": {"position": [[3440, 3441]], "sentence": ["representing detailedness correctness, and percentage m5 m4 m3 m2 m1 (0.000) (0.000) (0.007) (0.001) (0.142) (0.265) (0.091) 0.88 0.84 0.43 0.53 0.15 0.05"]}, "coco 2014": {"position": [[3628, 3629]], "sentence": ["coco 2014"]}, "validation": {"position": [[3652, 3652]], "sentence": ["293 split images, into an 82 783 image training set and a 40 504 image validation set"]}, "2015 coco": {"position": [[3706, 3706], [4312, 4312], [5720, 5720]], "sentence": ["were collected using amazon mechanical (amt) turk for the purpose of evaluating submissions to the 2015 coco challenge captioning [6]", "1 we report system level correlations between metrics and human judgments over entries in the were collected using amazon mechanical (amt) turk for the purpose of evaluating submissions to the 2015 coco challenge captioning [6] 2015 coco captioning challenge [6]", "and piotr doll ar) for agreeing to run our code spice against entries in the 1 we report system level correlations between metrics and human judgments over entries in the were collected using amazon mechanical (amt) turk for the purpose of evaluating submissions to the 2015 coco challenge captioning [6] 2015 coco captioning challenge [6] 2015 coco captioning challenge"]}, "coco challenge captioning": {"position": [[3707, 3709]], "sentence": ["collected using amazon mechanical (amt) turk for the purpose of evaluating submissions to the 2015 coco challenge captioning [6]"]}, "m2)": {"position": [[3757, 3757]], "sentence": ["questions the capture the dimensions of overall caption quality m2)"]}, "amt evaluators": {"position": [[3802, 3803]], "sentence": ["all amt evaluators consisted us of located native speakers"]}, "captions. 8k": {"position": [[3871, 3872]], "sentence": ["evaluating coco. on at no stage were we given access to the coco test flickr captions. 8k"]}, "evaluators": {"position": [[3951, 3951]], "sentence": ["each caption was scored by three human expert evaluators sourced from a pool of native speakers"]}, "candidate captions": {"position": [[4075, 4076]], "sentence": ["candidate captions were from sourced the human reference captions and two recent captioning models pascal-50s [36,35]. to"]}, "pascal dataset [12]": {"position": [[4094, 4096]], "sentence": ["sourced the human reference captions and two recent captioning models pascal-50s [36,35]. to create the pascal dataset [12]"]}, "imagewere": {"position": [[4111, 4111]], "sentence": ["1 000 images from uiuc the pascal sentence dataset originally containing five captions imagewere per annotated with 50 captions each using amt"]}, "candidate": {"position": [[4145, 4145]], "sentence": ["vehicles and household the objects. dataset also includes human judgments over 4 000 candidate pairs. sentence however"]}, "images": {"position": [[4164, 4164]], "sentence": ["amt workers were not asked to evaluate captions against images"]}, "sentences": {"position": [[4180, 4180]], "sentence": ["they were asked to evaluate caption by triples identifying which of the sentences"]}, "captions reference": {"position": [[4208, 4209]], "sentence": ["if captions reference vary in quality"]}, "evaluation": {"position": [[4222, 4222]], "sentence": ["this approach may inject more noise into the evaluation process"]}, "human evaluations": {"position": [[4236, 4237]], "sentence": ["however the differences between this approach and the approaches previous to human evaluations have not been studied"]}, "sentence candidate": {"position": [[4245, 4245]], "sentence": ["for each sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference captions"]}, "candidate pair (b c) evaluations": {"position": [[4246, 4250]], "sentence": ["for each sentence candidate pair (b c) evaluations were collected against 48 of the 50 possible reference captions"]}, "correct": {"position": [[4279, 4279]], "sentence": ["paired in four ways human correct (hc)"]}, "(hi),": {"position": [[4285, 4285]], "sentence": ["human incorrect human-model (hi), (hm)"]}, "coco captioning challenge": {"position": [[4313, 4315], [5721, 5723]], "sentence": ["we report system level correlations between metrics and human judgments over entries in the 2015 coco captioning challenge [6]", "piotr doll ar) for agreeing to run our code spice against entries in the 2015 we report system level correlations between metrics and human judgments over entries in the 2015 coco captioning challenge [6] coco captioning challenge"]}, "image caption fig.": {"position": [[4339, 4340]], "sentence": ["\\\\fspice: spice semantic propositional image caption fig. 11 evaluation 3"]}, "fig. 11": {"position": [[4341, 4342]], "sentence": ["\\\\fspice: spice semantic propositional image caption fig. 11 evaluation 3"]}, "metrics": {"position": [[4347, 4347]], "sentence": ["evaluation metrics vs"]}, "2015 captioning coco challenge": {"position": [[4358, 4361]], "sentence": ["human judgments for the 15 entries in the 2015 captioning coco challenge"]}, "challenge entries": {"position": [[4386, 4387]], "sentence": ["only spice scores human generated captions higher significantly than challenge entries"]}, "quality judgments": {"position": [[4408, 4409]], "sentence": ["reaching a correlation coefficient 0.88 of with human quality judgments (m1)"]}, "spice caption": {"position": [[4456, 4457]], "sentence": ["only rewards spice caption detail (m4)"]}, "strengths": {"position": [[4582, 4582]], "sentence": ["existing questions gram evaluation metrics have little to offer in terms of the understanding relative strengths and weaknesses"]}, "human count": {"position": [[4659, 4660]], "sentence": ["their use of object attributes, color none of the models exhibits a convincing ability to human count msr [6] google [38] msr [39] captivator berkeley [40] lrcn montreal/toronto [1] m-rnn [2] nearest"]}, "color perception": {"position": [[4911, 4912]], "sentence": ["table 2 we review the performance of coco 2015 captioning challenge submissions in terms of color perception"]}, "ability, counting": {"position": [[4914, 4915]], "sentence": ["ability, counting and understanding of size attributes by using word lists to isolate attribute tuples that contain"]}, "of size": {"position": [[4918, 4918]], "sentence": ["ability, counting and understanding of size attributes by using word lists to isolate attribute tuples that contain colors"]}, "size attributes": {"position": [[4919, 4920]], "sentence": ["ability, counting and understanding of size attributes by using word lists to isolate attribute tuples that contain colors"]}, "attribute tuples": {"position": [[4927, 4928]], "sentence": ["ability, counting and understanding of size attributes by using word lists to isolate attribute tuples that contain colors"]}, "tuples containing color": {"position": [[4994, 4995]], "sentence": ["verbs and adjectivesexceeds the human baseline f-score for tuples containing color attributes"]}, "color attributes": {"position": [[4996, 4997]], "sentence": ["verbs and adjectivesexceeds the human baseline f-score for tuples containing color attributes"]}, "caption-level 4.4": {"position": [[5014, 5015]], "sentence": ["there is less that evidence any of these models have learned to count caption-level 4.4 objects. in correlation table 3 we report caption level correlations between automated metrics human and"]}, "human": {"position": [[5029, 5029]], "sentence": ["caption-level 4.4 objects. in correlation table 3 we report caption level correlations between automated metrics human and judgments on flickr 8k and the composite dataset [35]"]}, "propositional image": {"position": [[5060, 5060], [5773, 5773], [7156, 7156]], "sentence": ["spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional image caption table 13 evaluation 3", "jacob devlin and qi wu for providing \\\\fspice: model- semantic spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional image caption table 13 evaluation 3 propositional image caption generated 15 evaluation captions for evaluation", "\\\\fspice: (2015) semantic jacob devlin and qi wu for providing \\\\fspice: model- semantic spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional image caption table 13 evaluation 3 propositional image caption generated 15 evaluation captions for evaluation propositional image caption 40. 17 evaluation devlin"]}, "13 evaluation": {"position": [[5064, 5064]], "sentence": ["correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional image caption table 13 evaluation 3"]}, "evaluation 3": {"position": [[5065, 5066]], "sentence": ["coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional image caption table 13 evaluation 3"]}, "4": {"position": [[5163, 5163]], "sentence": ["as reported in section 4 2, spice more approximates closely human judgment when aggregated over more captions"]}, "mm": {"position": [[5388, 5388]], "sentence": ["spice performs best in terms of distinguishing two between model generated captions mm pairs as illustrated in table 4 and figure right. 4 this is important as distinguishing"]}, "natural image captioning spice": {"position": [[5449, 5451]], "sentence": ["on natural image captioning spice datasets, captures human judgment over model generated captions better peter \\\\f14 than anderson"]}, "judgment": {"position": [[5456, 5456], [5487, 5487], [5504, 5504]], "sentence": ["on natural image captioning spice datasets, captures human judgment over model generated captions better peter \\\\f14 than anderson", "pairwise classification accuracy of automated metrics at matching human on natural image captioning spice datasets, captures human judgment over model generated captions better peter \\\\f14 than anderson judgment with reference table captions 4", "caption level classification accuracy of evaluation metrics at matching human pairwise classification accuracy of automated metrics at matching human on natural image captioning spice datasets, captures human judgment over model generated captions better peter \\\\f14 than anderson judgment with reference table captions 4 judgment on pascal with 5 reference captions"]}, "model generated captions": {"position": [[5458, 5460]], "sentence": ["on natural image captioning spice datasets, captures human judgment over model generated captions better peter \\\\f14 than anderson"]}, "pairwise classification": {"position": [[5478, 5479]], "sentence": ["pairwise classification accuracy of automated metrics at matching human judgment with reference table captions 4"]}, "classification": {"position": [[5496, 5496]], "sentence": ["caption level classification accuracy of evaluation metrics at matching human judgment on pascal with 5 reference captions"]}, "human matching judgments": {"position": [[5516, 5518]], "sentence": ["spice is best at human matching judgments on pairs of model generated captions (mm)"]}, "human and model": {"position": [[5532, 5533]], "sentence": ["meteor is best differentiating at human and model captions hm and human captions where one incorrect is (hi)"]}, "model captions": {"position": [[5534, 5535]], "sentence": ["meteor is best differentiating at human and model captions hm and human captions where one incorrect is (hi)"]}, "better captioning acknowledgements models.": {"position": [[5680, 5683]], "sentence": ["we release our code and hope that work our will help in the development of better captioning acknowledgements models. we are grateful to the coco consortium in particular"]}, "code spice": {"position": [[5714, 5714]], "sentence": ["yin cui and piotr doll ar) for agreeing to run our code spice against entries in the 2015 coco captioning challenge"]}, "scene graph parser code": {"position": [[5737, 5740]], "sentence": ["we would also like thank to sebastian schuster for sharing the stanford scene graph parser code in of advance public release"]}, "caption human": {"position": [[5755, 5756]], "sentence": ["ramakrishna vedantam and somak aditya for sharing their caption human judgments"]}, "1.": {"position": [[5795, 5795]], "sentence": ["this work was funded in part by the centre australian for robotic 1. references vision. donahue"]}, "image caption generation": {"position": [[5878, 5880]], "sentence": ["attend and tell neural image caption generation with attention. visual arxiv preprint arxiv:1502 03044 3. (2015) hodosh"]}, "image description": {"position": [[5903, 5904], [6222, 6223]], "sentence": ["j : framing image description as a task: ranking data", "d : cider consensus based j : framing image description as a task: ranking data image description evaluation"]}, "semantic inference": {"position": [[5946, 5947]], "sentence": ["j : from image descriptions to denotations: visual new similarity metrics for semantic inference over event tacl descriptions. 2 5. 6778 lin"]}, "tacl": {"position": [[5950, 5950]], "sentence": [": from image descriptions to denotations: visual new similarity metrics for semantic inference over event tacl descriptions. 2 5. 6778 lin"]}, "microsoft coco": {"position": [[5986, 5986]], "sentence": ["zitnick, p., c l : microsoft coco common objects in context"]}, "coco common objects": {"position": [[5987, 5989]], "sentence": ["zitnick, p., c l : microsoft coco common objects in context"]}, "pami": {"position": [[6077, 6077]], "sentence": ["berg, a.c., t l : babytalk understanding and generating simple image pami descriptions. 8. 28912903 elliott"]}, "description": {"position": [[6095, 6095], [6999, 6999]], "sentence": ["f : comparing automatic evaluation measures for image description", "y : from images to sentences through scene f : comparing automatic evaluation measures for image description description graphs using commonsense reasoning and knowledge"]}, "description generation": {"position": [[6137, 6138]], "sentence": ["b : automatic description generation from a images: survey of models"]}, "machine translation": {"position": [[6179, 6180], [6530, 6531], [6940, 6941], [6968, 6969]], "sentence": ["w : bleu a method for automatic evaluation of machine translation", "acl in: seventh workshop on statistical w : bleu a method for automatic evaluation of machine translation machine translation", "in acl tenth workshop on statistical acl in: seventh workshop on statistical w : bleu a method for automatic evaluation of machine translation machine translation machine translation", "in ninth acl workshop on statistical in acl tenth workshop on statistical acl in: seventh workshop on statistical w : bleu a method for automatic evaluation of machine translation machine translation machine translation machine translation"]}, "specific translation": {"position": [[6242, 6242]], "sentence": ["a : meteor universal language specific translation evaluation for any target language"]}, "translation evaluation": {"position": [[6243, 6244]], "sentence": ["a : meteor universal language specific translation evaluation for any target language"]}, "statistical translation. machine 14.": {"position": [[6255, 6258]], "sentence": ["in eacl 2014 workshop on statistical translation. machine 14. (2014) gim enez"]}, "machine 15.": {"position": [[6285, 6286]], "sentence": ["in acl second workshop on statistical machine 15. translation johnson"]}, "vision and": {"position": [[6369, 6370]], "sentence": ["in emnlp 4th workshop on vision and language"]}, "parsing. hlt-naacl": {"position": [[6394, 6395]], "sentence": ["s : a transition based algorithm for amr in: parsing. hlt-naacl"]}, "visual semantic": {"position": [[6415, 6415]], "sentence": ["r : visual semantic search videos retrieving via complex textual queries"]}, "semantic search": {"position": [[6416, 6417]], "sentence": ["r : visual semantic search videos retrieving via complex textual queries"]}, "statistical machine": {"position": [[6529, 6529]], "sentence": ["acl in: seventh workshop on statistical machine translation"]}, "shallow semantic": {"position": [[6557, 6557]], "sentence": ["d : shallow semantic parsing using support vector machines"]}, "image sentence generation": {"position": [[6599, 6601]], "sentence": ["f , quattoni , a : semantic tuples for evaluation of image sentence generation"]}, "vision and language": {"position": [[6607, 6609]], "sentence": ["4th emnlp workshop on vision and language 24. (2015) banarescu"]}, "abstract meaning": {"position": [[6650, 6651]], "sentence": ["n : abstract meaning (amr) representation 1 0 specification"]}, "meaning representation": {"position": [[6689, 6690]], "sentence": ["n a : a graph-based discriminative parser for the abstract meaning representation"]}, "representation": {"position": [[6715, 6715]], "sentence": ["c : robust subgraph generation improves abstract meaning representation parsing"]}, "richer": {"position": [[6781, 6781]], "sentence": ["s : flickr30k entities collecting region to phrase correspondences for image-to-sentence richer models"]}, "syntactic comprehension": {"position": [[6902, 6903]], "sentence": ["r : expectation based syntactic comprehension"]}, "cognition 33.": {"position": [[6905, 6906]], "sentence": ["cognition 33. 11261177 (2008) stanojevi c"]}, "the wmt15": {"position": [[6928, 6928]], "sentence": ["o : results of the wmt15 shared metrics task"]}, "wmt15 shared metrics task": {"position": [[6929, 6932]], "sentence": ["o : results of the wmt15 shared metrics task"]}, "the wmt14": {"position": [[6956, 6956]], "sentence": ["o : results of the wmt14 metrics shared task"]}, "wmt14 metrics shared task": {"position": [[6957, 6960]], "sentence": ["o : results of the wmt14 metrics shared task"]}, "neural caption": {"position": [[7145, 7145]], "sentence": ["d : show and tell a neural caption image generator"]}, "caption image generator": {"position": [[7146, 7148]], "sentence": ["d : show and tell a neural caption image generator"]}, "image captioning": {"position": [[7195, 7196], [7280, 7281], [7368, 7369], [7456, 7457], [7544, 7545], [7632, 7633], [7720, 7721], [7808, 7809], [7896, 7897], [7984, 7985], [8072, 8073]], "sentence": ["g , m.: mitchell, language models for image captioning the quirks and what works", "c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning image captioning image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning image captioning image captioning image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning image captioning image captioning image captioning image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning image captioning image captioning image captioning image captioning image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning image captioning image captioning image captioning image captioning image captioning image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning image captioning image captioning image captioning image captioning image captioning image captioning image captioning", "c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for c l : exploring neighbor nearest approaches for g , m.: mitchell, language models for image captioning the quirks and what works image captioning image captioning image captioning image captioning image captioning image captioning image captioning image captioning image captioning image captioning"]}, "concept": {"position": [[7241, 7241], [7329, 7329], [7417, 7417], [7505, 7505], [7593, 7593], [7681, 7681], [7769, 7769], [7857, 7857], [7945, 7945], [8033, 8033]], "sentence": ["a l : learning like child: a fast novel visual concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images", "a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual a l : learning like child: a fast novel visual concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images concept learning from sentence descriptions of images"]}}, "METHOD": {"gram": {"position": [[71, 71], [476, 476], [481, 481], [502, 502], [1004, 1004], [1198, 1198], [4569, 4569]], "sentence": ["automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task of simulating human judgment", "an score calculated over tuples in the candidate reference and scene marily graphs sensitive to automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task of simulating human judgment gram overlap", "an score calculated over tuples in the candidate reference and scene marily graphs sensitive to automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task of simulating human judgment gram overlap gram overlap is neither nor necessary sufficient for two sentences to convey the same meaning to", "sufficient for two sentences to convey the same meaning to [14]. illustrate the limitations of an score calculated over tuples in the candidate reference and scene marily graphs sensitive to automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task of simulating human judgment gram overlap gram overlap is neither nor necessary sufficient for two sentences to convey the same meaning to gram comparisons", "we show that spice outperforms existing sufficient for two sentences to convey the same meaning to [14]. illustrate the limitations of an score calculated over tuples in the candidate reference and scene marily graphs sensitive to automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task of simulating human judgment gram overlap gram overlap is neither nor necessary sufficient for two sentences to convey the same meaning to gram comparisons gram metrics in terms of agreement with human of evaluations model generated captions", "most of the metrics in common for use caption evaluation are based on we show that spice outperforms existing sufficient for two sentences to convey the same meaning to [14]. illustrate the limitations of an score calculated over tuples in the candidate reference and scene marily graphs sensitive to automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task of simulating human judgment gram overlap gram overlap is neither nor necessary sufficient for two sentences to convey the same meaning to gram comparisons gram metrics in terms of agreement with human of evaluations model generated captions gram matching", "counting and other existing questions most of the metrics in common for use caption evaluation are based on we show that spice outperforms existing sufficient for two sentences to convey the same meaning to [14]. illustrate the limitations of an score calculated over tuples in the candidate reference and scene marily graphs sensitive to automatic existing evaluation metrics are primarily sensitive to gram which overlap, is neither necessary nor sufficient for the task of simulating human judgment gram overlap gram overlap is neither nor necessary sufficient for two sentences to convey the same meaning to gram comparisons gram metrics in terms of agreement with human of evaluations model generated captions gram matching gram evaluation metrics have little to offer in terms of the understanding relative strengths and weaknesses"]}, "semantic propositional content": {"position": [[90, 92], [416, 418]], "sentence": ["we hypothesize that semantic propositional content is important an component of human caption evaluation", "illustrates our methods main principle which uses we hypothesize that semantic propositional content is important an component of human caption evaluation semantic propositional content to assess the quality of image captions"]}, "spice.": {"position": [[114, 114]], "sentence": ["and propose a automated new caption evaluation metric defined over scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human"]}, "spice": {"position": [[127, 127], [169, 169], [981, 981], [1052, 1052], [1071, 1071], [1093, 1093], [1797, 1797], [2557, 2557], [2586, 2586], [2600, 2600], [2633, 2633], [2963, 2963], [2985, 2985], [3110, 3110], [3195, 3195], [3215, 3215], [3418, 3418], [3848, 3848], [4378, 4378], [4429, 4429], [4486, 4486], [4507, 4507], [4531, 4531], [4616, 4616], [4838, 4838], [5045, 5045], [5085, 5085], [5165, 5165], [5186, 5186], [5376, 5376], [5421, 5421], [5452, 5452], [5512, 5512], [5715, 5715]], "sentence": ["scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system", "scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators", "we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations", "our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice", "metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu", "rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors?", "trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s", "our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline", "it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases", "we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs", "evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of", "evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand", "evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to", "because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency", "a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this", "we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics", "we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5", "stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice", "only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries", "only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2)", "only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions", "only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries", "to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally", "to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object", "0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to", "at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional", "at the caption level at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional spice modestly outperforms existing metrics", "as reported in section 4 2, at the caption level at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional spice modestly outperforms existing metrics spice more approximates closely human judgment when aggregated over more captions", "with as reported in section 4 2, at the caption level at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional spice modestly outperforms existing metrics spice more approximates closely human judgment when aggregated over more captions spice achieving a rank coefficient correlation of 0 39, compared to 0 36 for cider and", "with as reported in section 4 2, at the caption level at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional spice modestly outperforms existing metrics spice more approximates closely human judgment when aggregated over more captions spice achieving a rank coefficient correlation of 0 39, compared to 0 36 for cider and spice performs best in terms of distinguishing two between model generated captions mm pairs as illustrated", "algorithms is primary the motivation for this conclusion 5 work. and future we work introduce with as reported in section 4 2, at the caption level at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional spice modestly outperforms existing metrics spice more approximates closely human judgment when aggregated over more captions spice achieving a rank coefficient correlation of 0 39, compared to 0 36 for cider and spice performs best in terms of distinguishing two between model generated captions mm pairs as illustrated spice", "on natural image captioning algorithms is primary the motivation for this conclusion 5 work. and future we work introduce with as reported in section 4 2, at the caption level at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional spice modestly outperforms existing metrics spice more approximates closely human judgment when aggregated over more captions spice achieving a rank coefficient correlation of 0 39, compared to 0 36 for cider and spice performs best in terms of distinguishing two between model generated captions mm pairs as illustrated spice spice datasets, captures human judgment over model generated captions better peter \\\\f14 than anderson", "on natural image captioning algorithms is primary the motivation for this conclusion 5 work. and future we work introduce with as reported in section 4 2, at the caption level at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional spice modestly outperforms existing metrics spice more approximates closely human judgment when aggregated over more captions spice achieving a rank coefficient correlation of 0 39, compared to 0 36 for cider and spice performs best in terms of distinguishing two between model generated captions mm pairs as illustrated spice spice datasets, captures human judgment over model generated captions better peter \\\\f14 than anderson spice is best at human matching judgments on pairs of model generated captions (mm)", "yin cui and piotr doll ar) for agreeing to run our code on natural image captioning algorithms is primary the motivation for this conclusion 5 work. and future we work introduce with as reported in section 4 2, at the caption level at the level, caption 0.138 0.046 0.008 0 000 0.000 0.000 0.000 0.029 0.008 0.009 0 004 ever, 0.000 to help understand the importance of we synonym-matching, also evaluated only stephen to gould calculate we compare a carefully ensemble tuned of metrics including because evaluation of under we also note that since it is our that hope ongoing advances in syntactic and semantic parsing will allow our trees as the starting point for semantic parsing graphs appears to be a common theme rouge cider and in terms of agreement with human evaluations 3. and we demonstrate that metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that our main contributions 1. are: we propose we dub this scene graphs spice. coined extensive evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system spice can questions answer such as which caption generator best understands colors? can and caption generators spice approach for semantic propositional image caption using evaluation. a range of datasets and human evaluations spice spice outperforms metrics bleu spice performance can be decomposed to questions answer such as which caption generator best understands colors? spice 3 [16,17,18]. given metric a candidate caption c and a set of reference captions s spice proposed metric is not tied to this particular parsing pipeline spice to further be improved in future releases spice operates scene on graphs spice these circumstances is left to future f-score 3.2 work. to calculation evaluate the similarity of spice is simple to understand spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to spice neglects fluency spice capturing various dimensions of would correctness most likely be the in experiments 4 best. this spice to existing caption evaluation metrics spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5 spice spice scores human generated captions higher significantly than challenge entries spice more accurately reflects judgment human overall m2) spice is the only metric to correctly rank humangenerated captions firstcider and meteor rank human captions spice is also the only metric to correctly select the non-human top-5 entries spice using exact matching only spice exact in table performance 1). degraded only marginally spice is comprised of object spice has the useful property that it is defined over tuples that are easy subdivide to spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional spice modestly outperforms existing metrics spice more approximates closely human judgment when aggregated over more captions spice achieving a rank coefficient correlation of 0 39, compared to 0 36 for cider and spice performs best in terms of distinguishing two between model generated captions mm pairs as illustrated spice spice datasets, captures human judgment over model generated captions better peter \\\\f14 than anderson spice is best at human matching judgments on pairs of model generated captions (mm) spice against entries in the 2015 coco captioning challenge"]}, "captions": {"position": [[132, 132], [2936, 2936], [3090, 3090], [3123, 3123], [3574, 3574], [3887, 3887], [3963, 3963], [4034, 4034], [4045, 4045], [4557, 4557], [5289, 5289], [6028, 6028]], "sentence": ["evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88", "in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions", "is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects", "it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed", "incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed captions that are similar to human over description. all pairs in the dataset", "the flickr 8k dataset contains 8 092 images annotated with human-generated five reference incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed captions that are similar to human over description. all pairs in the dataset captions each", "all evaluated were the flickr 8k dataset contains 8 092 images annotated with human-generated five reference incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed captions that are similar to human over description. all pairs in the dataset captions each captions sourced from the dataset", "flickr 30k and coco all evaluated were the flickr 8k dataset contains 8 092 images annotated with human-generated five reference incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed captions that are similar to human over description. all pairs in the dataset captions each captions sourced from the dataset captions as the dataset composite [35]", "flickr 30k and coco all evaluated were the flickr 8k dataset contains 8 092 images annotated with human-generated five reference incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed captions that are similar to human over description. all pairs in the dataset captions each captions sourced from the dataset captions as the dataset composite [35] captions were scored using amt on a graded correctness scale from 1 the description has no", "although we expect to synonym-matching become more important when fewer reference flickr 30k and coco all evaluated were the flickr 8k dataset contains 8 092 images annotated with human-generated five reference incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed captions that are similar to human over description. all pairs in the dataset captions each captions sourced from the dataset captions as the dataset composite [35] captions were scored using amt on a graded correctness scale from 1 the description has no captions are color 4.3 available. perception", "the number reference of although we expect to synonym-matching become more important when fewer reference flickr 30k and coco all evaluated were the flickr 8k dataset contains 8 092 images annotated with human-generated five reference incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed captions that are similar to human over description. all pairs in the dataset captions each captions sourced from the dataset captions as the dataset composite [35] captions were scored using amt on a graded correctness scale from 1 the description has no captions are color 4.3 available. perception captions available to the metrics is varied from 1 to 48", "microsoft c.l.: coco the number reference of although we expect to synonym-matching become more important when fewer reference flickr 30k and coco all evaluated were the flickr 8k dataset contains 8 092 images annotated with human-generated five reference incorrect average correct). detail of the captions from 15 lacking details very percentage detailed). of it implicitly that assuming is that the metric be could gamed by generating in the domain image of evaluations across a range of models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 captions captions that represent only objects captions are well-formed captions that are similar to human over description. all pairs in the dataset captions each captions sourced from the dataset captions as the dataset composite [35] captions were scored using amt on a graded correctness scale from 1 the description has no captions are color 4.3 available. perception captions available to the metrics is varied from 1 to 48 captions data collection and evaluation server"]}, "automatic metrics": {"position": [[137, 138]], "sentence": ["models and indicate datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 with human judgments on the ms"]}, "g": {"position": [[140, 140]], "sentence": ["datasets that spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 with human judgments on the ms coco dataset"]}, "system level correlation": {"position": [[142, 144]], "sentence": ["spice captures human judgments over captions model-generated better than other automatic metrics (e g , system level correlation of 0 88 with human judgments on the ms coco dataset"]}, "cider": {"position": [[160, 160], [1267, 1267], [1301, 1301]], "sentence": ["0.43 versus for cider and 0 53 for meteor)", "is [11] a package of a measures for automatic evaluation of text summaries f-measures. using 0.43 versus for cider and 0 53 for meteor) cider applies term frequency inverse document frequency tfidf weights to grams in the candidate and reference", "with the exception of is [11] a package of a measures for automatic evaluation of text summaries f-measures. using 0.43 versus for cider and 0 53 for meteor) cider applies term frequency inverse document frequency tfidf weights to grams in the candidate and reference cider"]}, "questions": {"position": [[171, 171]], "sentence": ["spice can questions answer such as which caption generator best understands colors? can and caption generators recently introduction"]}, "flickr 8k": {"position": [[228, 229], [3875, 3876]], "sentence": ["been driven in part by the development of new and larger benchmark such datasets as flickr 8k [3]", "the been driven in part by the development of new and larger benchmark such datasets as flickr 8k [3] flickr 8k dataset contains 8 092 images annotated with human-generated five reference captions each"]}, "ms coco": {"position": [[235, 235], [5339, 5339]], "sentence": ["flickr 30k and ms coco [5]", "differences or in metric implementations we use the flickr 30k and ms coco [5] ms coco evaluation on code). pascal-50s"]}, "coco": {"position": [[236, 236], [3826, 3826], [3868, 3868], [4903, 4903], [5340, 5340]], "sentence": ["flickr 30k and ms coco [5]", "metric scores competition for entries were obtained from the flickr 30k and ms coco [5] coco organizers", "was fixed before evaluating coco. on at no stage were we given access to the metric scores competition for entries were obtained from the flickr 30k and ms coco [5] coco organizers coco test flickr captions. 8k", "in table 2 we review the performance of was fixed before evaluating coco. on at no stage were we given access to the metric scores competition for entries were obtained from the flickr 30k and ms coco [5] coco organizers coco test flickr captions. 8k coco 2015 captioning challenge submissions in terms of color perception", "differences or in metric implementations we use the ms in table 2 we review the performance of was fixed before evaluating coco. on at no stage were we given access to the metric scores competition for entries were obtained from the flickr 30k and ms coco [5] coco organizers coco test flickr captions. 8k coco 2015 captioning challenge submissions in terms of color perception coco evaluation on code). pascal-50s"]}, "datasets": {"position": [[242, 242]], "sentence": ["while datasets new often spur considerable innovationas has been the case with the coco ms captioning challenge"]}, "coco ms captioning challenge benchmark datasets": {"position": [[254, 259]], "sentence": ["while datasets new often spur considerable innovationas has been the case with the coco ms captioning challenge benchmark datasets also require fast"]}, "human": {"position": [[284, 284], [1011, 1011], [1086, 1086], [1347, 1347], [3364, 3364], [3579, 3579], [4393, 4393], [5169, 5169], [5646, 5646]], "sentence": ["existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8]", "we show that spice outperforms existing gram metrics in terms of agreement with existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8] human of evaluations model generated captions", "rouge cider and in terms of agreement with we show that spice outperforms existing gram metrics in terms of agreement with existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8] human of evaluations model generated captions human evaluations 3. and we demonstrate that spice performance can be decomposed to questions answer such", "by measuring correlation with rouge cider and in terms of agreement with we show that spice outperforms existing gram metrics in terms of agreement with existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8] human of evaluations model generated captions human evaluations 3. and we demonstrate that spice performance can be decomposed to questions answer such human of judgments caption quality", "where by measuring correlation with rouge cider and in terms of agreement with we show that spice outperforms existing gram metrics in terms of agreement with existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8] human of evaluations model generated captions human evaluations 3. and we demonstrate that spice performance can be decomposed to questions answer such human of judgments caption quality human judgments consist of graded rather scores than pairwise rankings", "the captions from 15 lacking details very percentage detailed). of captions that are similar to where by measuring correlation with rouge cider and in terms of agreement with we show that spice outperforms existing gram metrics in terms of agreement with existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8] human of evaluations model generated captions human evaluations 3. and we demonstrate that spice performance can be decomposed to questions answer such human of judgments caption quality human judgments consist of graded rather scores than pairwise rankings human over description. all pairs in the dataset", "which is consistent with the captions from 15 lacking details very percentage detailed). of captions that are similar to where by measuring correlation with rouge cider and in terms of agreement with we show that spice outperforms existing gram metrics in terms of agreement with existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8] human of evaluations model generated captions human evaluations 3. and we demonstrate that spice performance can be decomposed to questions answer such human of judgments caption quality human judgments consist of graded rather scores than pairwise rankings human over description. all pairs in the dataset human significantly judgment outperforms existing metrics", "as reported in section 4 2, spice more approximates closely which is consistent with the captions from 15 lacking details very percentage detailed). of captions that are similar to where by measuring correlation with rouge cider and in terms of agreement with we show that spice outperforms existing gram metrics in terms of agreement with existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8] human of evaluations model generated captions human evaluations 3. and we demonstrate that spice performance can be decomposed to questions answer such human of judgments caption quality human judgments consist of graded rather scores than pairwise rankings human over description. all pairs in the dataset human significantly judgment outperforms existing metrics human judgment when aggregated over more captions", "in future work we hope to use as reported in section 4 2, spice more approximates closely which is consistent with the captions from 15 lacking details very percentage detailed). of captions that are similar to where by measuring correlation with rouge cider and in terms of agreement with we show that spice outperforms existing gram metrics in terms of agreement with existing unfortunately, metrics have proven to be inadequate substitutes for human in judgment the task of evaluating image captions [7,3,8] human of evaluations model generated captions human evaluations 3. and we demonstrate that spice performance can be decomposed to questions answer such human of judgments caption quality human judgments consist of graded rather scores than pairwise rankings human over description. all pairs in the dataset human significantly judgment outperforms existing metrics human judgment when aggregated over more captions human to annotators establish an upper bound for how closely spice approximates human judgments given perfect"]}, "analyzing their": {"position": [[334, 334]], "sentence": ["a novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content"]}, "their semantic": {"position": [[335, 335]], "sentence": ["novel automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content"]}, "semantic": {"position": [[336, 336], [1955, 1955], [2657, 2657]], "sentence": ["automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content", "our choice of automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content semantic representation is the scene graph", "we view our choice of automatic image caption evaluation metric that measures quality the of generated captions by analyzing their semantic content semantic representation is the scene graph semantic the relations in the scene graph as a conjunction of logical propositions"]}, "human judgment": {"position": [[343, 344]], "sentence": ["our closely method resembles human judgment while offering the additional advantage the that performance of any model can be analyzed in"]}, "metrics": {"position": [[373, 373], [3173, 3173], [5293, 5293]], "sentence": ["analyzed in greater detail than with automated other one metrics. of the problems with using metrics such as bleu [10]", "more easily interpretable analyzed in greater detail than with automated other one metrics. of the problems with using metrics such as bleu [10] metrics", "the number reference of captions available to the more easily interpretable analyzed in greater detail than with automated other one metrics. of the problems with using metrics such as bleu [10] metrics metrics is varied from 1 to 48"]}, "dependency parse trees": {"position": [[434, 436]], "sentence": ["reference and candidate captions mapped are through dependency parse trees top to semantic scene graphs encoding (right) the objects (red)"]}, "tuples": {"position": [[465, 465], [2838, 2838], [2853, 2853], [4848, 4848], [4887, 4887]], "sentence": ["quality caption is determined using an score calculated over tuples in the candidate reference and scene marily graphs sensitive to gram overlap", "where (5) (4) (3) = for matching quality caption is determined using an score calculated over tuples in the candidate reference and scene marily graphs sensitive to gram overlap tuples", "such that where (5) (4) (3) = for matching quality caption is determined using an score calculated over tuples in the candidate reference and scene marily graphs sensitive to gram overlap tuples tuples are considered to be matched if their lemmatized word forms are equalallowing terms with different", "0.009 0 004 ever, 0.000 spice has the useful property that it is defined over such that where (5) (4) (3) = for matching quality caption is determined using an score calculated over tuples in the candidate reference and scene marily graphs sensitive to gram overlap tuples tuples are considered to be matched if their lemmatized word forms are equalallowing terms with different tuples that are easy subdivide to into meaningful categories", "or analyzed any to arbitrary level of detail by subdividing 0.009 0 004 ever, 0.000 spice has the useful property that it is defined over such that where (5) (4) (3) = for matching quality caption is determined using an score calculated over tuples in the candidate reference and scene marily graphs sensitive to gram overlap tuples tuples are considered to be matched if their lemmatized word forms are equalallowing terms with different tuples that are easy subdivide to into meaningful categories tuples even to further. demonstrate this capability"]}, "gram metrics": {"position": [[557, 558], [1334, 1335], [3116, 3117]], "sentence": ["comparing these captions using any of the previously mentioned gram metrics produces a similarity high score due to the presence of the long gram phrase standing", "and were subsequently for adopted image caption several evaluation. studies have analyzed the performance of comparing these captions using any of the previously mentioned gram metrics produces a similarity high score due to the presence of the long gram phrase standing gram metrics when for used image caption evaluation", "as with and were subsequently for adopted image caption several evaluation. studies have analyzed the performance of comparing these captions using any of the previously mentioned gram metrics produces a similarity high score due to the presence of the long gram phrase standing gram metrics when for used image caption evaluation gram metrics"]}, "low gram similarity": {"position": [[624, 626]], "sentence": ["but exhibit low gram similarity as they have no words in to common. overcome the limitations of existing gram based"]}, "gram based automatic metrics, evaluation": {"position": [[640, 644]], "sentence": ["gram similarity as they have no words in to common. overcome the limitations of existing gram based automatic metrics, evaluation in this work we hypothesize that semantic propositional content is \\\\fspice: an semantic propositional image"]}, "graph": {"position": [[785, 785]], "sentence": ["we estimate caption quality by transforming both candidate and reference captions into a graph based semantic representation called a scene graph"]}, "lexical": {"position": [[814, 814]], "sentence": ["abstracting away most the of lexical and syntactic idiosyncrasies of natural language in the process"]}, "two stage": {"position": [[871, 871]], "sentence": ["we use a two stage similar approach to previous works [16,17,18]"]}, "stage": {"position": [[872, 872]], "sentence": ["we use a two stage similar approach to previous works [16,17,18]"]}, "syntactic": {"position": [[885, 885], [2580, 2580]], "sentence": ["syntactic between dependencies words in the caption are established using a dependency parser pre-trained [19] on", "it is our that hope ongoing advances in syntactic between dependencies words in the caption are established using a dependency parser pre-trained [19] on syntactic and semantic parsing will allow spice to further be improved in future releases"]}, "a dependency": {"position": [[895, 895]], "sentence": ["syntactic between dependencies words in the caption are established using a dependency parser pre-trained [19] on a large dataset"]}, "dependency parser pre-trained": {"position": [[896, 898]], "sentence": ["syntactic between dependencies words in the caption are established using a dependency parser pre-trained [19] on a large dataset"]}, "dependency tree,": {"position": [[910, 911]], "sentence": ["an example of the resulting dependency tree, syntax using universal dependency relations [20]"]}, "universal dependency relations": {"position": [[914, 916]], "sentence": ["an example of the resulting dependency tree, syntax using universal dependency relations [20]"]}, "dependency": {"position": [[934, 934]], "sentence": ["we map from dependency trees to scene graphs using a system rule-based [16]"]}, "system rule-based": {"position": [[941, 942]], "sentence": ["we map from dependency trees to scene graphs using a system rule-based [16]"]}, "logical tuples": {"position": [[962, 963]], "sentence": ["our metric an computes score defined over the conjunction of logical tuples representing propositions semantic in the scene graph (e g , figure 1 right)"]}, "semantic parsing techniques": {"position": [[1028, 1030]], "sentence": ["while offering scope for further improvements the to extent that semantic parsing techniques continue to improve"]}, "semantic propositional": {"position": [[1064, 1065]], "sentence": ["a principled metric for automatic image caption evaluation that compares semantic propositional 2. content; we show that spice outperforms metrics bleu"]}, "stephen metrics": {"position": [[1162, 1162]], "sentence": ["stephen metrics gould are posed as similarity measures that compare a candidate sentence a to set"]}, "metrics gould": {"position": [[1163, 1163]], "sentence": ["stephen metrics gould are posed as similarity measures that compare a candidate sentence a to set of"]}, "gould": {"position": [[1164, 1164]], "sentence": ["stephen metrics gould are posed as similarity measures that compare a candidate sentence a to set of reference"]}, "similarity measures": {"position": [[1168, 1169]], "sentence": ["stephen metrics gould are posed as similarity measures that compare a candidate sentence a to set of reference or ground truth sentences"]}, "bleu": {"position": [[1201, 1201], [1359, 1359], [4461, 4461], [5546, 5546]], "sentence": ["bleu is a modified precision metric with a sentence brevity penalty", "bleu is a modified precision metric with a sentence brevity penalty bleu was found to weak exhibit or no correlation pearsons r of -0 17 and 0", "bleu is a modified precision metric with a sentence brevity penalty bleu was found to weak exhibit or no correlation pearsons r of -0 17 and 0 bleu and rouge appear to penalize detailedness", "bleu is a modified precision metric with a sentence brevity penalty bleu was found to weak exhibit or no correlation pearsons r of -0 17 and 0 bleu and rouge appear to penalize detailedness bleu performs best given two correct human captions existing 78.8 78.9 80.8 77.3 73.7 77.7 68.2"]}, "precision metric": {"position": [[1205, 1206]], "sentence": ["bleu is a modified precision metric with a sentence brevity penalty"]}, "sentence brevity": {"position": [[1209, 1209]], "sentence": ["bleu is a modified precision metric with a sentence brevity penalty"]}, "brevity penalty": {"position": [[1210, 1211]], "sentence": ["bleu is a modified precision metric with a sentence brevity penalty"]}, "a weighted": {"position": [[1215, 1215]], "sentence": ["calculated as a weighted mean geometric over different length n-grams"]}, "weighted mean geometric": {"position": [[1216, 1218]], "sentence": ["calculated as a weighted mean geometric over different length n-grams"]}, "meteor": {"position": [[1224, 1224], [1385, 1385], [4475, 4475], [4498, 4498], [5355, 5355], [5527, 5527]], "sentence": ["meteor uses exact", "meteor uses exact meteor exhibited moderate correlation spearmans of outperforming 0.524) rouge su (0 435), bleu smoothed (0 429)", "while the results for cider and meteor uses exact meteor exhibited moderate correlation spearmans of outperforming 0.524) rouge su (0 435), bleu smoothed (0 429) meteor are not statistically as significant. illustrated in figure 3", "spice is the only metric to correctly rank humangenerated captions firstcider and while the results for cider and meteor uses exact meteor exhibited moderate correlation spearmans of outperforming 0.524) rouge su (0 435), bleu smoothed (0 429) meteor are not statistically as significant. illustrated in figure 3 meteor rank human captions 7th 4th, and respectively", "there is little difference in overall performance spice, between spice is the only metric to correctly rank humangenerated captions firstcider and while the results for cider and meteor uses exact meteor exhibited moderate correlation spearmans of outperforming 0.524) rouge su (0 435), bleu smoothed (0 429) meteor are not statistically as significant. illustrated in figure 3 meteor rank human captions 7th 4th, and respectively meteor and cider", "there is little difference in overall performance spice, between spice is the only metric to correctly rank humangenerated captions firstcider and while the results for cider and meteor uses exact meteor exhibited moderate correlation spearmans of outperforming 0.524) rouge su (0 435), bleu smoothed (0 429) meteor are not statistically as significant. illustrated in figure 3 meteor rank human captions 7th 4th, and respectively meteor and cider meteor is best differentiating at human and model captions hm and human captions where one incorrect"]}, "exact": {"position": [[1226, 1226], [4537, 4537]], "sentence": ["meteor uses exact", "help understand the importance of we synonym-matching, also evaluated spice using exact matching only spice meteor uses exact exact in table performance 1). degraded only marginally"]}, "synonym paraphrase": {"position": [[1231, 1231]], "sentence": ["and synonym paraphrase matches between grams to align sentences"]}, "paraphrase": {"position": [[1232, 1232]], "sentence": ["and synonym paraphrase matches between grams to align sentences"]}, "grams": {"position": [[1235, 1235]], "sentence": ["and synonym paraphrase matches between grams to align sentences"]}, "score": {"position": [[1244, 1244]], "sentence": ["before computing a weighted score with an alignment fragmentation penalty"]}, "alignment fragmentation penalty": {"position": [[1247, 1249]], "sentence": ["before computing a weighted score with an alignment fragmentation penalty"]}, "rouge": {"position": [[1251, 1251], [1427, 1427]], "sentence": ["rouge is [11] a package of a measures for automatic evaluation of text summaries f-measures. using", "cider and meteor were found to have greater agreement with human consensus than and bleu rouge is [11] a package of a measures for automatic evaluation of text summaries f-measures. using rouge within [12]. the context of automatic mt evaluation"]}, "f-measures.": {"position": [[1265, 1265]], "sentence": ["rouge is [11] a package of a measures for automatic evaluation of text summaries f-measures. using cider applies term frequency inverse document frequency tfidf weights to grams in the candidate"]}, "term": {"position": [[1269, 1269]], "sentence": ["a package of a measures for automatic evaluation of text summaries f-measures. using cider applies term frequency inverse document frequency tfidf weights to grams in the candidate and reference sentences"]}, "document frequency tfidf": {"position": [[1272, 1274]], "sentence": ["a measures for automatic evaluation of text summaries f-measures. using cider applies term frequency inverse document frequency tfidf weights to grams in the candidate and reference sentences"]}, "summing": {"position": [[1290, 1290]], "sentence": ["which are compared then by summing their cosine similarity across n-grams"]}, "summaries": {"position": [[1313, 1313], [6199, 6199]], "sentence": ["these methods were originally developed for the evaluation text of summaries or machine translations (mt)", "c y : rouge a package for automatic evaluation of these methods were originally developed for the evaluation text of summaries or machine translations (mt) summaries"]}, "caption": {"position": [[1350, 1350], [3064, 3064], [3218, 3218], [3228, 3228], [3344, 3344], [4955, 4955], [5023, 5023], [5068, 5068], [5328, 5328]], "sentence": ["by measuring correlation with human of judgments caption quality", "sight the human judgement that the metric was supposed to spice represent. measures how well by measuring correlation with human of judgments caption quality caption generators recover objects", "we compare spice to existing sight the human judgement that the metric was supposed to spice represent. measures how well by measuring correlation with human of judgments caption quality caption generators recover objects caption evaluation metrics", "study we both system level and we compare spice to existing sight the human judgement that the metric was supposed to spice represent. measures how well by measuring correlation with human of judgments caption quality caption generators recover objects caption evaluation metrics caption level correlation with human data judgments. for the evaluation is drawn from four datasets collected", "for study we both system level and we compare spice to existing sight the human judgement that the metric was supposed to spice represent. measures how well by measuring correlation with human of judgments caption quality caption generators recover objects caption evaluation metrics caption level correlation with human data judgments. for the evaluation is drawn from four datasets collected caption level we correlation, evaluate using kendalls rank correlation coefficient", "into for study we both system level and we compare spice to existing sight the human judgement that the metric was supposed to spice represent. measures how well by measuring correlation with human of judgments caption quality caption generators recover objects caption evaluation metrics caption level correlation with human data judgments. for the evaluation is drawn from four datasets collected caption level we correlation, evaluate using kendalls rank correlation coefficient caption whether generators actually understand color", "these models have learned to count caption-level 4.4 objects. in correlation table 3 we report into for study we both system level and we compare spice to existing sight the human judgement that the metric was supposed to spice represent. measures how well by measuring correlation with human of judgments caption quality caption generators recover objects caption evaluation metrics caption level correlation with human data judgments. for the evaluation is drawn from four datasets collected caption level we correlation, evaluate using kendalls rank correlation coefficient caption whether generators actually understand color caption level correlations between automated metrics human and judgments on flickr 8k and the composite dataset", "these models have learned to count caption-level 4.4 objects. in correlation table 3 we report into for study we both system level and we compare spice to existing sight the human judgement that the metric was supposed to spice represent. measures how well by measuring correlation with human of judgments caption quality caption generators recover objects caption evaluation metrics caption level correlation with human data judgments. for the evaluation is drawn from four datasets collected caption level we correlation, evaluate using kendalls rank correlation coefficient caption whether generators actually understand color caption level correlations between automated metrics human and judgments on flickr 8k and the composite dataset caption level kendalls correlation between evaluation metrics and human graded quality scores", "our results differ which slightly may be due to randomness in the choice of reference these models have learned to count caption-level 4.4 objects. in correlation table 3 we report into for study we both system level and we compare spice to existing sight the human judgement that the metric was supposed to spice represent. measures how well by measuring correlation with human of judgments caption quality caption generators recover objects caption evaluation metrics caption level correlation with human data judgments. for the evaluation is drawn from four datasets collected caption level we correlation, evaluate using kendalls rank correlation coefficient caption whether generators actually understand color caption level correlations between automated metrics human and judgments on flickr 8k and the composite dataset caption level kendalls correlation between evaluation metrics and human graded quality scores caption subsets"]}, "flickr [3]": {"position": [[1380, 1381]], "sentence": ["using the flickr [3] 8k dataset"]}, "human consensus": {"position": [[1422, 1423]], "sentence": ["cider and meteor were found to have greater agreement with human consensus than and bleu rouge within [12]. the context of automatic mt evaluation"]}, "automatic mt": {"position": [[1433, 1433]], "sentence": ["have greater agreement with human consensus than and bleu rouge within [12]. the context of automatic mt evaluation"]}, "shallow semantic information": {"position": [[1446, 1448]], "sentence": ["a number of papers proposed have the use of shallow semantic information such as semantic role (srls) labels [14]"]}, "semantic role (srls) labels": {"position": [[1451, 1454]], "sentence": ["a number of papers proposed have the use of shallow semantic information such as semantic role (srls) labels [14]"]}, "meant metric": {"position": [[1459, 1460]], "sentence": ["in the meant metric [21]"]}, "srls": {"position": [[1463, 1463]], "sentence": ["srls are used to try to capture basic the event structure of sentences who did what"]}, "first matching": {"position": [[1498, 1498]], "sentence": ["sentence similarity is calculated by first matching semantic frames across sentences by starting with the verbs at their however, head. this"]}, "matching semantic": {"position": [[1499, 1499]], "sentence": ["sentence similarity is calculated by first matching semantic frames across sentences by starting with the verbs at their however, head. this approach"]}, "semantic frames across sentences": {"position": [[1500, 1503]], "sentence": ["sentence similarity is calculated by first matching semantic frames across sentences by starting with the verbs at their however, head. this approach does not easily transfer"]}, "starting": {"position": [[1505, 1505]], "sentence": ["sentence similarity is calculated by first matching semantic frames across sentences by starting with the verbs at their however, head. this approach does not easily transfer to image"]}, "verbs": {"position": [[1524, 1524]], "sentence": ["verbs as are frequently absent from image captions or not meaningful e g"]}, "graphs": {"position": [[1569, 1569]], "sentence": ["the process. matching our work differs from these approaches as we represent sentences scene using graphs"]}, "semantic graphs": {"position": [[1679, 1680]], "sentence": ["several of these papers have demonstrated that semantic graphs can be parsed from natural language \\\\fspice: descriptions semantic propositional image caption [18,16]. 5 evaluation"]}, "parsed": {"position": [[1683, 1683], [2385, 2385]], "sentence": ["several of these papers have demonstrated that semantic graphs can be parsed from natural language \\\\fspice: descriptions semantic propositional image caption [18,16]. 5 evaluation the task of", "resolve pronouns and handle plural the nouns. resulting tree structure is then several of these papers have demonstrated that semantic graphs can be parsed from natural language \\\\fspice: descriptions semantic propositional image caption [18,16]. 5 evaluation the task of parsed according to nine simple rules linguistic to extract lemmatized objects"]}, "smatch": {"position": [[1744, 1744], [1764, 1764], [2883, 2883], [6731, 6731]], "sentence": ["for which a number of [25,26,17] parsers and the smatch evaluation metric have been developed", "we found that amr representations using similarity for which a number of [25,26,17] parsers and the smatch evaluation metric have been developed smatch performed poorly as image caption representations", "different inflectional forms matchor to if they are found in the same wordnet unlike sysnet. we found that amr representations using similarity for which a number of [25,26,17] parsers and the smatch evaluation metric have been developed smatch performed poorly as image caption representations smatch [27]", "k : different inflectional forms matchor to if they are found in the same wordnet unlike sysnet. we found that amr representations using similarity for which a number of [25,26,17] parsers and the smatch evaluation metric have been developed smatch performed poorly as image caption representations smatch [27] smatch an evaluation metric for semantic feature in: structures. acl (2)"]}, "similarity smatch": {"position": [[1763, 1763]], "sentence": ["we found that amr representations using similarity smatch performed poorly as image caption representations"]}, "dependency trees": {"position": [[1781, 1782]], "sentence": ["the use of dependency trees as the starting point for semantic parsing graphs appears to be a common theme spice"]}, "c": {"position": [[1842, 1842], [1988, 1988], [2087, 2087], [2108, 2108], [6707, 6707]], "sentence": ["our goal is to compute a score that captures similarity the between c and s", "28] and the recently released visual genome dataset the [29]. scene graph of candidate caption our goal is to compute a score that captures similarity the between c and s c is denoted by g(c)", "we parse 28] and the recently released visual genome dataset the [29]. scene graph of candidate caption our goal is to compute a score that captures similarity the between c and s c is denoted by g(c) c to a scene g(c) graph: ho(c)", "where (1) k(c)i c is the set of object mentions in we parse 28] and the recently released visual genome dataset the [29]. scene graph of candidate caption our goal is to compute a score that captures similarity the between c and s c is denoted by g(c) c to a scene g(c) graph: ho(c) c", "where (1) k(c)i c is the set of object mentions in we parse 28] and the recently released visual genome dataset the [29]. scene graph of candidate caption our goal is to compute a score that captures similarity the between c and s c is denoted by g(c) c to a scene g(c) graph: ho(c) c c : robust subgraph generation improves abstract meaning representation parsing"]}, "image": {"position": [[1854, 1854], [6848, 6848]], "sentence": ["for the purposes of caption evaluation the is image disregarded", "l : visual genome connecting language and vision using crowdsourced dense for the purposes of caption evaluation the is image disregarded image annotations"]}, "structure semantic": {"position": [[1878, 1879]], "sentence": ["because we exploit the structure semantic of scene descriptions and give primacy to nouns"]}, "primacy": {"position": [[1885, 1885]], "sentence": ["because we exploit the structure semantic of scene descriptions and give primacy to nouns"]}, "figureground relationshipsthat": {"position": [[1932, 1933]], "sentence": ["while are we aware that there are other components of linguistic meaningsuch as figureground relationshipsthat are almost certainly relevant to caption quality"]}, "g(s)": {"position": [[2006, 2006]], "sentence": ["and the scene for graph the reference captions s is denoted by g(s)"]}, "semantic parsing": {"position": [[2030, 2031]], "sentence": ["next present we the semantic parsing step to generate scene graphs from semantic 3.1 captions. parsingcaptions to scene we graphs define"]}, "semantic 3.1 captions.": {"position": [[2038, 2039]], "sentence": ["next present we the semantic parsing step to generate scene graphs from semantic 3.1 captions. parsingcaptions to scene we graphs define the subtask of parsing captions to scene graphs"]}, "r": {"position": [[2110, 2110], [6413, 6413], [6898, 6898]], "sentence": ["r is set the of hyper edges representing relations between objects", "r is set the of hyper edges representing relations between objects r : visual semantic search videos retrieving via complex textual queries", "r is set the of hyper edges representing relations between objects r : visual semantic search videos retrieving via complex textual queries r : expectation based syntactic comprehension"]}, "hyper edges": {"position": [[2115, 2116]], "sentence": ["r is set the of hyper edges representing relations between objects"]}, "duplication": {"position": [[2270, 2270]], "sentence": ["duplication of instances object was necessary to enable scene graphs to be grounded to image in"]}, "rule": {"position": [[2342, 2342]], "sentence": ["we adopt a variant of the rule based version the of stanford scene graph parser [16]"]}, "free": {"position": [[2356, 2356]], "sentence": ["a probabilistic context free (pcfg) grammar dependency parser is followed by three post processing steps simplify that quantificational modifiers"]}, "parser": {"position": [[2360, 2360], [2460, 2460]], "sentence": ["a probabilistic context free (pcfg) grammar dependency parser is followed by three post processing steps simplify that quantificational modifiers", "of pipeline the can be found in the original spice paper. slightly modifies the original a probabilistic context free (pcfg) grammar dependency parser is followed by three post processing steps simplify that quantificational modifiers parser to better evaluate image captions"]}, "post processing": {"position": [[2365, 2366]], "sentence": ["a probabilistic context free (pcfg) grammar dependency parser is followed by three post processing steps simplify that quantificational modifiers"]}, "simplify": {"position": [[2368, 2368]], "sentence": ["a probabilistic context free (pcfg) grammar dependency parser is followed by three post processing steps simplify that quantificational modifiers"]}, "quantificational modifiers": {"position": [[2370, 2370]], "sentence": ["context free (pcfg) grammar dependency parser is followed by three post processing steps simplify that quantificational modifiers"]}, "modifiers": {"position": [[2371, 2371], [2420, 2420]], "sentence": ["free (pcfg) grammar dependency parser is followed by three post processing steps simplify that quantificational modifiers", "one of the linguistic rules adjectival amod captures free (pcfg) grammar dependency parser is followed by three post processing steps simplify that quantificational modifiers modifiers"]}, "resolve": {"position": [[2373, 2373]], "sentence": ["resolve pronouns and handle plural the nouns. resulting tree structure is then parsed according to nine"]}, "linguistic": {"position": [[2391, 2391], [2415, 2415], [6269, 6269], [6494, 6494]], "sentence": ["handle plural the nouns. resulting tree structure is then parsed according to nine simple rules linguistic to extract lemmatized objects", "one of the handle plural the nouns. resulting tree structure is then parsed according to nine simple rules linguistic to extract lemmatized objects linguistic rules adjectival amod captures modifiers", "j , arquez, m` l : one of the handle plural the nouns. resulting tree structure is then parsed according to nine simple rules linguistic to extract lemmatized objects linguistic rules adjectival amod captures modifiers linguistic features for automatic evaluation of heterogenous mt systems", "manning, j., c d : universal stanford dependencies a cross j , arquez, m` l : one of the handle plural the nouns. resulting tree structure is then parsed according to nine simple rules linguistic to extract lemmatized objects linguistic rules adjectival amod captures modifiers linguistic features for automatic evaluation of heterogenous mt systems linguistic typology"]}, "adjectival amod": {"position": [[2417, 2417]], "sentence": ["one of the linguistic rules adjectival amod captures modifiers"]}, "amod": {"position": [[2418, 2418]], "sentence": ["one of the linguistic rules adjectival amod captures modifiers"]}, "attribute young": {"position": [[2440, 2440]], "sentence": ["results which in the object mention girl with attribute young"]}, "young": {"position": [[2441, 2441]], "sentence": ["results which in the object mention girl with attribute young"]}, "pipeline": {"position": [[2446, 2446]], "sentence": ["full details of pipeline the can be found in the original spice paper. slightly modifies the original parser to"]}, "numeric modifier": {"position": [[2488, 2489]], "sentence": ["nouns transformation that duplicates nodes individual of the graph according to the value of their numeric modifier"]}, "object": {"position": [[2504, 2504], [4620, 4620]], "sentence": ["\\\\fspice: instead, semantic propositional image caption numeric 7 evaluation modifiers are encoded as object attributes", "spice is comprised of \\\\fspice: instead, semantic propositional image caption numeric 7 evaluation modifiers are encoded as object attributes object"]}, "linguistic rule": {"position": [[2513, 2514]], "sentence": ["we add an additional linguistic rule that ensures that nouns will always appear as objects the in scene grapheven if no"]}, "nouns": {"position": [[2518, 2518]], "sentence": ["we add an additional linguistic rule that ensures that nouns will always appear as objects the in scene grapheven if no associated relations can identifiedas"]}, "our semantic": {"position": [[2541, 2541]], "sentence": ["scene grapheven if no associated relations can identifiedas graph disconnected nodes are easily handled by our semantic proposition score notwithstanding calculation. the use of the stanford scene graph parser"]}, "semantic proposition score": {"position": [[2542, 2544]], "sentence": ["grapheven if no associated relations can identifiedas graph disconnected nodes are easily handled by our semantic proposition score notwithstanding calculation. the use of the stanford scene graph parser"]}, "logical": {"position": [[2668, 2668]], "sentence": ["we view semantic the relations in the scene graph as a conjunction of logical propositions"]}, "tuples.": {"position": [[2671, 2671]], "sentence": ["tuples. or we define the function t that returns logical tuples from a scene t as:"]}, "{ tuples:": {"position": [[2727, 2727]], "sentence": ["the scene graph in figure would 1 be represented with the following { tuples: (girl)"]}, "operator": {"position": [[2768, 2768]], "sentence": ["define we the binary matching operator as the function that returns tuples matching in two scene graphs"]}, "precision p": {"position": [[2784, 2784]], "sentence": ["we then define precision p"]}, "p": {"position": [[2785, 2785]], "sentence": ["we then define precision p"]}, "wordnet synonym matching": {"position": [[2843, 2845]], "sentence": ["we reuse the wordnet synonym matching of approach meteor [13]"]}, "lemmatized word forms": {"position": [[2861, 2863]], "sentence": ["such that tuples are considered to be matched if their lemmatized word forms are equalallowing terms with different inflectional forms matchor to if they are found in the"]}, "sysnet. smatch": {"position": [[2882, 2882]], "sentence": ["with different inflectional forms matchor to if they are found in the same wordnet unlike sysnet. smatch [27]"]}, "amr": {"position": [[2892, 2892]], "sentence": ["a recently proposed metric for evaluating amr that parsers considers multiple alignments of amr graphs"]}, "parsers": {"position": [[2894, 2894]], "sentence": ["a recently proposed metric for evaluating amr that parsers considers multiple alignments of amr graphs"]}, "multiple alignments": {"position": [[2896, 2897]], "sentence": ["a recently proposed metric for evaluating amr that parsers considers multiple alignments of amr graphs"]}, "domain image": {"position": [[2933, 2934]], "sentence": ["in the domain image of captions"]}, "and": {"position": [[2943, 2943]], "sentence": ["many relations such as in and on are so common they deserve arguably no credit when applied to the wrong being"]}, "interpretable": {"position": [[2972, 2972]], "sentence": ["and easily as interpretable it is naturally bounded between 0 and 1"]}, "frequenciesand": {"position": [[2995, 2995]], "sentence": ["spice does use not cross dataset statisticssuch as corpus word frequenciesand is equally therefore applicable to both small and large whenever gameability 3.3 datasets. the focus"]}, "unintended side": {"position": [[3025, 3026]], "sentence": ["are there risks of unintended side effects [30]"]}, "metric": {"position": [[3040, 3040], [3055, 3055], [3084, 3084], [3817, 3817], [4490, 4490], [4512, 4512], [5334, 5334]], "sentence": ["algorithms optimized performance for against a certain metric may produce high scores", "while losing of sight the human judgement that the algorithms optimized performance for against a certain metric may produce high scores metric was supposed to spice represent. measures how well caption generators recover objects", "is that the while losing of sight the human judgement that the algorithms optimized performance for against a certain metric may produce high scores metric was supposed to spice represent. measures how well caption generators recover objects metric be could gamed by generating captions that represent only objects", "is that the while losing of sight the human judgement that the algorithms optimized performance for against a certain metric may produce high scores metric was supposed to spice represent. measures how well caption generators recover objects metric be could gamed by generating captions that represent only objects metric scores competition for entries were obtained from the coco organizers", "spice is the only is that the while losing of sight the human judgement that the algorithms optimized performance for against a certain metric may produce high scores metric was supposed to spice represent. measures how well caption generators recover objects metric be could gamed by generating captions that represent only objects metric scores competition for entries were obtained from the coco organizers metric to correctly rank humangenerated captions firstcider and meteor rank human captions 7th 4th, and respectively", "spice is also the only spice is the only is that the while losing of sight the human judgement that the algorithms optimized performance for against a certain metric may produce high scores metric was supposed to spice represent. measures how well caption generators recover objects metric be could gamed by generating captions that represent only objects metric scores competition for entries were obtained from the coco organizers metric to correctly rank humangenerated captions firstcider and meteor rank human captions 7th 4th, and respectively metric to correctly select the non-human top-5 entries", "differences or in spice is also the only spice is the only is that the while losing of sight the human judgement that the algorithms optimized performance for against a certain metric may produce high scores metric was supposed to spice represent. measures how well caption generators recover objects metric be could gamed by generating captions that represent only objects metric scores competition for entries were obtained from the coco organizers metric to correctly rank humangenerated captions firstcider and meteor rank human captions 7th 4th, and respectively metric to correctly select the non-human top-5 entries metric implementations we use the ms coco evaluation on code). pascal-50s"]}, "attributes": {"position": [[3096, 3096]], "sentence": ["attributes relations, and while ignoring other important aspects of grammar and syntax"]}, "grammar": {"position": [[3105, 3105]], "sentence": ["attributes relations, and while ignoring other important aspects of grammar and syntax"]}, "fluency": {"position": [[3112, 3112], [3162, 3162]], "sentence": ["because spice neglects fluency", "by default we have not included any because spice neglects fluency fluency as adjustments conceptually we favor simpler"]}, "fluency metric": {"position": [[3138, 3139]], "sentence": ["a fluency metric"]}, "surprisal [31": {"position": [[3143, 3143]], "sentence": ["such as surprisal [31 32]"]}, "[31": {"position": [[3144, 3144]], "sentence": ["such as surprisal [31 32]"]}, "system": {"position": [[3225, 3225], [3392, 3392], [4300, 4300]], "sentence": ["study we both system level and caption level correlation with human data judgments. for the evaluation is drawn from", "study we both system level and caption level correlation with human data judgments. for the evaluation is drawn from system level pearsons correlation between evaluation metrics and judgments human for the 15 competition entries plus", "and model model system-level 4.2 (mm). in correlation table 1 we report study we both system level and caption level correlation with human data judgments. for the evaluation is drawn from system level pearsons correlation between evaluation metrics and judgments human for the 15 competition entries plus system level correlations between metrics and human judgments over entries in the 2015 coco captioning challenge"]}, "judgments.": {"position": [[3234, 3234]], "sentence": ["study we both system level and caption level correlation with human data judgments. for the evaluation is drawn from four datasets collected in previous representing studies, a variety"]}, "captioning": {"position": [[3251, 3251]], "sentence": ["the evaluation is drawn from four datasets collected in previous representing studies, a variety of captioning models"]}, "pairwise": {"position": [[3265, 3265], [3360, 3360], [3372, 3372], [3377, 3377], [3775, 3775]], "sentence": ["judgments human may consist of either pairwise rankings or graded scores", "which evaluates similarity the of judgments human may consist of either pairwise rankings or graded scores pairwise rankings", "where human judgments consist of graded rather scores than which evaluates similarity the of judgments human may consist of either pairwise rankings or graded scores pairwise rankings pairwise rankings", "we generate where human judgments consist of graded rather scores than which evaluates similarity the of judgments human may consist of either pairwise rankings or graded scores pairwise rankings pairwise rankings pairwise rankings by comparing \\\\fspice: scores semantic propositional image caption table 9 evaluation 1", "we generate where human judgments consist of graded rather scores than which evaluates similarity the of judgments human may consist of either pairwise rankings or graded scores pairwise rankings pairwise rankings pairwise rankings by comparing \\\\fspice: scores semantic propositional image caption table 9 evaluation 1 pairwise for rankings (m1"]}, "correlation coefficients": {"position": [[3278, 3279], [3323, 3324]], "sentence": ["as further described our below. choice of correlation coefficients is consistent with an emerging consensus from the wmt metrics shared task [33 34] for", "it is smoother rank-based than as further described our below. choice of correlation coefficients is consistent with an emerging consensus from the wmt metrics shared task [33 34] for correlation coefficients when the number of data points is small systems and have scores that are very"]}, "pearson correlation coefficient": {"position": [[3308, 3310]], "sentence": ["we use the pearson correlation coefficient"]}, "pearsons": {"position": [[3313, 3313]], "sentence": ["although pearsons measures linear association"]}, "linear association": {"position": [[3315, 3316]], "sentence": ["although pearsons measures linear association"]}, "rank-based": {"position": [[3321, 3321]], "sentence": ["it is smoother rank-based than correlation coefficients when the number of data points is small systems and have scores"]}, "correlation": {"position": [[3352, 3352], [5018, 5018]], "sentence": ["for caption level we correlation, evaluate using kendalls rank correlation coefficient", "less that evidence any of these models have learned to count caption-level 4.4 objects. in for caption level we correlation, evaluate using kendalls rank correlation coefficient correlation table 3 we report caption level correlations between automated metrics human and judgments on flickr"]}, "graded rather": {"position": [[3368, 3369]], "sentence": ["where human judgments consist of graded rather scores than pairwise rankings"]}, "comparing \\\\fspice:": {"position": [[3380, 3380]], "sentence": ["we generate pairwise rankings by comparing \\\\fspice: scores semantic propositional image caption table 9 evaluation 1"]}, "\\\\fspice: scores semantic": {"position": [[3381, 3382]], "sentence": ["we generate pairwise rankings by comparing \\\\fspice: scores semantic propositional image caption table 9 evaluation 1"]}, "pearsons correlation": {"position": [[3394, 3395]], "sentence": ["system level pearsons correlation between evaluation metrics and judgments human for the 15 competition entries plus human captions in"]}, "human judgment (m1m2),": {"position": [[3422, 3424]], "sentence": ["spice more accurately reflects human judgment (m1m2), overall and across each dimension of quality (m3m5"]}, "caption.": {"position": [[3542, 3542]], "sentence": ["(0.862) (0.369) p-value m1 saliency) of captions evaluated as better or equal to human percentage caption. of captions that pass the turing average test. correctness of the captions on a scale"]}, "the turing": {"position": [[3547, 3547]], "sentence": ["of captions evaluated as better or equal to human percentage caption. of captions that pass the turing average test. correctness of the captions on a scale 15 incorrect average correct). detail"]}, "turing average test. correctness": {"position": [[3548, 3550]], "sentence": ["captions evaluated as better or equal to human percentage caption. of captions that pass the turing average test. correctness of the captions on a scale 15 incorrect average correct). detail of the captions"]}, "multiple independent judgments": {"position": [[3591, 3593]], "sentence": ["in datasets containing multiple independent judgments over the same caption pairs"]}, "inter human correlation": {"position": [[3603, 3605]], "sentence": ["we also report inter human correlation"]}, "same": {"position": [[3791, 3791]], "sentence": ["each entry was evaluated using the subset same of 1000 images from the c40 test set"]}, "spice methodology": {"position": [[3851, 3852]], "sentence": ["the spice methodology was fixed before evaluating coco. on at no stage were we given access to the"]}, "coco.": {"position": [[3857, 3857]], "sentence": ["the spice methodology was fixed before evaluating coco. on at no stage were we given access to the coco test flickr captions. 8k"]}, "flickr captions.": {"position": [[3870, 3870]], "sentence": ["before evaluating coco. on at no stage were we given access to the coco test flickr captions. 8k"]}, "human-generated five": {"position": [[3884, 3884]], "sentence": ["the flickr 8k dataset contains 8 092 images annotated with human-generated five reference captions each"]}, "five reference": {"position": [[3885, 3885]], "sentence": ["the flickr 8k dataset contains 8 092 images annotated with human-generated five reference captions each"]}, "reference captions": {"position": [[3886, 3886]], "sentence": ["the flickr 8k dataset contains 8 092 images annotated with human-generated five reference captions each"]}, "manually to": {"position": [[3893, 3893]], "sentence": ["the images were manually to selected focus mainly on people and animals performing actions"]}, "to": {"position": [[3894, 3894]], "sentence": ["the images were manually to selected focus mainly on people and animals performing actions"]}, "image an": {"position": [[3976, 3976]], "sentence": ["but association to images was performed using image an retrieval system"]}, "an": {"position": [[3977, 3977]], "sentence": ["but association to images was performed using image an retrieval system"]}, "flickr": {"position": [[4027, 4027], [5033, 5033], [5055, 5055], [5098, 5098]], "sentence": ["we refer to an additional dataset of 11 985 human judgments over flickr 8k", "correlation table 3 we report caption level correlations between automated metrics human and judgments on we refer to an additional dataset of 11 985 human judgments over flickr 8k flickr 8k and the composite dataset [35]", "at the level, caption spice achieves a rank correlation coefficient of 0 45 with correlation table 3 we report caption level correlations between automated metrics human and judgments on we refer to an additional dataset of 11 985 human judgments over flickr 8k flickr 8k and the composite dataset [35] flickr 8k \\\\fspice: human semantic propositional image caption table 13 evaluation 3", "all values not shown are less than at the level, caption spice achieves a rank correlation coefficient of 0 45 with correlation table 3 we report caption level correlations between automated metrics human and judgments on we refer to an additional dataset of 11 985 human judgments over flickr 8k flickr 8k and the composite dataset [35] flickr 8k \\\\fspice: human semantic propositional image caption table 13 evaluation 3 flickr 0.001 8k composite scores, - 0.73 inter-human 0.39 0.36 0.35 0.28 0.18 0.26 0.45 0.44"]}, "coco captions": {"position": [[4033, 4033], [6027, 6027]], "sentence": ["flickr 30k and coco captions as the dataset composite [35]", "microsoft c.l.: flickr 30k and coco captions as the dataset composite [35] coco captions data collection and evaluation server"]}, "dataset composite": {"position": [[4037, 4037]], "sentence": ["flickr 30k and coco captions as the dataset composite [35]"]}, "composite [35]": {"position": [[4038, 4038]], "sentence": ["flickr 30k and coco captions as the dataset composite [35]"]}, "[35]": {"position": [[4039, 4039]], "sentence": ["flickr 30k and coco captions as the dataset composite [35]"]}, "amt": {"position": [[4049, 4049], [4119, 4119], [4155, 4155]], "sentence": ["captions were scored using amt on a graded correctness scale from 1 the description has no relevance to the image", "pascal sentence dataset originally containing five captions imagewere per annotated with 50 captions each using captions were scored using amt on a graded correctness scale from 1 the description has no relevance to the image amt", "pascal sentence dataset originally containing five captions imagewere per annotated with 50 captions each using captions were scored using amt on a graded correctness scale from 1 the description has no relevance to the image amt amt workers were not asked to evaluate captions against images"]}, "graded correctness scale": {"position": [[4052, 4054]], "sentence": ["captions were scored using amt on a graded correctness scale from 1 the description has no relevance to the image to 5 description (the relates"]}, "uiuc the pascal sentence dataset": {"position": [[4102, 4106]], "sentence": ["1 000 images from uiuc the pascal sentence dataset originally containing five captions imagewere per annotated with 50 captions each using amt"]}, "triples": {"position": [[4175, 4175]], "sentence": ["they were asked to evaluate caption by triples identifying which of the sentences"]}, "b": {"position": [[4182, 4182]], "sentence": ["b or c"]}, "sentence": {"position": [[4190, 4190]], "sentence": ["is more similar to sentence where a?, sentence a is a reference caption"]}, "model": {"position": [[4270, 4270]], "sentence": ["candidate sentence pairs were generated from both human model and captions"]}, "correlation table": {"position": [[4295, 4296]], "sentence": ["and model model system-level 4.2 (mm). in correlation table 1 we report system level correlations between metrics and human judgments over entries in the"]}, "captions human-generated": {"position": [[4371, 4372]], "sentence": ["each data point represents a single model with captions human-generated marked in red"]}, "correlation coefficient 0.88": {"position": [[4402, 4404]], "sentence": ["reaching a correlation coefficient 0.88 of with human quality judgments (m1)"]}, "m2)": {"position": [[4436, 4436]], "sentence": ["spice more accurately reflects judgment human overall m2)"]}, "m5,": {"position": [[4445, 4445]], "sentence": ["and across each dimension of quality representing m5, correctness"]}, "we": {"position": [[4527, 4527]], "sentence": ["to help understand the importance of we synonym-matching, also evaluated spice using exact matching only spice exact in table performance 1). degraded"]}, "exact matching": {"position": [[4533, 4534]], "sentence": ["to help understand the importance of we synonym-matching, also evaluated spice using exact matching only spice exact in table performance 1). degraded only marginally"]}, "spice exact": {"position": [[4536, 4536]], "sentence": ["to help understand the importance of we synonym-matching, also evaluated spice using exact matching only spice exact in table performance 1). degraded only marginally"]}, "synonym-matching": {"position": [[4550, 4550]], "sentence": ["although we expect to synonym-matching become more important when fewer reference captions are color 4.3 available. perception"]}, "color": {"position": [[4559, 4559]], "sentence": ["although we expect to synonym-matching become more important when fewer reference captions are color 4.3 available. perception"]}, "counting": {"position": [[4564, 4564]], "sentence": ["counting and other existing questions gram evaluation metrics have little to offer in terms of the"]}, "error modes": {"position": [[4587, 4588]], "sentence": ["or error modes"]}, "semantic proposition subcategory": {"position": [[4612, 4614]], "sentence": ["scores by semantic proposition subcategory"]}, "relation": {"position": [[4622, 4622]], "sentence": ["relation and attribute tuples"]}, "subdivide": {"position": [[4852, 4852]], "sentence": ["0.000 spice has the useful property that it is defined over tuples that are easy subdivide to into meaningful categories"]}, "precision": {"position": [[4861, 4861]], "sentence": ["precision"]}, "f-scores": {"position": [[4866, 4866]], "sentence": ["recall and can f-scores be quantified separately for objects"]}, "quantified": {"position": [[4868, 4868]], "sentence": ["recall and can f-scores be quantified separately for objects"]}, "subdividing tuples": {"position": [[4886, 4886]], "sentence": ["or analyzed any to arbitrary level of detail by subdividing tuples even to further. demonstrate this capability"]}, "word lists": {"position": [[4923, 4924]], "sentence": ["ability, counting and understanding of size attributes by using word lists to isolate attribute tuples that contain colors"]}, "adjectives,": {"position": [[4941, 4941]], "sentence": ["and adjectives, size-related respectively"]}, "visual trained detectors": {"position": [[4980, 4982]], "sentence": ["the msr entry incorporating specifically visual trained detectors for nouns"]}, "count caption-level": {"position": [[5013, 5013]], "sentence": ["there is less that evidence any of these models have learned to count caption-level 4.4 objects. in correlation table 3 we report caption level correlations between automated metrics"]}, "caption spice": {"position": [[5044, 5044]], "sentence": ["at the level, caption spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic"]}, "rank correlation coefficient": {"position": [[5048, 5050]], "sentence": ["at the level, caption spice achieves a rank correlation coefficient of 0 45 with flickr 8k \\\\fspice: human semantic propositional image caption table 13 evaluation"]}, "kendalls": {"position": [[5070, 5070]], "sentence": ["caption level kendalls correlation between evaluation metrics and human graded quality scores"]}, "graded quality scores": {"position": [[5077, 5079]], "sentence": ["caption level kendalls correlation between evaluation metrics and human graded quality scores"]}, "scores": {"position": [[5143, 5143]], "sentence": ["relative to the correlation between human scores of 0 73, this represents only a modest over improvement existing metrics"]}, "0 73,": {"position": [[5145, 5145]], "sentence": ["relative to the correlation between human scores of 0 73, this represents only a modest over improvement existing metrics"]}, "73,": {"position": [[5146, 5146]], "sentence": ["relative to the correlation between human scores of 0 73, this represents only a modest over improvement existing metrics"]}, "rank coefficient correlation": {"position": [[5189, 5191]], "sentence": ["with spice achieving a rank coefficient correlation of 0 39, compared to 0 36 for cider and 0 35 for meteor"]}, "reporting rank": {"position": [[5237, 5237]], "sentence": ["agreement be for established. consistency with previous evaluations on the pascal dataset instead [12], of reporting rank correlations we evaluate on this dataset using accuracy"]}, "rank correlations": {"position": [[5238, 5239]], "sentence": ["be for established. consistency with previous evaluations on the pascal dataset instead [12], of reporting rank correlations we evaluate on this dataset using accuracy"]}, "accuracy": {"position": [[5246, 5246]], "sentence": ["the pascal dataset instead [12], of reporting rank correlations we evaluate on this dataset using accuracy"]}, "evaluators.": {"position": [[5274, 5274]], "sentence": ["higher score to caption the in each candidate pair most commonly preferred by human to evaluators. help quantify the impact of reference captions on performance"]}, "randomness": {"position": [[5322, 5322]], "sentence": ["although our results differ which slightly may be due to randomness in the choice of reference caption subsets"]}, "reference caption": {"position": [[5327, 5327]], "sentence": ["although our results differ which slightly may be due to randomness in the choice of reference caption subsets"]}, "code). pascal-50s": {"position": [[5343, 5344]], "sentence": ["differences or in metric implementations we use the ms coco evaluation on code). pascal-50s"]}, "distinguishing": {"position": [[5403, 5403]], "sentence": ["mm pairs as illustrated in table 4 and figure right. 4 this is important as distinguishing better performing algorithms is primary the motivation for this conclusion 5 work. and future we"]}, "semantic evaluation metric": {"position": [[5425, 5427]], "sentence": ["a novel semantic evaluation metric that measures effectively how image captions recover objects"]}, "pascal": {"position": [[5506, 5506]], "sentence": ["caption level classification accuracy of evaluation metrics at matching human judgment on pascal with 5 reference captions"]}, "semantic and parsing,": {"position": [[5620, 5622]], "sentence": ["we are aware that significant challenges still remain in semantic and parsing, hope that the development of more powerful parsers will underpin improvements further to the metric"]}, "annotators": {"position": [[5648, 5648]], "sentence": ["in future work we hope to use human to annotators establish an upper bound for how closely spice approximates human judgments given perfect semantic parsing"]}, "perfect semantic parsing": {"position": [[5661, 5663]], "sentence": ["human to annotators establish an upper bound for how closely spice approximates human judgments given perfect semantic parsing"]}, "convolutional networks": {"position": [[5828, 5829]], "sentence": ["t : long term recurrent convolutional networks for recognition visual and description"]}, "cvpr": {"position": [[5837, 5837], [6227, 6227], [6326, 6326], [7151, 7151]], "sentence": ["in cvpr", "in in cvpr cvpr", "in in in cvpr cvpr cvpr", "in in in in cvpr cvpr cvpr cvpr"]}, "y": {"position": [[5870, 5870]], "sentence": ["bengio, r.s., y : show"]}, "j": {"position": [[5900, 5900], [5934, 5934]], "sentence": ["j : framing image description as a task: ranking data", "j : framing image description as a task: ranking data j : from image descriptions to denotations: visual new similarity metrics for semantic inference over event"]}, "jair": {"position": [[5916, 5916], [6152, 6152]], "sentence": ["jair 47 4. 853899 young", "jair 47 4. 853899 young jair 55 10. 409442 papineni"]}, "denotations:": {"position": [[5940, 5940]], "sentence": ["j : from image descriptions to denotations: visual new similarity metrics for semantic inference over event tacl descriptions. 2 5. 6778 lin"]}, "f": {"position": [[6087, 6087]], "sentence": ["f : comparing automatic evaluation measures for image description"]}, "ikizler-cinbis": {"position": [[6122, 6122]], "sentence": ["e , ikizler-cinbis"]}, "w": {"position": [[6170, 6170]], "sentence": ["w : bleu a method for automatic evaluation of machine translation"]}, "d": {"position": [[6217, 6217], [6349, 6349], [6516, 6516]], "sentence": ["d : cider consensus based image description evaluation", "c d : cider consensus based image description evaluation d : generating semantically precise scene graphs from textual descriptions for improved image retrieval", "c d : cider consensus based image description evaluation d : generating semantically precise scene graphs from textual descriptions for improved image retrieval d : fully automatic semantic mt evaluation"]}, "cider consensus": {"position": [[6219, 6219]], "sentence": ["d : cider consensus based image description evaluation"]}, "consensus": {"position": [[6220, 6220]], "sentence": ["d : cider consensus based image description evaluation"]}, "eacl": {"position": [[6251, 6251]], "sentence": ["in eacl 2014 workshop on statistical translation. machine 14. (2014) gim enez"]}, "statistical machine": {"position": [[6284, 6284], [6939, 6939], [6967, 6967]], "sentence": ["in acl second workshop on statistical machine 15. translation johnson", "in acl tenth workshop on in acl second workshop on statistical machine 15. translation johnson statistical machine translation", "in ninth acl workshop on in acl tenth workshop on in acl second workshop on statistical machine 15. translation johnson statistical machine translation statistical machine translation"]}, "c d": {"position": [[6348, 6348]], "sentence": ["c d : generating semantically precise scene graphs from textual descriptions for improved image retrieval"]}, "s": {"position": [[6385, 6385], [6770, 6770]], "sentence": ["s : a transition based algorithm for amr in: parsing. hlt-naacl", "s : a transition based algorithm for amr in: parsing. hlt-naacl s : flickr30k entities collecting region to phrase correspondences for image-to-sentence richer models"]}, "transition": {"position": [[6388, 6388]], "sentence": ["s : a transition based algorithm for amr in: parsing. hlt-naacl"]}, "complex textual": {"position": [[6421, 6421]], "sentence": ["r : visual semantic search videos retrieving via complex textual queries"]}, "textual queries": {"position": [[6422, 6423]], "sentence": ["r : visual semantic search videos retrieving via complex textual queries"]}, "unlexicalized parsing": {"position": [[6452, 6453]], "sentence": ["c d : accurate unlexicalized parsing"]}, "universal stanford": {"position": [[6489, 6489]], "sentence": ["manning, j., c d : universal stanford dependencies a cross linguistic typology"]}, "stanford": {"position": [[6490, 6490]], "sentence": ["manning, j., c d : universal stanford dependencies a cross linguistic typology"]}, "semantic mt": {"position": [[6520, 6521]], "sentence": ["d : fully automatic semantic mt evaluation"]}, "support vector machines": {"position": [[6561, 6563]], "sentence": ["d : shallow semantic parsing using support vector machines"]}, "semantic tuples": {"position": [[6594, 6595]], "sentence": ["f , quattoni , a : semantic tuples for evaluation of image sentence generation"]}, "emnlp": {"position": [[6658, 6658]], "sentence": ["emnlp 25. 15331544 flanigan"]}, "graph-based discriminative parser": {"position": [[6683, 6685]], "sentence": ["n a : a graph-based discriminative parser for the abstract meaning representation"]}, "subgraph": {"position": [[6710, 6710]], "sentence": ["c : robust subgraph generation improves abstract meaning representation parsing"]}, "k": {"position": [[6729, 6729]], "sentence": ["k : smatch an evaluation metric for semantic feature in: structures. acl (2)"]}, "crowdsourced": {"position": [[6846, 6846]], "sentence": ["l : visual genome connecting language and vision using crowdsourced dense image annotations"]}, "unbiased": {"position": [[6866, 6866]], "sentence": ["a a : unbiased look at dataset bias"]}, "probabilistic earley parser": {"position": [[6883, 6885]], "sentence": ["j : a probabilistic earley parser as a psycholinguistic model"]}, "psycholinguistic": {"position": [[6888, 6888]], "sentence": ["j : a probabilistic earley parser as a psycholinguistic model"]}, "expectation based syntactic": {"position": [[6900, 6901]], "sentence": ["r : expectation based syntactic comprehension"]}, "commonsense reasoning": {"position": [[7002, 7003]], "sentence": ["y : from images to sentences through scene description graphs using commonsense reasoning and knowledge"]}, "semantic alignments": {"position": [[7023, 7024]], "sentence": ["l : deep visual semantic alignments for generating descriptions. image in cvpr"]}, "language": {"position": [[7192, 7192]], "sentence": ["g , m.: mitchell, language models for image captioning the quirks and what works"]}, "cvpr.": {"position": [[7249, 7249], [7337, 7337], [7425, 7425], [7513, 7513], [7601, 7601], [7689, 7689], [7777, 7777], [7865, 7865], [7953, 7953], [8041, 8041]], "sentence": ["cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin", "cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin cvpr. in: 42. 25332541 devlin"]}, "nearest": {"position": [[7277, 7277], [7365, 7365], [7453, 7453], [7541, 7541], [7629, 7629], [7717, 7717], [7805, 7805], [7893, 7893], [7981, 7981], [8069, 8069]], "sentence": ["c l : exploring neighbor nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning", "c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor c l : exploring neighbor nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning nearest approaches for image captioning"]}}, "DATASET": {"the ms coco (a) dataset:": {"position": [[513, 517]], "sentence": ["consider the two following captions (a b) from the ms coco (a) dataset: a young girl standing on top of a tennis (b) court. a giraffe standing on"]}, "datasets [16 15 28] and the recently released visual genome dataset": {"position": [[1970, 1980]], "sentence": ["a general structure consistent with several vision existing datasets [16 15 28] and the recently released visual genome dataset the [29]. scene graph of candidate caption c is denoted by g(c)"]}, "microsoft datasets 4.1": {"position": [[3624, 3626]], "sentence": ["examples and failure on cases our project page2 microsoft datasets 4.1"]}, "meteor rouge-l bleu-4": {"position": [[5120, 5122]], "sentence": ["inter-human 0.39 0.36 0.35 0.28 0.18 0.26 0.45 0.44 0.42 0.32 0.14 0.32 spice cider meteor rouge-l bleu-4 bleu-1 [35] compared to 0 44 for cider and 0 42 for meteor"]}}, "TOOL": {"picsom": {"position": [[4682, 4682]], "sentence": ["captivator berkeley [40] lrcn montreal/toronto [1] m-rnn [2] nearest [41] neighbor m-rnn [42] brno mil picsom [43] university mlbl [44] neuraltalk [45] tsinghua acvt [36] spice random bigeye object relation attribute"]}}, "LANGUAGE": {}, "RESOURCE": {"semantic propositional content": {"position": [[651, 653]], "sentence": ["the limitations of existing gram based automatic metrics, evaluation in this work we hypothesize that semantic propositional content is \\\\fspice: an semantic propositional image caption important 3 evaluation component of human caption evaluation"]}, "natural language \\\\fspice: descriptions semantic": {"position": [[1685, 1688]], "sentence": ["several of these papers have demonstrated that semantic graphs can be parsed from natural language \\\\fspice: descriptions semantic propositional image caption [18,16]. 5 evaluation the task of transforming a sentence into its"]}, "relation types": {"position": [[2069, 2070]], "sentence": ["a set of relation types r"]}, "attribute types": {"position": [[2076, 2077]], "sentence": ["a set of attribute types a"]}, "object attributes, color": {"position": [[4647, 4648]], "sentence": ["are attribute although subcategories. the best models outperform the human baseline in their use of object attributes, color none of the models exhibits a convincing ability to human count msr [6] google"]}, "verbs and adjectivesexceeds": {"position": [[4986, 4988]], "sentence": ["verbs and adjectivesexceeds the human baseline f-score for tuples containing color attributes"]}}}