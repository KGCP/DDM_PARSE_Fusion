<?xml version='1.0' encoding='utf-8'?>
<document><section><heading>Introduction</heading></section><section><heading>Methodology</heading></section><section><heading>Conclusion</heading></section><section ID="1"><heading>Introduction</heading><paragraph><sentence>In the 21st century, the importance of developing cutting-edge scientific research is self-evident for every country.</sentence><sentence>Therefore,eachcountry’sgovernmentresearchfundingagenciesarewillingtoprovidemuchscientificresearchfunding tosupportessentialandcutting-edgescientificresearcheachyear.</sentence><sentence>Determiningwhetherascientificresearchprojectis worthyoffundingisasignificantandrigorousstepforfundingagencies.</sentence><sentence>Toobtainfinancialsupport,scientistsand researchersalwayswriteresearchproposalstopresenttheirresearchplansandexplainthesignificanceoftheprojectto thefundingagencies<reference>1</reference></sentence><sentence /><sentence>Usually,thegovernmentresearchfundingagenciesreceivethousandsofresearchproposals eachyear,whicharereviewedonlybyexpertpanels.</sentence><sentence>However,withtheincreaseinthenumberofresearchproposals andthedevelopmentofdataminingtechniques,fundingagenciesareincreasinglyusingdataminingmodelstoassistin themanualreviewofresearchproposals.</sentence><sentence>Atthesametime,itmustbemadeclearthatrelyingsolelyondatamining modelstoreplacemanualchecksisnotreliable.</sentence><sentence>Applyingdataminingmodelstoaresearchproposalhasseveralbenefits.</sentence><sentence>First,dataminingmodelscanbrieflyintroduce theessentialfeaturesoftheresearchproposalstohelphumanevaluatorsbetterscreentheexcellentresearchproposals, suchastheinfluentialfeaturesofthedataminingmodelsacrossalltheresearchproposals.</sentence><sentence>Second,aneffectivedata mining model can help human evaluators understand the research proposals’ strengths and weaknesses during the manualreviewprocess.</sentence><sentence>Next,ahigh-qualitydataminingmodelcanbeappliedtodevelopproceduresandguidelinesfor humanassessorstoevaluatefutureresearchproposalstoimprovethequalityofassessments.</sentence><sentence>Fourth,forgovernmentor fundingagencies,differentfundingprojectsshouldbeestablishedtoimprovethequalityofvarioustypesofresearch.</sentence><sentence>Dataminingmodelscanbetterunderstandhowtoensurethathumanevaluatorsrespondtothesenecessaryqualities.</sentence><sentence>Basedonthebenefitsandmotivationsmentionedabove,wehopetoapplyadataminingmodelwithanappropriate featureextractiontechniquetopredicthighIC-scoreresearchproposalsbasedontheICscoresassignedbytheexpert reviewers.</sentence><sentence>Meanwhile,theotherprimarygoaloftheprojectistodevelopapredictivevocabularyforcontemporaneous RunningTitleforHeader proposalsandtounderstandhowthemodelinferredresearchproposalswithhighICscoresfromthedatafeatures.</sentence><sentence>In addition,wefocusonproposinganefficientfeatureextractiontechniqueratherthanthechoiceofclassifiers,sowe choosetheverycommonDecisionTree(DT)andRFclassifiersforexperimentalcomparison.</sentence><sentence>Thecontributionsofthispaperarelistedasfollows: • Astrictexperimentalpipelineforanalysinggrantapplicationsisgiven,andtheexperimentalresultsproveits feasibility.</sentence><sentence>• AmodelisproposedwithaRandomForestclassifieroverdocumentsencodedwithfeaturesdenotingthe presenceorabsenceofunigramterms.TheunigramtermsareencodedbyamodifiedTermFrequency-Inverse DocumentFrequency(TF-IDF)algorithm,whichonlyimplementstheIDFpartofTF-IDF.</sentence><sentence>• TheproposedmodelforpredictinghighIC-scoreresearchproposalscanachieveanaccuracyof84.17%across alltypesofgrantapplications.</sentence><sentence>Thispaperisdividedintosixsections.</sentence><sentence>Inthefirstsection,theproject’smotivationandproblemstatementarebriefly introduced.</sentence><sentence>Insection2,thebackgroundandrelatedworkofthisprojectareintroduced.</sentence><sentence>Themethodologysection mainlydescribesthepipelineweapplyforthisresearchproject.</sentence><sentence>Section4bringstheoveralldesignoftheproject.</sentence><sentence>Then, theexperimentalsettingsandimplementationswiththehardwareplatformsareintroducedinthissection.</sentence><sentence>Thefifth sectiongivestheexperimentalresultsofthisprojectandcarriesonthefurtheranalysis.</sentence></paragraph></section><section ID="2"><heading>Methodology</heading><paragraph><sentence>sandfuturework aredescribedinsection6.</sentence><sentence>2 RelatedWork 2.1 Computerscienceinevaluatinggrantapplications Oztaysietal.</sentence><sentence>proposedamulti-criteriaapproachtoevaluateresearchproposalsbasedoninterval-valuedintuitionistic fuzzysets.</sentence><sentence>Inthismethod,afuzzypreferencerelationmatrixwasusedtodeterminetherelativeimportanceofcriteria.</sentence><sentence>ThePreferenceSelectionIndex(PSI)wasanotherinterestingmethodtoevaluateresearchgrantapplications<reference>3</reference></sentence><sentence /><sentence>One advantageofapplyingthePSImethodwasthattheresearcherdidnotneedtodeterminetheweightcriteria.</sentence><sentence>Another similarandrecentlyrelatedworkwastheresearchpaperclassificationsystembuiltbasedontheTF-IDFandLDA schemes<reference>4</reference></sentence><sentence /><sentence>ThissystemusedaLatentDirichletallocation(LDA)schemetoextractrepresentativekeywordsfrom theabstractofeachpaper<reference>5</reference></sentence><sentence /><sentence>TheK-meansclusteringalgorithm<reference>6</reference></sentence><sentence>wasappliedtogrouppaperswithsimilartopics basedontheTF-IDFvectorencodingofeachpaper.</sentence><sentence>TheresultsshowedthattheLDAwith30keywordsusingTF-IDF obtainedthebestF-scorecomparedwiththeLDAwithfewerkeywords.</sentence><sentence>2.2 Termvectorsandstatisticalmeasuresintextrepresentation TF-IDFiscommonlyusedindataminingandinformationretrieval.</sentence><sentence>TFindicatesthefrequencyofawordinadocument oracollectionofdocuments.</sentence><sentence>WhencalculatingTF,allthewordsfromdocumentsaretreatedasequallyimportant.</sentence><sentence>However,inpractice,peopleonlypayattentiontoacertainofwords.</sentence><sentence>Forexample,“this",“are",and“it"usuallydonot representimportantinmostcases.</sentence><sentence>Then,theIDFisimplementedtoadjustthetermweightsindocumentswhichcan increasetheweightsofthoserarebutimportantwordsandweighdownthosefrequentwordsbutlessimportant.</sentence><sentence>In 2016,GuoandYang<reference>7</reference></sentence><sentence>analysedtheshortcomingsoftheTF-IDFalgorithm.</sentence><sentence>Then,anintra-classdispersionalgorithm basedonTF-IDFwasproposed.</sentence><sentence>Chenetal.</sentence><sentence>proposedanewtermweightingtechniquecalledTermFrequency &amp;InverseGravityMoment(TF-IGM),whichwasmainlyusedtomeasuretheclassdiscriminationofaterm.</sentence><sentence>The experimentalresultsshowedthattheTF-IGMperformedbetterthanthetraditionalTF-IDFinthreestandardcorpora.</sentence><sentence>DasandChakraborty<reference>9</reference></sentence><sentence>proposedatextsentimentclassificationtechniquebasedontheTF-IDFalgorithmandNext Word Negation (NWN).</sentence><sentence>In addition, this study also compared the binary bag of words, TF-IDF, and TF-IDF with NWNalgorithms.</sentence><sentence>FanandQin<reference>1</reference><reference>0</reference></sentence><sentence>proposedanotherimprovedTF-IDFalgorithm,TF-IDCRF,whichfocusedonthe relationshipbetweenclassesintheclassificationmodel.</sentence><sentence>In2019,animprovedTF-IDFalgorithmbasedonclassification discriminationstrengthwasproposedfortextclassification<reference>1</reference><reference>1</reference></sentence><sentence /><sentence>2.3 Dataminingmodelsintextclassification In the field of data mining, the DT classifier is widely welcome for its advantage of showing how models make decisionsaccordingtothedatafeatures<reference>1</reference><reference>2</reference></sentence><sentence /><sentence>RFclassifierisanotherpopulardataminingmodel.</sentence><sentence>Thetermforestcan beinterpretedtomeanthateachclassifierintheensembleisaDTclassifier,whileallcombinationsofclassifiersarea forest<reference>1</reference><reference>3</reference></sentence><sentence /><sentence>IntheRFclassifier,eachdecisiontreealsoselectstheoptimalattributebasedontheAttributeSelection 2 RunningTitleforHeader Measures(ASM).Atthesametime,eachdecisiontreedependsindependentlyonarandomsample.</sentence><sentence>TheRFclassifier votesoneachtreeinspecificclassificationproblemsandselectsthemostpopularcategoryasthefinalresult.</sentence><sentence>In2016,anewsclassificationmethodwasproposedbasedontheTF-IDFalgorithmandSupportVectorMachine(SVM)<reference>1</reference><reference>4</reference></sentence><sentence /><sentence>Basedonadifferentnumberofn-gramsandvariousdatasets,fivedataminingclassifierswerebuiltandcompared<reference>1</reference><reference>5</reference></sentence><sentence /><sentence>Theresultscanguideresearcherstoselectanappropriatedataminingmodelaccordingtothesizeofthedata set.</sentence><sentence>Fourdifferentdataminingmodelswereimplementedwithfivedifferentensemblemethods,andtheexperimental resultsshowedthattheRFclassifierwiththeBaggingensemblemethodachievedthebestperformance<reference>1</reference><reference>6</reference></sentence><sentence /><sentence>Wongsoet al.</sentence><sentence>[17]appliedTF-IDFandSVDalgorithms[18]tothefeatureselectionstepandcomparethetwoalgorithms<reference>1</reference><reference>7</reference><reference>1</reference><reference>8</reference></sentence><sentence>ppliedTF-IDFandSVDalgorithms[18]tothefeatureselectionstepandcomparethetwoalgorithms.</sentence><sentence>At thesametime,theMultivariateBernoulliNaiveBayes<reference>1</reference><reference>9</reference></sentence><sentence>andSVMwerecomparedinthisstudy.</sentence><sentence>Finally,withthe combinationofTF-IDFandMultivariateBernoulliNaiveBayes,newsarticlesintheIndonesianLanguagecorpuswere classified,andthebestresultwasobtained<reference>1</reference><reference>7</reference></sentence><sentence /><sentence>3</sentence></paragraph></section><section ID="3"><heading>Conclusion</heading><paragraph><sentence>Figure1: Thepipelineonanalysinggrantapplications.</sentence><sentence>Thissectiondetailstheworkflowofourproposedpipelineandthedataminingmodel.</sentence><sentence>Fig.1showsthepipelineof analysinggrantapplications.</sentence><sentence>3.1 Dataset Thedatasetusedtoanalysethegrantapplicationsisthe2019grantapplicationssubmittedtoanAustralianGovernment researchfundingagency.</sentence><sentence>3,805researchproposalsaregiveninthisdatasetwithpeer-reviewedICscores(1-7).</sentence><sentence>In addition,theentiredatasetcontainsdifferenttypesofgrantapplications,suchasSynergyGrants,StandardProject Grants,andIdeasGrants.BesidestheICscore,reviewersalsoscoreseveralotherassessmentscores,suchas“Feasibility Score”or“SignificanceScore.” SinceallresearchproposalsaresavedinPDFformat, extractingthetextfromPDFfilesisnecessary.</sentence><sentence>AMetadata Extractor&amp;Loader(MEL)<reference>2</reference><reference>0</reference></sentence><sentence>toolisappliedtoextracttextfromPDFresearchproposalsandsaveitinaJSONfile withmetadatasetsandcontent.</sentence><sentence>Bydefault,allJSONfilesarestoredinCouchDBdatabase<reference>2</reference><reference>1</reference></sentence><sentence>basedontheproposal index.</sentence><sentence>Beforedesigningthewholepipeline,astatisticalanalysisisrequiredbasedontheICscoresofresearchproposals.</sentence><sentence>Atthesametime,somefundamentalvaluesarealsothebasisofdesigningtheentirepipeline,suchasthemedianIC scoreandmodeICscore.</sentence><sentence>Table1showsastatisticofICscores,showingthat3,693researchproposalshavevalidIC scores.</sentence><sentence>Inaddition,99researchproposalsdonotcontainanICscore,13ofwhichhaveanICscorebelow1.0,andthese researchproposalsthereforenotbeusedinthisproject.</sentence><sentence>Table1alsoshowsthemedianICscore,5.0,themostfrequent ICscore.</sentence><sentence>3.2 Textpre-processingforgrantapplications HaCohen-Kerneretal.</sentence><sentence>provedthattextpre-processingtechniquescouldmakethemodelachievebetterperformance thanwithoutthetextpre-processingstep.</sentence><sentence>Afterallthetextisextracted,allcharacters,whetheruppercaseorlowercase, areconvertedtolowercase.</sentence><sentence>Then,thenumbersarealsoremovedbecausethenumbersintheresearchproposalsare notrelevantforfutureanalysis.</sentence><sentence>Thirdly,removingpunctuationsandtokenizingbywhitespacearealsoadopted,which makethetextintosmallpiecescalledtokens.</sentence><sentence>3 RunningTitleforHeader Table1: StatisticalsummaryoftheICscoresofgrantapplications.</sentence><sentence>Statistic Value ICscorecounting 3,693 Standarddeviation 0.65 Median 5.00 Mode 5.00 Mean 4.92 Min 1.75 25% 4.50 50% 5.00 75% 5.36 Max 6.90 Table2: Experimentaldesignofhigh,lowandmoderateIC-scoreresearchproposalsselection.</sentence><sentence>No.</sentence><sentence>LowICscore HighICscore ModerateICscore 1 score&lt;4.25(0∼15%) score&gt;5.63(85%∼100%) 5.63≥score≥4.25(15%∼85%) 2 score&lt;4.36(0∼20%) score&gt;5.5(80%∼100%) 5.5≥score≥4.36(20%∼80%) 3 score&lt;4.5(0∼25%) score&gt;5.375(75%∼100%) 5.375≥score≥4.5(25%∼75%) 4 score&lt;4.57(0∼30%) score&gt;5.25(70%∼100%) 5.25≥score≥4.57(30%∼70%) 5 score&lt;4.75(0∼35%) score&gt;5.2(65%∼100%) 5.2≥score≥4.75(35%∼65%) 6 score&lt;4.75(0∼40%) score&gt;5.08(60%∼100%) 5.08≥score≥4.75(40%∼60%) Inaddition,thedeletionofstopwordsisoneofthemostcrucialtextpre-processingtechniques.</sentence><sentence>Fanietal.</sentence><sentence>have shown that deleting stop words can improve the performance of data mining tasks.</sentence><sentence>Therefore, we create a list of customstopwordsaccordingtotheIDFformulaanddeletetheIDFvalueofthetermfromthetextlowerthan1.0.</sentence><sentence>The reasonforchoosing1.0isthatafterimplementationsomepreliminaryexperiments,weconfirmthatthefeaturewords consideredasimportantbyDTandRFclassifierswilllessthan1000words.</sentence><sentence>Meanwhile,thewordswhoseIDFvalue arelessthan1.0onlyaccountfor0.2%ofthetotalwords,andtheyareallcommonwordssuchas“next”,“shift”and “other”.</sentence><sentence>Webelievethatthesewordsappeartoofrequentlyandhavenoinfluenceontheexperimentalresults.</sentence><sentence>Finally, textstemmingisthelasttechniqueweapplyinthetextpre-processingstep.</sentence><sentence>Textstemmingisatechniqueforreducing eachwordtoitsrootformat<reference>2</reference><reference>4</reference></sentence><sentence /><sentence>Ithelpstoreducethevocabularyandsurfacesyntaxtogetclosertothemeaningof eachterm,andthePorterStemmingalgorithm<reference>2</reference><reference>5</reference></sentence><sentence>isimplementedinthisstep.</sentence><sentence>3.3 Selectionofhigh,lowandmoderateIC-scoreresearchproposals AssumingthattheclassificationmodelistrainedonresearchproposalswithsignificantlyhighandlowICscores,better resultswillbeobtainedifindeterminateresearchproposalsareavoided.</sentence><sentence>Therefore,weconductseveralexperimentsto determinethecut-offpointtodistinguishresearchproposalswithhighandlowICscoresfrommoderateICscores.</sentence><sentence>WedecidetousetheTF-IDFalgorithmandRFclassifierintheselectionexperiment.</sentence><sentence>Thetrainingdatarangecanbe selectedbytrainingseveralcut-offpoints,suchas15,20,25,30,35,40.</sentence><sentence>Table2showsalistofexperimentsforhigh, low,andmoderateIC-scoreresearchproposalsselection.</sentence><sentence>Forexample,inthefirstexperiment,allresearchproposalsare rankedaccordingtotheICscorefrom1to7,with0∼15%aslowIC-scoreresearchproposalsand85%∼100%ashigh ICscoreresearchproposals.</sentence><sentence>Thentheselectedresearchproposalsaredividedinto85%trainingsetand15%testset.</sentence><sentence>In additiontotheaboveresearchproposals,another15%ofresearchproposalsarerandomlyselectedbasedontheoriginal distributioninthemoderaterange.</sentence><sentence>Then,thefinaltestdatacombinestheprevioustestdatasetandtheselected15% moderateIC-scoreresearchproposals.</sentence><sentence>ItcanbefoundthattheactualsizeofthemoderateIC-scoreresearchproposals andthefinaltestdatavarieswiththecut-offoptions.</sentence><sentence>Inthefollowingexperiments,eachexperimentisdesignedand conductedthesameasthefirstexperiment.</sentence><sentence>Ifthenumberofintermediate-levelresearchproposalsisshort,e.g.,the No.5andNo.6experiments,allmoderateIC-scoreresearchproposalsareconsidered.</sentence><sentence>Somedetailsrequirefurtherexplanation.</sentence><sentence>Firstly,800researchproposals(400withlowICscoresand400withhigh ICscores)arerandomlyselectedtoimplementtheRFclassifierineachexperiment.</sentence><sentence>Onereasonisthatitcanreduce thetrainingscalebydrawingsamplingfromthetrainingdataset,thusreducingthemodel’stime.</sentence><sentence>Themostimportant 4 RunningTitleforHeader reasonisthateachresearchproposalhasmanyuniqueterms,thentherewillbemorethan110,000uniquetermsinthe corpusfor800researchproposals,whichmayleadtoinsufficientmemoryduringmodeltraining.</sentence><sentence>Secondly, in the final test data set, the moderate IC-score research proposals are labelled based on the median IC scoreof5.0.</sentence><sentence>ProposalswithanICscoregreaterthan5.0arelabelledashighIC-scoreproposalsinthefinaltestdata set.</sentence><sentence>IftheresearchproposalshaveanICscoreoflessthan5.0,theseproposalsarelabelledaslowIC-scoreresearch proposals.</sentence><sentence>Thirdly,itneedstobeexplainedthatresearchproposalswithborderlineICscoresarenotselectedwhen selectingresearchproposalswithhighorlowICscores.</sentence><sentence>Forexample,thelowICscoreforexperimentNo.1issetat 15%,whichis4.25.</sentence><sentence>Then,theproposalswithanICscoreof4.25willnotbeconsideredthelowIC-scoreresearch proposals.</sentence><sentence>Fourthly,thelowICscoreboundaryof4.75isselectedinbothexperiments5and6.</sentence><sentence>Thereasonisthatthe ICscoreof4.75iscommon,andmorethan5%(35%∼40%)researchproposalshavethisICscore.</sentence><sentence>Table3: TheselectionexperimentalresultsbasedonRFclassifier,TF-IDFalgorithmandunigramfeatures.</sentence><sentence>No.</sentence><sentence>Testaccuracy(%) 1 66.91 2 65.88 3 62.17 4 56.84 5 65 6 Lessthan50 AccordingtotheresultsfromTable3,thebestperformancecanachieve66.91%basedonanRFclassifier,TF-IDF algorithmwith15%and85%cut-offoption.</sentence><sentence>Then,the15%,85%optionisselectedforfutureexperimentsondifferent dataminingmodelswithfeatureextractiontechniques.</sentence><sentence>3.4 Designandapplythefeatureextractiontechnique WeproposeamodifiedTF-IDFalgorithm,whichonlyimplementstheIDFpartofTF-IDFasthefeatureextraction technique.</sentence><sentence>Inspecific,ifthetermexistsatleastonceinthedocuments,specifytheIDFvalueforthistermdirectly.</sentence><sentence>In addition,ifatermdoesnotexistinthedocuments,thenthetermisassignedavalueof0.</sentence><sentence>Thedesignofthismodified featureextractionalgorithmfollowstheideathatraretermscandefineinnovativeness.</sentence><sentence>Theexperimentalsoconsidersthen-grams<reference>2</reference><reference>6</reference></sentence><sentence /><sentence>Unigramisthemostcommonchoicefortextclassificationtasks, butbigramandtrigrammaybetterrepresentscientificterms,wherebigramistwoconsecutivewordsinasentence, andtrigramisthreeconsecutivewordsinasentence.</sentence><sentence>Atthesametime,whencollectingproposals,wealsoconsider deletingthewordsthatonlyexistonceortwice,becauseveryraretermstendnottobepredictive.</sentence><sentence>Inaddition,the bigrammentionedinthispaperdenotesacombinationoftheunigramsandbigrams.</sentence><sentence>Thetrigramdenotesacombination oftheunigrams,bigrams,andtrigrams.</sentence><sentence>3.5 Applydataminingmodelswithgrantapplications ThispaperusesDTandRFclassifiersfortextclassificationbecausewewouldliketofindoutthemostinfluential termsandunderstoodhowthedataminingmodelpredictshighandlowIC-scoreresearchproposals.</sentence><sentence>TheDTandRF classifiersareconvenienttopresentthisvaluableinformation.</sentence><sentence>Basedontheexperimentalresultofthehighandlow IC-scoreresearchproposalsselection, allexperimentsareconductedwiththelowIC-scoreresearchproposals(IC score0∼15%)andthehighIC-scoreresearchproposals(ICscore85%∼100%).</sentence><sentence>Inthecomparisonstudyoffeature extractiontechniques,400researchproposalsforeachlowandhighICscorearerandomlyselectedformodeltraining, andthetrainingdatais85%,andthetestdatais15%.</sentence><sentence>Inordertoanalysetheproposedmodelintheend,the100most influentialtermsfromthecollectionsofresearchproposalsareextractedbythefunctionfromscikit-learnlibrary<reference>2</reference><reference>7</reference></sentence><sentence>whichbringusanintuitiveunderstandingofhowmucheachtermcontributestoreducingtheweightedimpurities.</sentence><sentence>3.6 AnalysemoderateIC-scoregrantapplications WealsoconductseveralexperimentstoanalysemoderateIC-scoreresearchproposalsbasedontheproposedmodel.</sentence><sentence>ThepurposeofthisseriesofexperimentsistodeterminewhetherthereisarelationbetweenproposalswithmoderateIC scoresandthatofhighandlowICscores.</sentence><sentence>SincetheproposedmodelistrainedbasedonthelowIC-scoreproposalsof 0∼15%andhighIC-scoreproposalsof85∼100%,therangeofresearchproposalswithmoderateICscoreis15%∼85%.</sentence><sentence>BasedonthemedianICscore,theselectionrangeoftestingmoderateICscorebytestingseveralcut-offoptions,suchas 5 RunningTitleforHeader Table4: ExperimentaldesignforanalyzingmoderateIC-scoreresearchproposals.</sentence><sentence>No.</sentence><sentence>NewlowIC-scorerange NewhighIC-scorerange 1 15%∼20% 80%∼85% 2 15%∼25% 75%∼85% 3 15%∼30% 70%∼85% 4 15%∼35% 65%∼85% 5 15%∼40% 60%∼85% 6 15%∼45% 55%∼85% 7 15%∼50% 50%∼85% 20,25,30,35,40,45,and50.</sentence><sentence>Table4showsalistofexperimentsusedtoanalysetheresearchproposalsofmoderateIC score.</sentence><sentence>ConsideringthesymmetricdistributionoftheICscores,newresearchproposalswithlowandhighICscoresare selectedineachexperiment,andperformanceanalysisisconductedbasedontheproposedtrainingmodel.</sentence><sentence>Inaddition totheexperimentsinTable4,anotherexperimentisdesignedtocheckthemedianIC-scoreresearchproposals(IC score=5.0)topredicttheproportionofhighorlowIC-scoreresearchproposalsratherthancalculatethetestaccuracy.</sentence><sentence>4 ExperimentalSettings Thissectiondescribesallexperimentalsettingsforthispaper.</sentence><sentence>Initially,MEL<reference>2</reference><reference>0</reference></sentence><sentence>isimplementedthroughasetof Python-basedmethodstoextractmetadataforallsupportedfiletypes.</sentence><sentence>ToextractmetadatafromthePDFversionofa file,theTesseract-OCRmethod[28]andpdftotexttool[29]areapplied<reference>2</reference><reference>8</reference><reference>2</reference><reference>9</reference></sentence><sentence>ractmetadatafromthePDFversionofa file,theTesseract-OCRmethod[28]andpdftotexttool[29]areapplied.</sentence><sentence>Inthestatisticalanalysisofgrantapplications, thePythonlanguageandNumpylibrary<reference>3</reference><reference>0</reference></sentence><sentence>areusedtocalculatethemedian,mode,andotherstatisticalmeasurements ofICscore.</sentence><sentence>Intheexperimentsofselectinghigh,low,andmoderateIC-scoreresearchproposalsandimplementingthe dataminingmodels,thescikit-learnlibrary<reference>2</reference><reference>7</reference></sentence><sentence>isappliedtoimplementtheDTandRFclassifiers.</sentence><sentence>Thepythonlibrary gensim<reference>3</reference><reference>1</reference></sentence><sentence>isusedtoimplementtheTF-IDFalgorithmandthenewlyproposedmodifiedTF-IDFalgorithm.</sentence><sentence>Hyper-parametertuningisasignificantstepinapplyingdataminingmodels,andtheBayesianOptimizationtool<reference>3</reference><reference>2</reference></sentence><sentence>is applied.</sentence><sentence>The first step to implement Bayesian Optimization is to define the data mining model, such as the RF classifieranditsparametersandcorrespondingbounds.</sentence><sentence>Inaddition,wealsoneedtoimplementthescoringmethod andthecross-validationsetup.</sentence><sentence>Secondly,themaximizemethodisusedtorunthetechniquewithn_iterandinit_points parameters.</sentence><sentence>Then_iterisdefinedforthenumberofstepstoruntheoptimizationfunction.</sentence><sentence>Themoresteps,theeasierit istofindthebestaccuracyvalue.</sentence><sentence>Theinit_pointsisdefinedforrandomexplorationontheparameterspace,whichhelps toexplorethediversityofthespace.</sentence><sentence>Finally,theparametervaluesforeachaccuracyarelisted,highlightingthebest combinationoftheparameterandthetargetvalue.</sentence><sentence>To find the hyper-parameters of the RF classifier, the range of each parameter is set as follows: max_depth = (5, 60),min_samples_split=(10,100),max_features=(0.1,0.999),max_samples_leaf=(10,50)andn_estimation= (100,400).</sentence><sentence>FortheDTclassifier,therangesettingsforfindinghyper-parametersareasfollows: max_depth=(3,10), min_samples_split=(3,10),max_features=(0.1,0.999),andmax_samples_leaf=(3,10).</sentence><sentence>Themax_depthparameter indicatesthemaximumdepthofthetree,andthemax_featuresdenotesthenumberoffeaturestoconsiderwhenfinding thebestsplit<reference>2</reference><reference>7</reference></sentence><sentence /><sentence>Theparametersmin_samples_leaf,min_samples_split,andn_estimatorsaredefinedastheminimum numberofsamplesneededonaleafnode,theminimumnumberofsamplesneededtosplitaninternalnode,andthe numberoftreesintheforest,respectively.</sentence><sentence>AllexperimentsrelatedtoRFclassifierandDTclassifieradoptthesame settingofthehyper-parameterrange.</sentence><sentence>Meanwhile,the10-foldcross-validationmethodisalsoappliedinfindingthe hyper-parameters.</sentence><sentence>ToevaluatetheperformanceofthenewlyproposedmodifiedTF-IDFalgorithmandtheTF-IDFalgorithmwithdifferent dataminingclassifiers,theclassificationaccuracy(Acc),F1scoreareselectedastheevaluationmetrics.</sentence><sentence>Thehardware platformisMacBookProwithIntelCorei72.9GHzQuard-Coreprocessor.</sentence><sentence>Thememoryconfigurationis16GB2133 MHzLPDDR3.</sentence><sentence>5 ExperimentalResult Table 5 shows the performance of the TF-IDF algorithm with DT and RF classifiers.</sentence><sentence>It can be found that the RF classifiercanconsistentlyachievebetterperformancethantheDTclassifierunderthedifferentsettingsofthen-grams anddeletionofrareterms.</sentence><sentence>6 RunningTitleforHeader Table5: ExperimentalresultswithTF-IDFalgorithm.</sentence><sentence>No.</sentence><sentence>Model Acc(%) F1score(%) 1 DTclassifier+unigram 76.67 71.43 2 DTclassifier+unigram+removewordsexist≤1time 76.67 71.43 3 DTclassifier+unigram+removewordsexist≤2times 79.17 76.47 4 DTclassifier+bigram 75.83 76.03 5 DTclassifier+bigram+removewordsexist≤1time 73.33 68.63 6 DTclassifier+bigram+removewordsexist≤2times 71.67 66.67 7 DTclassifier+trigram 76.67 71.43 8 DTclassifier+trigram+removewordsexist≤1time 76.67 71.43 9 DTclassifier+trigram+removewordsexist≤2times 76.67 71.43 10 RFclassifier+unigram 81.67 78.85 11 RFclassifier+unigram+removewordsexist≤1time 80.83 77.67 12 RFclassifier+unigram+removewordsexist≤2times 80 76.47 13 RFclassifier+bigram 80.83 80.34 14 RFclassifier+bigram+removewordsexist≤1time 80.83 80.00 15 RFclassifier+bigram+removewordsexist≤2times 79.17 77.88 16 RFclassifier+trigram 81.67 78.85 17 RFclassifier+trigram+removewordsexist≤1time 80 76.47 18 RFclassifier+trigram+removewordsexist≤2times 81.67 78.85 Table6: ExperimentalresultswiththenewlyproposedmodifiedTF-IDFalgorithm.</sentence><sentence>No.</sentence><sentence>Model Acc(%) F1score(%) 1 DTclassifier+unigram 77.5 74.29 2 DTclassifier+unigram+removewordsexist≤1time 80.83 78.10 3 DTclassifier+unigram+removewordsexist≤2times 77.5 75.23 4 DTclassifier+bigram 79.17 75.73 5 DTclassifier+bigram+removewordsexist≤1time 79.17 77.48 6 DTclassifier+bigram+removewordsexist≤2times 79.17 73.68 7 DTclassifier+trigram 76.67 69.57 8 DTclassifier+trigram+removewordsexist≤1time 76.67 69.57 9 DTclassifier+trigram+removewordsexist≤2times 74.17 73.50 10 RFclassifier+unigram 84.17 81.55 11 RFclassifier+unigram+removewordsexist≤1time 84.17 81.55 12 RFclassifier+unigram+removewordsexist≤2times 84.17 81.55 13 RFclassifier+bigram 84.17 81.55 14 RFclassifier+bigram+removewordsexist≤1time 83.34 80.39 15 RFclassifier+bigram+removewordsexist≤2times 84.17 81.55 16 RFclassifier+trigram 84.17 81.55 17 RFclassifier+trigram+removewordsexist≤1time 84.17 81.55 18 RFclassifier+trigram+removewordsexist≤2times 84.17 81.55 7 RunningTitleforHeader Figure2: Confusionmatrixon“unseen”testsetoftheproposedmodel.</sentence><sentence>Table6showstheperformanceofthenewlyproposedmodifiedTF-IDFalgorithmwithDTandRFclassifiers.</sentence><sentence>Basedon thecomparisonofTable5andTable6,thebestperformanceisachievedwith84.17%accuracybytheRFclassifierwith thenewlyproposedmodifiedTF-IDFalgorithmexcepttheNo.14modelcombinationinTable6.</sentence><sentence>Thehyper-parameters aremax_depth=22,max_features=0.9931,min_samples_leaf=11,min_samples_split=67andn_estimation= 102.</sentence><sentence>Toincludeallthetermsfromthecorpus,wechoosetheRFclassifierbasedonunigramandthemodifiedTF-IDF algorithmasthefinalproposedmodel.</sentence><sentence>Anotherreasonwhywedonotchoosethebigramandtrigramcombinations astheproposedmodelisthebigramandtrigramtermsareinfactnotregardedasessentialfeaturesbyDTandRF classifiers.</sentence><sentence>Featuresextractedfromtheproposedmodelshowsthatonly618featuresareconsideredsignificant,based ontensofthousandsoffeaturesintheresearchproposals.</sentence><sentence>Basedonthecomparisonofthetwotables,itcanbefoundthattheproposedmodifiedTF-IDFalgorithmispractical andeffectivedespitetwoorthreeexceptionsexist.</sentence><sentence>Atthesametime,theexperimentalresultsprovethatthecoreidea ofdefiningthemodifiedTF-IDFalgorithmismeaningfulandshowtheraretermsassociatedwithinnovativeness.</sentence><sentence>It shouldalsobenotedthatthenewlyproposedmodifiedTF-IDFalgorithmcanbeunderstoodasasimpleencoding technique,suchastakingthevalue0ortheIDFvalueofthetermdependingonwhetherthetermexistsintheresearch proposals.</sentence><sentence>Basedonthedecisiontreeplotsgeneratedbythebestperformancemodel,itcanbefoundthatthemodified TF-IDFalgorithmdoesnotaffecttheshapeofthetreeasseeninthetreegraph,helpingtounderstandwhetherthe chosensplittermisrareorcommon.</sentence><sentence>Fromtheresultoffindinghyper-parameters,itcanbefoundthatthebestperformingmodeldoesnotuseallthefeatures toapplywiththedataminingalgorithms,suchastheRFclassifieronlyuses99.31%features.</sentence><sentence>Inaddition,although weconsiderdifferentn-grams,especiallybigramandtrigram,withremovingscarcewords,Table5andTable6could provethatitmighthelpbutnotalways.</sentence><sentence>Moreover,basedonthesamefeatureextractionalgorithm,theclassification accuracyoftheRFclassifierisalwaysbetterthanthatoftheDTclassifier.</sentence><sentence>Nevertheless,theresultsoftheDTclassifier arestillcrucialbecausetheplotofDTclassifiercontainsallthedecisions.</sentence><sentence>Fig.2showstheconfusionmatrixoftheproposedmodelforthe“unseen”testdata.</sentence><sentence>Itshows13highIC-scoreresearch proposalsareincorrectlypredictedaslowIC-scoreproposals.</sentence><sentence>Inaddition,6researchproposalswithlowICscoresare guessedwronglywhichtheyarepredictedashighIC-scoreproposals.</sentence><sentence>Thenumber59denotesthattheproposedmodel correctlypredicts59researchproposalswithlowICscoresand42withhighICscores.</sentence><sentence>8 RunningTitleforHeader Table7: ExperimentalresultsonanalysingmoderateIC-scoreresearchproposals.</sentence><sentence>No.</sentence><sentence>TestAcc(%) 1 75.91 2 73.43 3 72.5 4 65.63 5 65.25 6 64 7 61.38 Inadditiontoanalysingtheconfusionmatrix,wealsoextractthe100mostinfluentialfeaturesfromtheproposedmodel, whichgivesanintuitiveunderstandingofhowmucheachfeaturecontributestoreducingtheweightedimpurities.</sentence><sentence>The top100featuresgiveusabetterunderstandingofwhatisgoingoninsidetheblackbox.</sentence><sentence>Ameasureofthefeature importanceisvaluableforinternalmodeldevelopmentpurposesbyshowingtowhatextentfeaturescontributetotest data.</sentence><sentence>Althoughtheclassifierisonlyestablishedforthe2019grantapplicationsandmaynotpredictthehighresearch proposalsforfutureapplications,theseuniquetermsarestillvaluableandmeaningfulasareferenceforevaluators.</sentence><sentence>Table7bringstheperformanceofcheckingresearchproposalsofmoderateICscoresbasedontheproposedmodel.</sentence><sentence>Basedonthetestaccuracy, itcanbeconcludedthatthereisacorrelationbetweenthemoderateIC-scoreresearch proposalsandhigh/lowIC-scoreresearchproposals.</sentence><sentence>Moreover,itiseasytofindthattheproposedmodelcanbetter predicttheresearchproposalsclosetotheoriginaltrainingsetsettings(0∼15%forlowICscoreand85%∼100%for highICscore).</sentence><sentence>BasedontheconfusionmatrixaboveandtheexperimentalresultsofcheckingmoderateIC-scoreresearchproposals,it canbefoundthatthemodelisalwaysmoreaccurateinpredictingresearchproposalswithlowICscoresthanwith highICscores.</sentence><sentence>Meanwhile,theresearchproposalswiththemedianICscoreof5.0arepredictedtobeabout37.2% withhigh-ICscoreresearchproposalsandabout62.8%withlow-ICscoreresearchproposals.</sentence><sentence>Therefore,itcanbe concludedthatresearchproposalswithhighICscoresusemorediverselanguagethanthosewithlowIC-score.</sentence><sentence>In additiontotheexperimentsanalysingallgrantapplications,wefollowthesamepipelineandestablishanewmodelto evaluateIdeasGrantapplicationsonly,theonewithinnovationcriteria.</sentence><sentence>Applyingthesamemethodbutwithdifferent hyper-parameters,thebestperformingmodelforanalysingtheIdeasGrantscanreachanaccuracyof82.5%.</sentence><sentence>Inevery IdeasGrantapplication,thereisasectioncalled“InnovationandCreativitystatement.” Wealsoextractthispartfrom eachIdeasgrantandanalyseusingtheproposedpipeline.</sentence><sentence>Theexperimentalresultshowsthattheproposedmethod canachieve68.33%accuracyonanalysing“InnovationandCreativitystatement”sectionsonlyfromIdeasGrants.</sentence><sentence>AlthoughweguesstheICscoreismorerelevanttothe“InnovationandCreativitystatement”comparedwithother sections,asevaluatorsmaydescribetheirinnovationinthissection,theexperimentalresultdoesnotsupportourguess.</sentence><sentence>6</sentence></paragraph></section><section><heading>References</heading><reference ID="1"><authors><author>MartynDenscombe. Theroleofresearchproposalsinbusiness</author><author>managementeducation. TheInternational
JournalofManagementEducation</author></authors><title>11(3):142–149</title><journal /><year>2013</year></reference><reference ID="2"><authors><author>BasarOztaysi</author></authors><title>SeziCevikOnar</title><journal /><year>2017</year></reference><reference ID="3"><authors><author>SitiSundari</author></authors><title>MasdukiNizamFadli</title><journal /><year>2019</year></reference><reference ID="4"><authors><author>Sang-Woon Kim</author><author>Joon-Min Gil. Research paper classification systems based on tf-idf</author><author>lda schemes.
Human-centricComputing</author><author>InformationSciences</author></authors><title>9(1):30</title><journal /><year>2019</year></reference><reference ID="5"><authors><author>Chyi-KweiYau</author></authors><title>AlanPorter</title><journal /><year>2014</year></reference><reference ID="6"><authors><author>KiriWagstaff</author></authors><title>ClaireCardie</title><journal /><year>2001</year></reference><reference ID="7"><authors><author>AizhangGuo</author><author>TaoYang. Research</author><author>improvementoffeaturewordsweightbasedontfidfalgorithm. In
2016IEEEInformationTechnology</author></authors><title>Networking</title><journal /><year>2016</year></reference><reference ID="8"><authors><author>KewenChen</author></authors><title>ZupingZhang</title><journal /><year>2016</year></reference><reference ID="9"><authors><author>BijoyanDas</author><author>SaritChakraborty. Animprovedtextsentimentclassificationmodelusingtf-idf</author><author>nextword
negation. arXivpreprintarXiv:1806.06407</author></authors><title>2018.</title><journal /><year>2018</year></reference><reference ID="10"><authors><author>Huilong Fan</author><author>Yongbin Qin. Research on text classification based on improved tf-idf algorithm. In 2018
InternationalConferenceonNetwork</author></authors><title>Communication</title><journal /><year>2018</year></reference><reference ID="11"><authors><author>TingZhang</author><author>ShuzhiSamGe. Animprovedtf-idfalgorithmbasedonclassdiscriminativestrengthfortext
categorizationondesensitizeddata. InProceedingsofthe20193rdInternationalConferenceonInnovationin
ArtificialIntelligence</author></authors><title>pages39–44</title><journal /><year>2019</year></reference><reference ID="12"><authors><author>SRasoulSafavian</author><author>DavidL</author><author>grebe. Asurveyofdecisiontreeclassifiermethodology. IEEEtransactionson
systems</author></authors><title>man</title><journal /><year>1991</year></reference><reference ID="13"><authors><author>JiaweiHan</author></authors><title>JianPei</title><journal /><year>2011</year></reference><reference ID="14"><authors><author>SeyyedMohammadHosseinDadgar</author></authors><title>MohammadShirzadAraghi</title><journal /><year>2016</year></reference><reference ID="15"><authors><author>TomasPranckevicˇius</author><author>VirginijusMarcinkevicˇius. Comparisonofnaivebayes</author></authors><title>randomforest</title><journal /><year>2017</year></reference><reference ID="16"><authors><author>Aytug˘ Onan</author></authors><title>SerdarKorukog˘lu</title><journal /><year>2016</year></reference><reference ID="17"><authors><author>RiniWongso</author></authors><title>FerdinandAriandyLuwinda</title><journal /><year>2017</year></reference><reference ID="18"><authors><author>HervéAbdi. Singularvaluedecomposition(svd)</author><author>generalizedsingularvaluedecomposition. Encyclopediaof
measurement</author><author>statistics</author></authors><title>pages907–912</title><journal /><year>2007</year></reference><reference ID="19"><authors><author>Sang-BumKim</author></authors><title>Kyoung-SooHan</title><journal /><year>2006</year></reference><reference ID="20"><authors><author>SergioJ.RodríguezMéndez</author></authors><title>PouyaG.Omran</title><journal /><year>2021</year></reference><reference ID="21"><title /><journal /><year /></reference><reference ID="22"><authors><author>YaakovHaCohen-Kerner</author></authors><title>DanielMiller</title><journal /><year>2020</year></reference><reference ID="23"><authors><author>HosseinFani</author></authors><title>MasoudBashari</title><journal /><year>2018</year></reference><reference ID="24"><authors><author>JasmeetSingh</author><author>VishalGupta. Textstemming: Approaches</author></authors><title>applications</title><journal /><year>2016</year></reference><reference ID="25"><authors><author>PeterWillett. Theporterstemmingalgorithm: then</author><author>now. Program</author></authors><title>2006.</title><journal /><year>2006</year></reference><reference ID="26"><authors><author>AbinashTripathy</author></authors><title>AnkitAgrawal</title><journal /><year>2016</year></reference><reference ID="27"><authors><author>F.Pedregosa</author></authors><title>G.Varoquaux</title><journal /><year>2011</year></reference><reference ID="28"><authors><author>DMalmgren. textract. Inhttps://textract.readthedocs.io/</author></authors><title>2014.</title><journal /><year>2014</year></reference><reference ID="29"><authors><author>LLCGlyph&amp;Cog. Xpdf: PDFviewer</author><author>toolkit. Inhttps://www.xpdfreader.com/</author></authors><title>2014.</title><journal /><year>2014</year></reference><reference ID="30"><authors><author>WesMcKinney. Pythonfordataanalysis: DatawranglingwithP</author><author>as</author></authors><title>NumPy</title><journal /><year>2012</year></reference><reference ID="31"><authors><author>RadimRˇehu˚ˇrek</author><author>PetrSojka. SoftwareFrameworkforTopicModellingwithLargeCorpora. InProceedingsof
theLREC2010WorkshoponNewChallengesforNLPFrameworks</author></authors><title>pages45–50</title><journal>May</journal><year>2010</year></reference><reference ID="32"><authors><author>Jasper Snoek</author></authors><title>Hugo Larochelle</title><journal /><year>2012</year></reference></section></document>